{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2499ce49-6323-444b-ac95-9b5c876cf698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir('dataset/Diff_Quality_Estimation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16b5b800-b349-40bc-b749-654bd2df0905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds import SQLiteCodeDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24c775a5-40fd-4a6d-bfd8-a53e955cc97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from filenames import train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ccbf582-d789-4a23-94f4-332a328f7da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "device = torch.device('cuda')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/codebert-base\", num_labels=2).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a6704de-fba0-4f14-8774-82d2110f882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.roberta.base_model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12bcfc5d-c63e-4f85-9180-c8ba04e37b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.roberta.encoder.layer[10:].parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "110b674a-4566-41d6-a5c1-97cac873f838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight: False [50265, 768]\n",
      "roberta.embeddings.position_embeddings.weight: False [514, 768]\n",
      "roberta.embeddings.token_type_embeddings.weight: False [1, 768]\n",
      "roberta.embeddings.LayerNorm.weight: False [768]\n",
      "roberta.embeddings.LayerNorm.bias: False [768]\n",
      "roberta.encoder.layer.0.attention.self.query.weight: False [768, 768]\n",
      "roberta.encoder.layer.0.attention.self.query.bias: False [768]\n",
      "roberta.encoder.layer.0.attention.self.key.weight: False [768, 768]\n",
      "roberta.encoder.layer.0.attention.self.key.bias: False [768]\n",
      "roberta.encoder.layer.0.attention.self.value.weight: False [768, 768]\n",
      "roberta.encoder.layer.0.attention.self.value.bias: False [768]\n",
      "roberta.encoder.layer.0.attention.output.dense.weight: False [768, 768]\n",
      "roberta.encoder.layer.0.attention.output.dense.bias: False [768]\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight: False [768]\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias: False [768]\n",
      "roberta.encoder.layer.0.intermediate.dense.weight: False [3072, 768]\n",
      "roberta.encoder.layer.0.intermediate.dense.bias: False [3072]\n",
      "roberta.encoder.layer.0.output.dense.weight: False [768, 3072]\n",
      "roberta.encoder.layer.0.output.dense.bias: False [768]\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight: False [768]\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias: False [768]\n",
      "roberta.encoder.layer.1.attention.self.query.weight: False [768, 768]\n",
      "roberta.encoder.layer.1.attention.self.query.bias: False [768]\n",
      "roberta.encoder.layer.1.attention.self.key.weight: False [768, 768]\n",
      "roberta.encoder.layer.1.attention.self.key.bias: False [768]\n",
      "roberta.encoder.layer.1.attention.self.value.weight: False [768, 768]\n",
      "roberta.encoder.layer.1.attention.self.value.bias: False [768]\n",
      "roberta.encoder.layer.1.attention.output.dense.weight: False [768, 768]\n",
      "roberta.encoder.layer.1.attention.output.dense.bias: False [768]\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight: False [768]\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias: False [768]\n",
      "roberta.encoder.layer.1.intermediate.dense.weight: False [3072, 768]\n",
      "roberta.encoder.layer.1.intermediate.dense.bias: False [3072]\n",
      "roberta.encoder.layer.1.output.dense.weight: False [768, 3072]\n",
      "roberta.encoder.layer.1.output.dense.bias: False [768]\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight: False [768]\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias: False [768]\n",
      "roberta.encoder.layer.2.attention.self.query.weight: False [768, 768]\n",
      "roberta.encoder.layer.2.attention.self.query.bias: False [768]\n",
      "roberta.encoder.layer.2.attention.self.key.weight: False [768, 768]\n",
      "roberta.encoder.layer.2.attention.self.key.bias: False [768]\n",
      "roberta.encoder.layer.2.attention.self.value.weight: False [768, 768]\n",
      "roberta.encoder.layer.2.attention.self.value.bias: False [768]\n",
      "roberta.encoder.layer.2.attention.output.dense.weight: False [768, 768]\n",
      "roberta.encoder.layer.2.attention.output.dense.bias: False [768]\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight: False [768]\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias: False [768]\n",
      "roberta.encoder.layer.2.intermediate.dense.weight: False [3072, 768]\n",
      "roberta.encoder.layer.2.intermediate.dense.bias: False [3072]\n",
      "roberta.encoder.layer.2.output.dense.weight: False [768, 3072]\n",
      "roberta.encoder.layer.2.output.dense.bias: False [768]\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight: False [768]\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias: False [768]\n",
      "roberta.encoder.layer.3.attention.self.query.weight: False [768, 768]\n",
      "roberta.encoder.layer.3.attention.self.query.bias: False [768]\n",
      "roberta.encoder.layer.3.attention.self.key.weight: False [768, 768]\n",
      "roberta.encoder.layer.3.attention.self.key.bias: False [768]\n",
      "roberta.encoder.layer.3.attention.self.value.weight: False [768, 768]\n",
      "roberta.encoder.layer.3.attention.self.value.bias: False [768]\n",
      "roberta.encoder.layer.3.attention.output.dense.weight: False [768, 768]\n",
      "roberta.encoder.layer.3.attention.output.dense.bias: False [768]\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight: False [768]\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias: False [768]\n",
      "roberta.encoder.layer.3.intermediate.dense.weight: False [3072, 768]\n",
      "roberta.encoder.layer.3.intermediate.dense.bias: False [3072]\n",
      "roberta.encoder.layer.3.output.dense.weight: False [768, 3072]\n",
      "roberta.encoder.layer.3.output.dense.bias: False [768]\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight: False [768]\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias: False [768]\n",
      "roberta.encoder.layer.4.attention.self.query.weight: False [768, 768]\n",
      "roberta.encoder.layer.4.attention.self.query.bias: False [768]\n",
      "roberta.encoder.layer.4.attention.self.key.weight: False [768, 768]\n",
      "roberta.encoder.layer.4.attention.self.key.bias: False [768]\n",
      "roberta.encoder.layer.4.attention.self.value.weight: False [768, 768]\n",
      "roberta.encoder.layer.4.attention.self.value.bias: False [768]\n",
      "roberta.encoder.layer.4.attention.output.dense.weight: False [768, 768]\n",
      "roberta.encoder.layer.4.attention.output.dense.bias: False [768]\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight: False [768]\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias: False [768]\n",
      "roberta.encoder.layer.4.intermediate.dense.weight: False [3072, 768]\n",
      "roberta.encoder.layer.4.intermediate.dense.bias: False [3072]\n",
      "roberta.encoder.layer.4.output.dense.weight: False [768, 3072]\n",
      "roberta.encoder.layer.4.output.dense.bias: False [768]\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight: False [768]\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias: False [768]\n",
      "roberta.encoder.layer.5.attention.self.query.weight: False [768, 768]\n",
      "roberta.encoder.layer.5.attention.self.query.bias: False [768]\n",
      "roberta.encoder.layer.5.attention.self.key.weight: False [768, 768]\n",
      "roberta.encoder.layer.5.attention.self.key.bias: False [768]\n",
      "roberta.encoder.layer.5.attention.self.value.weight: False [768, 768]\n",
      "roberta.encoder.layer.5.attention.self.value.bias: False [768]\n",
      "roberta.encoder.layer.5.attention.output.dense.weight: False [768, 768]\n",
      "roberta.encoder.layer.5.attention.output.dense.bias: False [768]\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight: False [768]\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias: False [768]\n",
      "roberta.encoder.layer.5.intermediate.dense.weight: False [3072, 768]\n",
      "roberta.encoder.layer.5.intermediate.dense.bias: False [3072]\n",
      "roberta.encoder.layer.5.output.dense.weight: False [768, 3072]\n",
      "roberta.encoder.layer.5.output.dense.bias: False [768]\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight: False [768]\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias: False [768]\n",
      "roberta.encoder.layer.6.attention.self.query.weight: False [768, 768]\n",
      "roberta.encoder.layer.6.attention.self.query.bias: False [768]\n",
      "roberta.encoder.layer.6.attention.self.key.weight: False [768, 768]\n",
      "roberta.encoder.layer.6.attention.self.key.bias: False [768]\n",
      "roberta.encoder.layer.6.attention.self.value.weight: False [768, 768]\n",
      "roberta.encoder.layer.6.attention.self.value.bias: False [768]\n",
      "roberta.encoder.layer.6.attention.output.dense.weight: False [768, 768]\n",
      "roberta.encoder.layer.6.attention.output.dense.bias: False [768]\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight: False [768]\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias: False [768]\n",
      "roberta.encoder.layer.6.intermediate.dense.weight: False [3072, 768]\n",
      "roberta.encoder.layer.6.intermediate.dense.bias: False [3072]\n",
      "roberta.encoder.layer.6.output.dense.weight: False [768, 3072]\n",
      "roberta.encoder.layer.6.output.dense.bias: False [768]\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight: False [768]\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias: False [768]\n",
      "roberta.encoder.layer.7.attention.self.query.weight: False [768, 768]\n",
      "roberta.encoder.layer.7.attention.self.query.bias: False [768]\n",
      "roberta.encoder.layer.7.attention.self.key.weight: False [768, 768]\n",
      "roberta.encoder.layer.7.attention.self.key.bias: False [768]\n",
      "roberta.encoder.layer.7.attention.self.value.weight: False [768, 768]\n",
      "roberta.encoder.layer.7.attention.self.value.bias: False [768]\n",
      "roberta.encoder.layer.7.attention.output.dense.weight: False [768, 768]\n",
      "roberta.encoder.layer.7.attention.output.dense.bias: False [768]\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight: False [768]\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias: False [768]\n",
      "roberta.encoder.layer.7.intermediate.dense.weight: False [3072, 768]\n",
      "roberta.encoder.layer.7.intermediate.dense.bias: False [3072]\n",
      "roberta.encoder.layer.7.output.dense.weight: False [768, 3072]\n",
      "roberta.encoder.layer.7.output.dense.bias: False [768]\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight: False [768]\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias: False [768]\n",
      "roberta.encoder.layer.8.attention.self.query.weight: False [768, 768]\n",
      "roberta.encoder.layer.8.attention.self.query.bias: False [768]\n",
      "roberta.encoder.layer.8.attention.self.key.weight: False [768, 768]\n",
      "roberta.encoder.layer.8.attention.self.key.bias: False [768]\n",
      "roberta.encoder.layer.8.attention.self.value.weight: False [768, 768]\n",
      "roberta.encoder.layer.8.attention.self.value.bias: False [768]\n",
      "roberta.encoder.layer.8.attention.output.dense.weight: False [768, 768]\n",
      "roberta.encoder.layer.8.attention.output.dense.bias: False [768]\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight: False [768]\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias: False [768]\n",
      "roberta.encoder.layer.8.intermediate.dense.weight: False [3072, 768]\n",
      "roberta.encoder.layer.8.intermediate.dense.bias: False [3072]\n",
      "roberta.encoder.layer.8.output.dense.weight: False [768, 3072]\n",
      "roberta.encoder.layer.8.output.dense.bias: False [768]\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight: False [768]\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias: False [768]\n",
      "roberta.encoder.layer.9.attention.self.query.weight: False [768, 768]\n",
      "roberta.encoder.layer.9.attention.self.query.bias: False [768]\n",
      "roberta.encoder.layer.9.attention.self.key.weight: False [768, 768]\n",
      "roberta.encoder.layer.9.attention.self.key.bias: False [768]\n",
      "roberta.encoder.layer.9.attention.self.value.weight: False [768, 768]\n",
      "roberta.encoder.layer.9.attention.self.value.bias: False [768]\n",
      "roberta.encoder.layer.9.attention.output.dense.weight: False [768, 768]\n",
      "roberta.encoder.layer.9.attention.output.dense.bias: False [768]\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight: False [768]\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias: False [768]\n",
      "roberta.encoder.layer.9.intermediate.dense.weight: False [3072, 768]\n",
      "roberta.encoder.layer.9.intermediate.dense.bias: False [3072]\n",
      "roberta.encoder.layer.9.output.dense.weight: False [768, 3072]\n",
      "roberta.encoder.layer.9.output.dense.bias: False [768]\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight: False [768]\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias: False [768]\n",
      "roberta.encoder.layer.10.attention.self.query.weight: True [768, 768]\n",
      "roberta.encoder.layer.10.attention.self.query.bias: True [768]\n",
      "roberta.encoder.layer.10.attention.self.key.weight: True [768, 768]\n",
      "roberta.encoder.layer.10.attention.self.key.bias: True [768]\n",
      "roberta.encoder.layer.10.attention.self.value.weight: True [768, 768]\n",
      "roberta.encoder.layer.10.attention.self.value.bias: True [768]\n",
      "roberta.encoder.layer.10.attention.output.dense.weight: True [768, 768]\n",
      "roberta.encoder.layer.10.attention.output.dense.bias: True [768]\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight: True [768]\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias: True [768]\n",
      "roberta.encoder.layer.10.intermediate.dense.weight: True [3072, 768]\n",
      "roberta.encoder.layer.10.intermediate.dense.bias: True [3072]\n",
      "roberta.encoder.layer.10.output.dense.weight: True [768, 3072]\n",
      "roberta.encoder.layer.10.output.dense.bias: True [768]\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight: True [768]\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias: True [768]\n",
      "roberta.encoder.layer.11.attention.self.query.weight: True [768, 768]\n",
      "roberta.encoder.layer.11.attention.self.query.bias: True [768]\n",
      "roberta.encoder.layer.11.attention.self.key.weight: True [768, 768]\n",
      "roberta.encoder.layer.11.attention.self.key.bias: True [768]\n",
      "roberta.encoder.layer.11.attention.self.value.weight: True [768, 768]\n",
      "roberta.encoder.layer.11.attention.self.value.bias: True [768]\n",
      "roberta.encoder.layer.11.attention.output.dense.weight: True [768, 768]\n",
      "roberta.encoder.layer.11.attention.output.dense.bias: True [768]\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight: True [768]\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias: True [768]\n",
      "roberta.encoder.layer.11.intermediate.dense.weight: True [3072, 768]\n",
      "roberta.encoder.layer.11.intermediate.dense.bias: True [3072]\n",
      "roberta.encoder.layer.11.output.dense.weight: True [768, 3072]\n",
      "roberta.encoder.layer.11.output.dense.bias: True [768]\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight: True [768]\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias: True [768]\n",
      "classifier.dense.weight: True [768, 768]\n",
      "classifier.dense.bias: True [768]\n",
      "classifier.out_proj.weight: True [2, 768]\n",
      "classifier.out_proj.bias: True [2]\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad} {list(param.shape)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bc41446-b0ea-4ae7-b33c-cf9dbf33ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cecce72f-77b3-46db-b185-b3a279fbd50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff00b2cb-93a4-4595-9904-1ca19c84cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = SQLiteCodeDataset('train_js', tokenizer, 512, lambda x: x[0], lambda x: [1, 0] if x[1] else [0, 1])\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7671d7-8ac6-40c7-a91e-29adda94a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f7778b7-d3ce-4748-a196-27a7ecc408b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ac6941b-748a-4b42-b7f0-b1a471d55d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('lang-classifier5.pt'))\n",
    "optimizer.load_state_dict(torch.load('lang-optimizer5.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29588084-30bb-4e34-8c0f-8f1415763646",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80190e96-1472-443b-b2dd-5598302d4739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Batch 100/10861, Loss: 0.0061"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m i, n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dataloader)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/py/CodeBERT/CodeReviewer/ds.py:33\u001b[0m, in \u001b[0;36mSQLiteCodeDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     32\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM data LIMIT 1 OFFSET ?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     item \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39mfetchone()\n\u001b[1;32m     35\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    dataset = SQLiteCodeDataset('train_js', tokenizer, 512, lambda x: x[0], lambda x: [1, 0] if x[1] else [0, 1])\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    i, n = 0, len(dataloader)\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        size = input_ids.size(0)\n",
    "        i += size\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * size\n",
    "\n",
    "        print(f\"\\rEpoch {epoch+1}/{num_epochs} - Batch {i//batch_size+1}/{n}, Loss: {running_loss/i:.4f}\", end=\"\")\n",
    "        \n",
    "    epoch_loss = running_loss / len(dataset)\n",
    "    print(f\"\\rEpoch {epoch+1}/{num_epochs}, Average Loss: {epoch_loss:.4f}          \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1254bbb7-fc29-4502-8f86-acff3b1b814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cacbdb2f-0864-4572-b1a9-1f686ad43c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'lang-classifier5.pt')\n",
    "torch.save(optimizer.state_dict(), 'lang-optimizer5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "483c3a5a-666c-43d3-a64d-bac163cf1bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:30<00:00,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.980625, Precision: 0.8848484848484849, Recall: 0.9240506329113924, F1: 0.9040247678018577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from filenames import all_files\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from ds import CodeDataset\n",
    "\n",
    "N = 1600  # Number of items you want in your DataLoader\n",
    "\n",
    "val_dataset = CodeDataset(all_files[0:1], '-testjs', tokenizer, 512, lambda x: x[0], lambda x: [1, 0] if x[1] else [0, 1])\n",
    "limited_dataset = Subset(val_dataset, indices=range(N))\n",
    "val_loader = DataLoader(limited_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    predictions, true_labels, list_logits = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "    \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions.extend(torch.argmax(logits, dim=1).tolist())\n",
    "            true_labels.extend(torch.argmax(labels, dim=1).tolist())\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    return accuracy, precision, recall, f1, predictions, true_labels\n",
    "\n",
    "# Example evaluation after training\n",
    "accuracy, precision, recall, f1, predictions, true_labels = evaluate(model, val_loader)\n",
    "print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c8cf88-d800-4e44-9a2f-3d38dc272f08",
   "metadata": {},
   "source": [
    "# Ajouter les code js reconnue par le nouveau model, à la dataset des js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "65c3d0ea-70b7-4cee-b1e2-e50521195c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls-train-chunk-1-extrajs.jsonl exists, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66459it [11:47, 93.99it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7709/66459 are saved to cls-train-chunk-1-extrajs.jsonl\n",
      "cls-train-chunk-2-extrajs.jsonl exists, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66459it [11:49, 93.69it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7889/66459 are saved to cls-train-chunk-2-extrajs.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66459it [11:48, 93.81it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7733/66459 are saved to cls-train-chunk-3-extrajs.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from filenames import train_files\n",
    "\n",
    "for filename in train_files:\n",
    "    savep = f\"{filename}-extra2js.jsonl\"\n",
    "    if os.path.exists(savep):\n",
    "        print(f\"{savep} exists, skipping\")\n",
    "        continue\n",
    "    with open(savep, \"w\") as fw:\n",
    "        with open(f\"{filename}-ready.jsonl\", \"r\") as fr:\n",
    "            i, j = 0, 0\n",
    "            for line in tqdm(fr):\n",
    "                code, label, js0unk_neg = json.loads(line)\n",
    "                i += 1\n",
    "                if js0unk_neg > 0:\n",
    "                    continue # not js\n",
    "                if js0unk_neg < 0:\n",
    "                    encoding = tokenizer.encode_plus(\n",
    "                        code,\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=512,\n",
    "                        return_token_type_ids=False,\n",
    "                        padding='max_length',\n",
    "                        return_attention_mask=True,\n",
    "                        return_tensors='pt',\n",
    "                        truncation=True,\n",
    "                    )\n",
    "                    outputs = model(\n",
    "                        encoding['input_ids'].to(device),\n",
    "                        attention_mask = encoding['attention_mask'].to(device)\n",
    "                    )\n",
    "                    logits = outputs.logits[0]\n",
    "                    if logits[0] > logits[1]:\n",
    "                        continue\n",
    "                j += 1\n",
    "                fw.write(line)\n",
    "            print(f\"{j}/{i} are saved to {savep}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be74bd47-5b5a-4662-89b7-97bafbfa7ab2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'line' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mline\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'line' is not defined"
     ]
    }
   ],
   "source": [
    "line"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
