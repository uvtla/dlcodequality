{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6db99f-03f8-4d9c-b270-11efa25b0cd7",
   "metadata": {},
   "source": [
    "# After getting files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8225e83-c777-4e68-8d88-1c2a82ca195f",
   "metadata": {},
   "source": [
    "## Insert into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00457c64-4536-4a2a-be83-04e1f1e66e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting dataset location\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "try:\n",
    "    dataset\n",
    "except:\n",
    "    print('getting dataset location')\n",
    "    dataset = Path('dataset').absolute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28b7e35-3703-4a56-9707-fadde8c3f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(dataset)\n",
    "\n",
    "js_dir = dataset / 'javascript'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40094612-df9b-4969-bdd3-94b0f201afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9c940ca-f75f-4f96-9960-3c301cdde7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3, json\n",
    "from tqdm import tqdm\n",
    "from contextlib import contextmanager\n",
    "\n",
    "train = js_dir / 'final' / 'jsonl' / 'train'\n",
    "os.chdir(train)\n",
    "\n",
    "@contextmanager\n",
    "def get_cursor(database_name='rsn_train'):\n",
    "    with sqlite3.connect(database_name) as conn:\n",
    "        yield conn.cursor()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b02a9475-3743-4131-bc5b-864a34c129c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "def simple_parse_xml(content, nested):\n",
    "    pattern = r'\\s*<([^\\s]*?)>\\s*'\n",
    "    open = re.search(pattern, content)\n",
    "    if not open:\n",
    "        return [content]\n",
    "    [open_start, open_end] = open.span()\n",
    "    before = content[:open_start]\n",
    "    tag = open.group(1)\n",
    "    inner_and_after = content[open_end:]\n",
    "    close = re.search(f'</{re.escape(tag)}>', inner_and_after)\n",
    "    if close:\n",
    "        [close_start, close_end] = close.span()\n",
    "    else:\n",
    "        close = re.search(pattern, inner_and_after)\n",
    "        if close:\n",
    "            [close_start, _] = close.span()\n",
    "        else:\n",
    "            close_start = len(inner_and_after)\n",
    "        close_end = close_start\n",
    "    inner = inner_and_after[:close_start]\n",
    "    after = inner_and_after[close_end:]\n",
    "    inner = simple_parse_xml(inner, nested) if nested else inner\n",
    "    return [[before, tag, inner], *simple_parse_xml(after, nested)]\n",
    "\n",
    "def atom_to_re(s):\n",
    "    tokens = [token.strip() for token in re.split(r'\\s+|(?=\\W)|(?<=\\W)', s.strip()) if token]\n",
    "    escaped = [regex.escape(token, special_only=True) for token in tokens]\n",
    "    return r'\\s*' + r'\\s*'.join(escaped) + r'\\s*'\n",
    "\n",
    "def str_to_re(s):\n",
    "    codes = re.split(r'\\s*(?://[^\\n]*(?:\\n|$)|/\\*.*?\\*/|\\.{3,})\\s*', s,  flags=re.DOTALL)\n",
    "    return '(?:.*?)'.join(atom_to_re(code) for code in codes)\n",
    "\n",
    "def node_to_re(node, c):\n",
    "    if type(node) == str:\n",
    "        return str_to_re(node), []\n",
    "    before, tag, content = node\n",
    "    before_re = str_to_re(before)\n",
    "    c[0] += 1\n",
    "    open_gr = c[0]\n",
    "    content_re, content_tags = make_regex(content, c)\n",
    "    c[0] += 1\n",
    "    close_gr = c[0]\n",
    "    open_re = '\\s*(|<'+re.escape(tag)+'>)\\s*'\n",
    "    close_re = '\\s*(|</'+re.escape(tag)+'>)\\s*'\n",
    "    return before_re+open_re+content_re+close_re, [(tag, open_gr, close_gr, content_tags)]\n",
    "\n",
    "\n",
    "def make_regex(tree, c):\n",
    "    regs, tags = zip(*(node_to_re(node, c) for node in tree))\n",
    "    return re.sub(r'(\\\\s\\*)+', r'\\\\s*', ''.join(regs)), [t for tag in tags for t in tag] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904bebf-5087-4d02-ad73-5e4f5aac1cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re\n",
    "omitted = []\n",
    "output = []\n",
    "for text_file in js_dir.glob('thread2-*.txt'):\n",
    "    outfile = str.replace(str(text_file), '.txt', '.json')\n",
    "    if os.path.exists(outfile):\n",
    "        continue\n",
    "    with open(text_file, 'r') as file:\n",
    "        file_contents = file.read()\n",
    "    parsed = simple_parse_xml(file_contents, nested=False)\n",
    "    output.append({\n",
    "        'file': str(text_file),\n",
    "        'split': [p[0] for p in parsed[:-1]] + [parsed[-1]],\n",
    "        'len': len(parsed),\n",
    "        'ok': len(parsed) == 11\n",
    "    })\n",
    "    if len(parsed) != 11:\n",
    "        omitted.append(text_file)\n",
    "        continue\n",
    "    with open(outfile, 'w') as file:\n",
    "        file.write(json.dumps([p[1:] for p in parsed]))\n",
    "\n",
    "with open('parse3.log', 'w') as file:\n",
    "    file.write(json.dumps(output))\n",
    "for out in output:\n",
    "    if not out['ok']:\n",
    "        continue\n",
    "    print(out['file'])\n",
    "    print(out['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709bdefe-5748-4a18-8be6-ae73ddaeead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    # cursor.execute('drop table if exists snippets')\n",
    "    # cursor.execute('drop table if exists region')\n",
    "    cursor.execute('create table if not exists snippets (ID INTEGER PRIMARY KEY, idx INTEGER, code TEXT, locations JSON, regions JSON, SRP boolean)')\n",
    "    cursor.execute('create table if not exists region (ID INTEGER PRIMARY KEY, code TEXT, vector JSON)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8996c197-897d-4d14-bb18-1034b3a38b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_region(code):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute('insert into region (code) values (?)', (code, ))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "def insert_snippet(id, index, code, locations, regions, srp):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute(\n",
    "            'insert into snippets (ID, idx, code, locations, regions, SRP) values (?, ?, ?, ?, ?, ?)',\n",
    "            (id, index, code, json.dumps(locations), json.dumps(regions), srp))\n",
    "\n",
    "def flat_wrong_tags(tags, code, m):\n",
    "    clean_tags = []\n",
    "    for tag in tags:\n",
    "        clean_tags += flat_wrong_tag(tag, clean_tags, code, m)\n",
    "    return clean_tags\n",
    "\n",
    "def flat_wrong_tag(tag, clean_tags, code, m):\n",
    "    name, open, close, sub_tags = tag\n",
    "    clean_sub_tags = flat_wrong_tags(sub_tags, code, m)\n",
    "    if m.group(open):\n",
    "        if len(clean_tags):\n",
    "            pname, popen, pclose, psub_tags = clean_tags[-1]\n",
    "            if not m.group(pclose):\n",
    "                clean_tags[-1] = (pname, popen, close, psub_tags)\n",
    "        return clean_sub_tags\n",
    "    return [(name, open, close, clean_sub_tags)]\n",
    "\n",
    "def tag_to_json(tag, code, m, handle_region):\n",
    "    name, open, close, sub_tags = tag\n",
    "    start = m.span(open)[0]\n",
    "    regions, body, end = tags_to_json(start, sub_tags, code, m, handle_region)\n",
    "    body += code[end:m.span(close)[0]] \n",
    "    region_id = handle_region(f'function {name} () {{\\n{body}\\n}}')\n",
    "    regions = [(start, region_id)] + regions\n",
    "    return regions\n",
    "\n",
    "\n",
    "def tags_to_json(outer_index, tags, code, m, handle_region):\n",
    "    regions = []\n",
    "    outer_body = ''\n",
    "    for tag in tags:\n",
    "        name, open, close, _ = tag\n",
    "        outer_body += code[outer_index:m.span(open)[0]] + '\\n' + name + '();\\n'\n",
    "        outer_index = m.span(close)[0]\n",
    "        regions += tag_to_json(tag, code, m, handle_region)\n",
    "    return regions, outer_body, outer_index\n",
    "\n",
    "\n",
    "def to_json(tags, code, m, handle_region):\n",
    "    regions, body, end = tags_to_json(0, tags, code, m, handle_region)\n",
    "    body += code[end:] \n",
    "    region_id = handle_region(body)\n",
    "    regions = [(0, region_id)] + regions\n",
    "    if len(regions) > 1:\n",
    "        regions.append((end, region_id))\n",
    "    return regions\n",
    "\n",
    "def strip_js_comments(js_code):\n",
    "    js_code = re.sub(r'\\n?//.*?\\n', '\\n', js_code)\n",
    "    js_code = re.sub(r'/\\*.*?\\*/', '', js_code, flags=re.DOTALL)\n",
    "    return js_code    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aca0325d-5b5d-43ad-9649-e4c60ccd872a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8780487804878049"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(js_dir.glob('thread2-*.json')))/len(list(js_dir.glob('thread2-*.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194654e4-f22c-452e-9dfb-5a953fb6a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ok, n_all = 0, 0\n",
    "for text_file in tqdm(list(js_dir.glob('thread2-*.json'))):\n",
    "    index = int(re.match('.*thread2-(.*)\\.json', str(text_file)).group(1))\n",
    "    limit = 10\n",
    "    with get_cursor() as cursor:\n",
    "        codes = list(cursor.execute('select id, code from shuffled limit ? offset ?', (limit, limit * index + 1)))\n",
    "    with open(text_file, 'r') as file:\n",
    "        file_contents = file.read()\n",
    "    for (id, code), obj in zip(codes, json.loads(file_contents)):\n",
    "        original = strip_js_comments(code)\n",
    "        xml = obj[1]\n",
    "        reg_str, tags = make_regex(simple_parse_xml(xml, True), [0])\n",
    "        reg = regex.compile(reg_str, flags = regex.DOTALL)\n",
    "        m = reg.match(original)\n",
    "        n_all += 1\n",
    "        if m:\n",
    "            n_ok += 1\n",
    "            tags = flat_wrong_tags(tags, original, m)\n",
    "            if len(tags) == 1 and not len(tags[0][3]):\n",
    "                tags = []\n",
    "            regions = to_json(tags, original, m, insert_region)\n",
    "            insert_snippet(id, index, original, *zip(*regions), len(tags) == 0)\n",
    "            \n",
    "            \n",
    "print(f'{n_ok}/{n_all}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a65ab-ab7c-4efe-a4d3-2f0f5145c12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    print(list(cursor.execute('select idx < 650, count(*), SRP from snippets group by idx < 650, SRP')))\n",
    "    print(list(cursor.execute('select count(*) from region')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fee2198e-ec9b-46ee-a700-4a102de567d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8663b41d-56f6-4ebd-ba46-8efbfe5be2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(ids):\n",
    "    with get_cursor() as cursor:\n",
    "        regions = list(cursor.execute(f'select id, code from region where id in ({ids})'))\n",
    "    for region in regions:\n",
    "        id, code = region\n",
    "        tokenized_inputs = tokenizer([code], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        tokenized_inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokenized_inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        #average_hidden_states = last_hidden_states.mean(dim=1)\n",
    "        yield id, list(last_hidden_states.cpu().numpy()[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5e81adab-31e7-478f-8f0c-9bc251b82e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6247\n"
     ]
    }
   ],
   "source": [
    "with get_cursor() as cursor:\n",
    "    print(next(cursor.execute('select count(*) from snippets'))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211bdf4d-2c35-4cd9-81b6-8f3775c59e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('drop table if exists tokenized')\n",
    "    cursor.execute('create table if not exists tokenized (ID INTEGER PRIMARY KEY, input_ids JSON, region_ids JSON)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655692b4-a8bc-4aeb-95f4-9a23fedb1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    snippets = list(cursor.execute('select id, code, locations, regions from snippets where idx < 650'))\n",
    "for (id, code, locations, regions) in tqdm(snippets):\n",
    "    tokens = tokenizer.encode_plus(code, truncation=True, return_offsets_mapping=True)\n",
    "    offset_mapping = tokens['offset_mapping']\n",
    "    regions, locations = json.loads(regions),json.loads(locations)\n",
    "    if not len(regions):\n",
    "        continue\n",
    "    locations.append(len(code))\n",
    "    i = 0\n",
    "    region_ids = []\n",
    "    for (start, end) in offset_mapping[1:-1]:\n",
    "        while start > locations[i+1]:\n",
    "            i += 1\n",
    "        region_ids.append(regions[i])\n",
    "    region_ids = [0] + region_ids + [0]\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute(\n",
    "            'insert into tokenized (ID, input_ids, region_ids) values (?, ?, ?)',\n",
    "            (id, json.dumps(tokens['input_ids']), json.dumps(region_ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bef707b-169d-4458-9f8c-18e57fd47407",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('drop table if exists train_snippets')\n",
    "    cursor.execute('create table train_snippets as select * from snippets where idx < 650')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5569bc6b-d6e2-453d-8dab-c16d2cca3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('create table if not exists current_batch(idx INTEGER)')\n",
    "    if not len(list(cursor.execute('select * from current_batch'))):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78bc3235-55f3-4130-bc94-40624dd4a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('create table if not exists last_batch(idx INTEGER, epoch_idx INTEGER)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fda53217-c701-4ded-9c2b-c8f63e77a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def set_last_batch(last_batch, epoch_idx):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute('update last_batch set idx = ?, epoch_idx = ?', (last_batch, epoch_idx))\n",
    "\n",
    "def get_unsafe_last_batch():\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute('select * from last_batch')\n",
    "        item = cursor.fetchone()\n",
    "        if not item:\n",
    "            cursor.execute('insert into last_batch values (0, 0)')\n",
    "            return 0\n",
    "        else:\n",
    "            return item\n",
    "\n",
    "def get_last_batch(num_batches):\n",
    "    last_batch, last_epoch = get_unsafe_last_batch()\n",
    "    if last_batch >= num_batches:\n",
    "        last_batch = 0\n",
    "        last_epoch += 1\n",
    "        set_last_batch(0, last_epoch)\n",
    "    return last_batch, last_epoch\n",
    "        \n",
    "def get_batch_tokens(batch_size, i):\n",
    "    with get_cursor() as cursor:\n",
    "        ids = ','.join(\n",
    "            str(x[0]) for x in cursor.execute('select id from train_snippets_shuffled limit ? offset ?', (batch_size, i * batch_size))\n",
    "        )\n",
    "        tokens = cursor.execute(f'select input_ids, region_ids from tokenized where id in ({ids})')\n",
    "        return [(json.loads(input_ids), json.loads(region_ids)) for (input_ids, region_ids) in tokens]\n",
    "        \n",
    "\n",
    "def get_batch_regions(tokens):\n",
    "    with get_cursor() as cursor:\n",
    "        region_ids_str = ','.join(set(str(region_id) for (_, region_ids) in tokens for region_id in region_ids))\n",
    "        return dict(get_labels(region_ids_str))\n",
    "\n",
    "def prepare_iteration(input_ids, region_ids, regions):\n",
    "    label_size = 768\n",
    "    size = len(input_ids)\n",
    "    if size > 512:\n",
    "        print(size)\n",
    "    empties = 512 - size\n",
    "    null_vector = [0] * label_size\n",
    "    for r_id in region_ids:\n",
    "        if r_id and not (r_id in regions):\n",
    "            print(r_id)\n",
    "    label = [regions[r_id] if r_id else null_vector for r_id in region_ids]\n",
    "    label += [null_vector] * empties\n",
    "    input_ids += [0] * empties\n",
    "    attention = [1] * size + [0] * empties\n",
    "    try:\n",
    "        torch.FloatTensor(label)\n",
    "    except:\n",
    "        print(regions[region_ids[1]])\n",
    "\n",
    "    return input_ids, attention, label\n",
    "\n",
    "def get_batch_data(batch_size, i):\n",
    "    tokens = get_batch_tokens(batch_size, i)\n",
    "    regions = get_batch_regions(tokens)\n",
    "    batch = [prepare_iteration(input_ids, region_ids, regions) for (input_ids, region_ids) in tokens]\n",
    "    input_ids, attention, label = zip(*batch)\n",
    "    return (torch.IntTensor(input_ids), torch.FloatTensor(attention), torch.FloatTensor(label)), tokens\n",
    "\n",
    "\n",
    "def get_epoch_part(i0, num_batches, batch_size):\n",
    "    return ((i, get_batch_data(batch_size, i)) for i in range(i0, num_batches, 1))\n",
    "\n",
    "def tee(text):\n",
    "    print(f\"\\r{text}\\r\")\n",
    "    with open('train-resnet.log', 'a') as file:\n",
    "        file.write(f\"{text}\\n\")\n",
    "\n",
    "def log(title, start_time, sizes, epoch, i, mean_loss, loss):\n",
    "    _, num_epochs, num_batches = sizes\n",
    "    i += 1\n",
    "    dt = time.time() - start_time\n",
    "    elapsed = timedelta(seconds=int(dt))\n",
    "    remaining = timedelta(seconds=int(dt*(num_batches-i)/i))\n",
    "    text = f\"{title}: {elapsed}<{remaining} Epoch {epoch+1}/{num_epochs} - Batch {i}/{num_batches}, Loss: {mean_loss:.4f} {loss.item():.4f}\"\n",
    "    tee(text)\n",
    "\n",
    "def save(epoch, i):\n",
    "    torch.save(model.state_dict(), f'searchnet-bcemodel-{epoch}-{i}.pt')\n",
    "    torch.save(optimizer.state_dict(), f'searchnet-bceoptimizer-{epoch}-{i}.pt')\n",
    "    set_last_batch(i, epoch)\n",
    "    tee(f'saving searchnet-bcemodel-{epoch}-{i}.pt\\n')\n",
    "\n",
    "def compute_loss(batch_data):\n",
    "    batch_vectors, batch_tokens = batch_data\n",
    "    input_ids, attention, labels = (c.to(device) for c in batch_vectors)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention)\n",
    "    return loss_fn(outputs.last_hidden_state, labels)\n",
    "\n",
    "\n",
    "def handle_train_batch(i, epoch, batch_data, performance, sizes):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = compute_loss(batch_data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    min_alpha, alpha, running_loss, start_time = performance\n",
    "\n",
    "    current_alpha = min_alpha + alpha\n",
    "    running_loss = (1 - current_alpha) * running_loss + current_alpha * loss.item()\n",
    "    alpha *= .5\n",
    "    \n",
    "    log('training', start_time, sizes, epoch, i, running_loss, loss)\n",
    "    if (i+1) % 100 == 0:\n",
    "        save(epoch, i)\n",
    "    return running_loss, alpha\n",
    "    \n",
    "def handle_val_batch(i0, i, epoch, batch_data, start_time, cum_loss, sizes):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss = compute_loss(batch_data)\n",
    "    \n",
    "    cum_loss += loss.item()\n",
    "    mean_loss = cum_loss / (i - i0 + 1)\n",
    "    \n",
    "    log('validation', start_time, sizes, epoch, i, mean_loss, loss)\n",
    "    return cum_loss\n",
    "    \n",
    "\n",
    "def handle_epoch(batch_size, num_epochs):\n",
    "    with get_cursor() as cursor:\n",
    "        epoch_size = next(cursor.execute('select count(*) from train_snippets'))[0]\n",
    "    num_batches = (epoch_size - 1) // batch_size + 1\n",
    "    train_batchs = 4 * num_batches // 5\n",
    "    i0, epoch0 = get_last_batch(train_batchs)\n",
    "    num_epochs += epoch0\n",
    "    i = i0 - 1\n",
    "    min_alpha = .2\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    alpha = 1 - min_alpha\n",
    "\n",
    "    model.train()\n",
    "    sizes = (train_batchs, num_epochs, num_batches)\n",
    "    tee('\\nTraining\\n')\n",
    "    for (i, batch_data) in get_epoch_part(i0, train_batchs, batch_size):\n",
    "        performance = (min_alpha, alpha, running_loss, start_time)\n",
    "        running_loss, alpha = handle_train_batch(i, epoch, batch_data, performance, sizes)\n",
    "    \n",
    "    \n",
    "    i0 = i + 1\n",
    "    save(epoch, i0)\n",
    "    \n",
    "    model.eval()\n",
    "    cum_loss = 0\n",
    "    tee('\\nValidation:\\n')\n",
    "    for (i, batch_data) in get_epoch_part(i0, num_batches, batch_size):\n",
    "        cum_loss = handle_val_batch(i0, i, epoch, batch_data, start_time, cum_loss, sizes)\n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59af576d-6e6b-4f4d-910d-ff23dab8cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_last_batch(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df189e27-706e-4ee0-81e2-ef57d1522b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training\n",
      "\n",
      "training: 0:00:02<0:14:31 Epoch 1/10 - Batch 1/320, Loss: 0.3166 0.3166\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m set_last_batch(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mhandle_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#model.load_state_dict(torch.load('searchnet-bcemodel-0-200.pt'))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 147\u001b[0m, in \u001b[0;36mhandle_epoch\u001b[0;34m(batch_size, num_epochs)\u001b[0m\n\u001b[1;32m    145\u001b[0m sizes \u001b[38;5;241m=\u001b[39m (train_batchs, num_epochs, num_batches)\n\u001b[1;32m    146\u001b[0m tee(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (i, batch_data) \u001b[38;5;129;01min\u001b[39;00m get_epoch_part(i0, train_batchs, batch_size):\n\u001b[1;32m    148\u001b[0m     performance \u001b[38;5;241m=\u001b[39m (min_alpha, alpha, running_loss, start_time)\n\u001b[1;32m    149\u001b[0m     running_loss, alpha \u001b[38;5;241m=\u001b[39m handle_train_batch(i, epoch, batch_data, performance, sizes)\n",
      "Cell \u001b[0;32mIn[23], line 70\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_epoch_part\u001b[39m(i0, num_batches, batch_size):\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ((i, \u001b[43mget_batch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i0, num_batches, \u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[23], line 63\u001b[0m, in \u001b[0;36mget_batch_data\u001b[0;34m(batch_size, i)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_batch_data\u001b[39m(batch_size, i):\n\u001b[1;32m     62\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m get_batch_tokens(batch_size, i)\n\u001b[0;32m---> 63\u001b[0m     regions \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch_regions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     batch \u001b[38;5;241m=\u001b[39m [prepare_iteration(input_ids, region_ids, regions) \u001b[38;5;28;01mfor\u001b[39;00m (input_ids, region_ids) \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[1;32m     65\u001b[0m     input_ids, attention, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n",
      "Cell \u001b[0;32mIn[23], line 38\u001b[0m, in \u001b[0;36mget_batch_regions\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_cursor() \u001b[38;5;28;01mas\u001b[39;00m cursor:\n\u001b[1;32m     37\u001b[0m     region_ids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mstr\u001b[39m(region_id) \u001b[38;5;28;01mfor\u001b[39;00m (_, region_ids) \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mfor\u001b[39;00m region_id \u001b[38;5;129;01min\u001b[39;00m region_ids))\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mget_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregion_ids_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mget_labels\u001b[0;34m(ids)\u001b[0m\n\u001b[1;32m      7\u001b[0m tokenized_inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenized_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m last_hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#average_hidden_states = last_hidden_states.mean(dim=1)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py:835\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    826\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    828\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    829\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    830\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    833\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    834\u001b[0m )\n\u001b[0;32m--> 835\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    848\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    513\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    514\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    515\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m         output_attentions,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py:413\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    403\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py:349\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    332\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    339\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    340\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    341\u001b[0m         hidden_states,\n\u001b[1;32m    342\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m         output_attentions,\n\u001b[1;32m    348\u001b[0m     )\n\u001b[0;32m--> 349\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py:299\u001b[0m, in \u001b[0;36mRobertaSelfOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    298\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 299\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1266\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "set_last_batch(0, 0)\n",
    "for epoch in range(num_epochs):\n",
    "    handle_epoch(batch_size, num_epochs)\n",
    "\n",
    "#model.load_state_dict(torch.load('searchnet-bcemodel-0-200.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78b5da4b-c5b6-4ced-9c0a-700d7ecc7a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_last_batch(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "932bf29a-b41d-48c4-be7f-a4e61abfc46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import torch\n",
    "device = torch.device('cuda')\n",
    "model = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e20882d-06a2-40de-96e1-645b661e1192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# Function to find the best matching between predicted and true labels\n",
    "def best_label_matching(y_true, y_pred):\n",
    "    D = np.max([y_pred.max(), y_true.max()]) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
    "    return list(zip(row_ind, col_ind))\n",
    "\n",
    "\n",
    "\n",
    "def compute_accuracy(data, true_labels):\n",
    "\n",
    "\n",
    "    unique_labels = np.unique(true_labels)\n",
    "    label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    true_labels = np.array([label_mapping[label] for label in true_labels])\n",
    "\n",
    "    \n",
    "    # Perform Hierarchical Agglomerative Clustering\n",
    "    Z = linkage(pdist(data), method='ward')\n",
    "    predicted_labels = fcluster(Z, t=len(set(true_labels)), criterion='maxclust')\n",
    "    \n",
    "    \n",
    "    # Find the best matching\n",
    "    ind = best_label_matching(true_labels, predicted_labels)\n",
    "    ind_dict = {i[0]: i[1] for i in ind}\n",
    "    \n",
    "    # Remap predicted labels to best matching true labels\n",
    "    remapped_predicted_labels = np.array([ind_dict.get(label, 0) for label in predicted_labels])\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    return accuracy_score(true_labels, remapped_predicted_labels)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a962ea4-5b19-4f37-972a-5985e53131e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.rand(512, 786)  # Replace with your actual data\n",
    "true_labels = np.random.randint(5, size=512)  # Replace with your actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0d7d52f-4906-4bdf-b6f9-d93446126463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.26171875\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {compute_accuracy(data, true_labels+10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a20d780a-15f0-4ab0-9f82-3a2c4e1c8d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "d = []\n",
    "def get_batcgh(batch_size, i):\n",
    "    batch, tokens = get_batch(batch_size, i)\n",
    "    input_ids, attention, label = (c.to(device) for c in batch)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention)[0]\n",
    "        data = outputs.cpu()[0].numpy()\n",
    "        labels = np.array(tokens[0][0])\n",
    "        d.append((data, labels, input_ids))\n",
    "        print(labels.shape)\n",
    "        #print(compute_accuracy(outputs.cpu()[0], np.array(tokens[0][0])))\n",
    "get_batcgh(1, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fab2ca97-c058-4356-ac5a-57d932952fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 786)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52c2711c-a137-45d0-9251-06df13a6e74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a3b92e1c-6721-44a5-9f17-524f733f3710",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data, true_labels, input_ids), = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4fe67bd-5f25-45bb-9e8b-a59334e78a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([0, 35435, 29857,   225, 45566,    36, 30766,     6,  5581,    43,\n",
    "         25522, 50118,  1437,   114, 48209, 48222,     4,   354, 48222,  1640,\n",
    "         30766, 35122, 25522, 50118,  1437,  1437,  1437,  3211,    92,  7773,\n",
    "         30192, 45803,  9089, 23088,   923,     7,    28,    41,  8932, 27645,\n",
    "         50118,  1437, 35524, 50140,  1437,   671, 29857,   225,  7605, 45566,\n",
    "          1640, 30766,     6,  5581,    43, 50118, 24303,     2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4dc09c7f-0ddc-47af-be38-f4c17b996445",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:58]\n",
    "true_labels = true_labels[:58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "494dde43-fa47-4cc6-8eac-0080e9ba575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = np.unique(true_labels)\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "true_labels = np.array([label_mapping[label] for label in true_labels])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da9bc09c-d573-41d6-8319-8a004a235836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(true_labels[:58])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a4b94a73-e426-4504-8753-975cae4d5901",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(pdist(data), method='ward')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2b786ff3-31a5-4706-a61a-eb79d9c64a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16, 28, 14, 15, 12, 35,  9, 30, 11, 31, 33, 17, 20, 23, 36, 10, 38,\n",
       "       26,  6, 37,  8, 31, 33, 17, 19, 19, 20, 23, 24, 21, 27, 39,  1,  2,\n",
       "       22,  3,  5,  4,  7, 32, 18, 20, 34, 18, 20, 25, 14, 15, 29, 13, 37,\n",
       "        8, 30, 11, 31, 18, 34, 16], dtype=int32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = fcluster(Z, t=len(set(true_labels)), criterion='maxclust')\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2094d89b-330e-412a-a26a-cf1cd0717f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ind = best_label_matching(true_labels, predicted_labels)\n",
    "ind_dict = {i[0]: i[1] for i in ind}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d493a7ca-9108-454e-a6fe-fa47c1d828ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "remapped_predicted_labels = np.array([ind_dict.get(label, 0) for label in predicted_labels])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "208166f6-98fb-4590-a915-7584eea06867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 31, 27, 11, 33,  6, 30,  3, 18,  8, 25, 37, 15, 10, 35, 39,  2,\n",
       "       12, 36, 16, 29,  8, 25, 37, 17, 17, 15, 10,  9, 20, 28, 34, 22, 23,\n",
       "       14,  4,  5,  7, 21, 26, 38, 15, 24, 38, 15, 13, 27, 11, 19, 32, 16,\n",
       "       29,  3, 18,  8, 38, 24,  1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remapped_predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9c731101-d3e4-42ff-ba7f-a581abfd95a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8103448275862069"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(true_labels, remapped_predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff47af24-e147-4608-8445-3634f0482eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeSegmentationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.codebert = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "        self.conv = nn.Conv1d(in_channels=768, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bilstm = nn.LSTM(input_size=256, hidden_size=128, num_layers=1, bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get embeddings from CodeBERT\n",
    "        embeddings = self.codebert(input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        \n",
    "        # Mask padding embeddings\n",
    "        padding_mask = (input_ids != padding_token_id)  # Define padding_token_id\n",
    "        embeddings[~padding_mask] = 0\n",
    "\n",
    "        # Pass through additional layers\n",
    "        embeddings = embeddings.transpose(1, 2)  # Needed for Conv1d\n",
    "        conv_out = self.conv(embeddings).transpose(1, 2)\n",
    "        lstm_out, _ = self.bilstm(conv_out)\n",
    "        # Further processing and return\n",
    "\n",
    "model = CodeSegmentationModel(model)  # Initialize with pre-trained CodeBERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ad4722b-a048-4a19-ab54-9b8fab23af24",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
