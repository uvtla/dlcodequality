{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6db99f-03f8-4d9c-b270-11efa25b0cd7",
   "metadata": {},
   "source": [
    "# After getting files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8225e83-c777-4e68-8d88-1c2a82ca195f",
   "metadata": {},
   "source": [
    "## Insert into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00457c64-4536-4a2a-be83-04e1f1e66e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting dataset location\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "try:\n",
    "    dataset\n",
    "except:\n",
    "    print('getting dataset location')\n",
    "    dataset = Path('dataset').absolute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28b7e35-3703-4a56-9707-fadde8c3f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(dataset)\n",
    "\n",
    "js_dir = dataset / 'javascript'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40094612-df9b-4969-bdd3-94b0f201afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904bebf-5087-4d02-ad73-5e4f5aac1cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "omitted = []\n",
    "output = []\n",
    "for text_file in js_dir.glob('thread2-*.txt'):\n",
    "    with open(text_file, 'r') as file:\n",
    "        file_contents = file.read()\n",
    "    parsed = simple_parse_xml(file_contents, nested=False)\n",
    "    output.append({\n",
    "        'file': str(text_file),\n",
    "        'split': [p[0] for p in parsed[:-1]] + [parsed[-1]],\n",
    "        'len': len(parsed),\n",
    "        'ok': len(parsed) == 11\n",
    "    })\n",
    "    if len(parsed) != 11:\n",
    "        omitted.append(text_file)\n",
    "        continue\n",
    "    with open(str.replace(str(text_file), '.txt', '.json'), 'w') as file:\n",
    "        file.write(json.dumps([p[1:] for p in parsed]))\n",
    "\n",
    "with open('parse2.log', 'w') as file:\n",
    "    file.write(json.dumps(output))\n",
    "for out in output:\n",
    "    if not out['ok']:\n",
    "        continue\n",
    "    print(out['file'])\n",
    "    print(out['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c9c940ca-f75f-4f96-9960-3c301cdde7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3, json\n",
    "from tqdm import tqdm\n",
    "from contextlib import contextmanager\n",
    "\n",
    "train = js_dir / 'final' / 'jsonl' / 'train'\n",
    "os.chdir(train)\n",
    "\n",
    "@contextmanager\n",
    "def get_cursor(database_name='rsn_train'):\n",
    "    with sqlite3.connect(database_name) as conn:\n",
    "        yield conn.cursor()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b02a9475-3743-4131-bc5b-864a34c129c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "def simple_parse_xml(content, nested):\n",
    "    pattern = r'\\s*<([^\\s]*?)>\\s*'\n",
    "    open = re.search(pattern, content)\n",
    "    if not open:\n",
    "        return [content]\n",
    "    [open_start, open_end] = open.span()\n",
    "    before = content[:open_start]\n",
    "    tag = open.group(1)\n",
    "    inner_and_after = content[open_end:]\n",
    "    close = re.search(f'</{re.escape(tag)}>', inner_and_after)\n",
    "    if close:\n",
    "        [close_start, close_end] = close.span()\n",
    "    else:\n",
    "        close = re.search(pattern, inner_and_after)\n",
    "        if close:\n",
    "            [close_start, _] = close.span()\n",
    "        else:\n",
    "            close_start = len(inner_and_after)\n",
    "        close_end = close_start\n",
    "    inner = inner_and_after[:close_start]\n",
    "    after = inner_and_after[close_end:]\n",
    "    inner = simple_parse_xml(inner, nested) if nested else inner\n",
    "    return [[before, tag, inner], *simple_parse_xml(after, nested)]\n",
    "\n",
    "def atom_to_re(s):\n",
    "    tokens = [token.strip() for token in re.split(r'\\s+|(?=\\W)|(?<=\\W)', s.strip()) if token]\n",
    "    escaped = [regex.escape(token, special_only=True) for token in tokens]\n",
    "    return r'\\s*' + r'\\s*'.join(escaped) + r'\\s*'\n",
    "\n",
    "def str_to_re(s):\n",
    "    codes = re.split(r'\\s*(?://[^\\n]*(?:\\n|$)|/\\*.*?\\*/|\\.{3,})\\s*', s,  flags=re.DOTALL)\n",
    "    return '(?:.*?)'.join(atom_to_re(code) for code in codes)\n",
    "\n",
    "def node_to_re(node, c):\n",
    "    if type(node) == str:\n",
    "        return str_to_re(node), []\n",
    "    before, tag, content = node\n",
    "    before_re = str_to_re(before)\n",
    "    c[0] += 1\n",
    "    open_gr = c[0]\n",
    "    content_re, content_tags = make_regex(content, c)\n",
    "    c[0] += 1\n",
    "    close_gr = c[0]\n",
    "    open_re = '\\s*(|<'+re.escape(tag)+'>)\\s*'\n",
    "    close_re = '\\s*(|</'+re.escape(tag)+'>)\\s*'\n",
    "    return before_re+open_re+content_re+close_re, [(tag, open_gr, close_gr, content_tags)]\n",
    "\n",
    "\n",
    "def make_regex(tree, c):\n",
    "    regs, tags = zip(*(node_to_re(node, c) for node in tree))\n",
    "    return re.sub(r'(\\\\s\\*)+', r'\\\\s*', ''.join(regs)), [t for tag in tags for t in tag] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "id": "709bdefe-5748-4a18-8be6-ae73ddaeead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('drop table if exists snippets')\n",
    "    cursor.execute('drop table if exists region')\n",
    "    cursor.execute('create table if not exists snippets (ID INTEGER PRIMARY KEY, idx INTEGER, code TEXT, locations JSON, regions JSON, SRP boolean)')\n",
    "    cursor.execute('create table if not exists region (ID INTEGER PRIMARY KEY, code TEXT, vector JSON)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "id": "8996c197-897d-4d14-bb18-1034b3a38b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_region(code):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute('insert into region (code) values (?)', (code, ))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "def insert_snippet(id, index, code, locations, regions, srp):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute(\n",
    "            'insert into snippets (ID, idx, code, locations, regions, SRP) values (?, ?, ?, ?, ?, ?)',\n",
    "            (id, index, code, json.dumps(locations), json.dumps(regions), srp))\n",
    "\n",
    "def flat_wrong_tags(tags, code, m):\n",
    "    clean_tags = []\n",
    "    for tag in tags:\n",
    "        clean_tags += flat_wrong_tag(tag, clean_tags, code, m)\n",
    "    return clean_tags\n",
    "\n",
    "def flat_wrong_tag(tag, clean_tags, code, m):\n",
    "    name, open, close, sub_tags = tag\n",
    "    clean_sub_tags = flat_wrong_tags(sub_tags, code, m)\n",
    "    if m.group(open):\n",
    "        if len(clean_tags):\n",
    "            pname, popen, pclose, psub_tags = clean_tags[-1]\n",
    "            if not m.group(pclose):\n",
    "                clean_tags[-1] = (pname, popen, close, psub_tags)\n",
    "        return clean_sub_tags\n",
    "    return [(name, open, close, clean_sub_tags)]\n",
    "\n",
    "def tag_to_json(tag, code, m, handle_region):\n",
    "    name, open, close, sub_tags = tag\n",
    "    start = m.span(open)[0]\n",
    "    regions, body, end = tags_to_json(start, sub_tags, code, m, handle_region)\n",
    "    body += code[end:m.span(close)[0]] \n",
    "    region_id = handle_region(f'function {name} () {{\\n{body}\\n}}')\n",
    "    regions = [(start, region_id)] + regions\n",
    "    return regions\n",
    "\n",
    "\n",
    "def tags_to_json(outer_index, tags, code, m, handle_region):\n",
    "    regions = []\n",
    "    outer_body = ''\n",
    "    for tag in tags:\n",
    "        name, open, close, _ = tag\n",
    "        outer_body += code[outer_index:m.span(open)[0]] + '\\n' + name + '();\\n'\n",
    "        outer_index = m.span(close)[0]\n",
    "        regions += tag_to_json(tag, code, m, handle_region)\n",
    "    return regions, outer_body, outer_index\n",
    "\n",
    "\n",
    "def to_json(tags, code, m, handle_region):\n",
    "    regions, body, end = tags_to_json(0, tags, code, m, handle_region)\n",
    "    body += code[end:] \n",
    "    region_id = handle_region(body)\n",
    "    regions = [(0, region_id)] + regions\n",
    "    if len(regions) > 1:\n",
    "        regions.append((end, region_id))\n",
    "    return regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "id": "194654e4-f22c-452e-9dfb-5a953fb6a0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 677/677 [02:49<00:00,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6183/6770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def strip_js_comments(js_code):\n",
    "    js_code = re.sub(r'\\n?//.*?\\n', '\\n', js_code)\n",
    "    js_code = re.sub(r'/\\*.*?\\*/', '', js_code, flags=re.DOTALL)\n",
    "    return js_code    \n",
    "\n",
    "n_ok, n_all = 0, 0\n",
    "for text_file in tqdm(list(js_dir.glob('thread2-*.json'))):\n",
    "    index = int(re.match('.*thread2-(.*)\\.json', str(text_file)).group(1))\n",
    "    limit = 10\n",
    "    with get_cursor() as cursor:\n",
    "        codes = list(cursor.execute('select id, code from shuffled limit ? offset ?', (limit, limit * index + 1)))\n",
    "    with open(text_file, 'r') as file:\n",
    "        file_contents = file.read()\n",
    "    for (id, code), obj in zip(codes, json.loads(file_contents)):\n",
    "        original = strip_js_comments(code)\n",
    "        xml = obj[1]\n",
    "        reg_str, tags = make_regex(simple_parse_xml(xml, True), [0])\n",
    "        reg = regex.compile(reg_str, flags = regex.DOTALL)\n",
    "        m = reg.match(original)\n",
    "        n_all += 1\n",
    "        if m:\n",
    "            n_ok += 1\n",
    "            tags = flat_wrong_tags(tags, original, m)\n",
    "            if len(tags) == 1 and not len(tags[0][3]):\n",
    "                tags = []\n",
    "            regions = to_json(tags, original, m, insert_region)\n",
    "            insert_snippet(id, index, original, *zip(*regions), False)\n",
    "            \n",
    "            \n",
    "print(f'{n_ok}/{n_all}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a62a65ab-ab7c-4efe-a4d3-2f0f5145c12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 533, 0), (0, 551, 1), (1, 2510, 0), (1, 2589, 1)]\n",
      "[(13709,)]\n"
     ]
    }
   ],
   "source": [
    "with get_cursor() as cursor:\n",
    "    print(list(cursor.execute('select idx < 650, count(*), SRP from snippets group by idx < 650, SRP')))\n",
    "    print(list(cursor.execute('select count(*) from region')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fee2198e-ec9b-46ee-a700-4a102de567d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8663b41d-56f6-4ebd-ba46-8efbfe5be2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13709 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 70, 768])\n",
      "torch.Size([768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13709 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(average_hidden_states\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     12\u001b[0m cursor\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselect id, vector from region where json_array_length(vector) > 768\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(json\u001b[38;5;241m.\u001b[39mloads(\u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetchone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     15\u001b[0m cursor\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdate region set vector = ? where id = ?\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     16\u001b[0m                (json\u001b[38;5;241m.\u001b[39mdumps([\u001b[38;5;28mfloat\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m average_hidden_states\u001b[38;5;241m.\u001b[39mnumpy()]), \u001b[38;5;28mid\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "with get_cursor() as cursor:\n",
    "    regions = cursor.execute('select id, code from region')\n",
    "    for region in tqdm(list(regions)):\n",
    "        id, code = region\n",
    "        tokenized_inputs = tokenizer([code], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokenized_inputs)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "        average_hidden_states = last_hidden_states.mean(dim=1)[0]\n",
    "        print(last_hidden_states.shape)\n",
    "        print(average_hidden_states.shape)\n",
    "        cursor.execute('select id, vector from region where json_array_length(vector) > 768')\n",
    "        print(len(json.loads(cursor.fetchone()[1])))\n",
    "        break\n",
    "        cursor.execute('update region set vector = ? where id = ?', \n",
    "                       (json.dumps([float(x) for x in average_hidden_states.numpy()]), id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "id": "66ad251e-5d63-4b14-a8b6-59dc033b24ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 35435, 120, 791, 1069, 40039, 43048, 25522, 50118, 1437, 1437, 50118, 16435, 15664, 40039, 791, 1069, 47006, 50118, 50118, 32845, 791, 1069, 40039, 48271, 47006, 50118, 24303, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 8), (9, 12), (12, 13), (13, 15), (15, 18), (18, 20), (21, 22), (22, 23), (24, 24), (25, 25), (25, 26), (26, 29), (29, 33), (33, 36), (36, 37), (37, 39), (39, 42), (42, 43), (43, 44), (44, 50), (50, 51), (51, 53), (53, 56), (56, 60), (60, 63), (63, 64), (64, 65), (0, 0)]}"
      ]
     },
     "execution_count": 893,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(code, return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "id": "211bdf4d-2c35-4cd9-81b6-8f3775c59e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('drop table if exists tokenized')\n",
    "    cursor.execute('create table if not exists tokenized (ID INTEGER PRIMARY KEY, input_ids JSON, region_ids JSON)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "id": "655692b4-a8bc-4aeb-95f4-9a23fedb1754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5099/5099 [00:11<00:00, 430.60it/s]\n"
     ]
    }
   ],
   "source": [
    "with get_cursor() as cursor:\n",
    "    snippets = list(cursor.execute('select id, code, locations, regions from snippets where idx < 650'))\n",
    "for (id, code, locations, regions) in tqdm(snippets):\n",
    "    tokens = tokenizer.encode_plus(code, truncation=True, return_offsets_mapping=True)\n",
    "    offset_mapping = tokens['offset_mapping']\n",
    "    regions, locations = json.loads(regions),json.loads(locations)\n",
    "    if not len(regions):\n",
    "        continue\n",
    "    locations.append(len(code))\n",
    "    i = 0\n",
    "    region_ids = []\n",
    "    for (start, end) in offset_mapping[1:-1]:\n",
    "        while start > locations[i+1]:\n",
    "            i += 1\n",
    "        region_ids.append(regions[i])\n",
    "    region_ids = [0] + region_ids + [0]\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute(\n",
    "            'insert into tokenized (ID, input_ids, region_ids) values (?, ?, ?)',\n",
    "            (id, json.dumps(tokens['input_ids']), json.dumps(region_ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ac61389-730d-4ccf-bff3-482d49b70117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6818bc7d-a5ac-464e-9d6a-63c9daee3a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(40, '[0, 35435, 18134, 17894, 43048, 25522, 50118, 1437, 10759, 25522, 5521, 4376, 35, 4285, 45106, 42119, 29, 6, 3156, 35, 4285, 45106, 22429, 42119, 29, 6, 7721, 35, 4285, 48837, 42119, 29, 24303, 5457, 120, 42119, 29, 7605, 250, 1020, 45463, 47006, 50118, 1437, 10759, 25522, 5521, 4376, 35, 1029, 45106, 42119, 29, 6, 3156, 35, 1029, 45106, 22429, 42119, 29, 6, 7721, 35, 1029, 48837, 42119, 29, 24303, 5457, 120, 42119, 29, 7605, 41555, 23185, 47006, 50140, 1437, 10759, 17928, 45985, 5457, 8932, 45985, 1640, 1043, 45106, 42119, 29, 6, 1029, 45106, 42119, 29, 4397, 50118, 1437, 10759, 3156, 45985, 5457, 8932, 45985, 1640, 1043, 45106, 22429, 42119, 29, 6, 1029, 45106, 22429, 42119, 29, 4397, 50118, 1437, 10759, 7721, 45985, 5457, 8932, 45985, 1640, 1043, 48837, 42119, 29, 6, 1029, 48837, 42119, 29, 4397, 50118, 1437, 10759, 34, 45985, 5457, 43912, 1640, 5521, 4376, 45985, 4, 32278, 23329, 45056, 3156, 45985, 4, 32278, 23329, 45056, 7721, 45985, 4, 32278, 23329, 4397, 50140, 1437, 114, 36, 7333, 45985, 43, 25522, 50118, 1437, 1437, 1437, 10759, 421, 14484, 4376, 104, 22393, 5457, 2718, 4, 46624, 1640, 32863, 33302, 1215, 8727, 3293, 1215, 47992, 6, 83, 6454, 1215, 534, 46508, 1723, 1215, 47992, 4397, 50118, 1437, 1437, 1437, 10759, 421, 22429, 104, 22393, 5457, 2718, 4, 46624, 1640, 32863, 33302, 1215, 8727, 3293, 1215, 47992, 6, 83, 6454, 1215, 37133, 14645, 1215, 3755, 43398, 1215, 47992, 4397, 50118, 1437, 1437, 1437, 10759, 421, 48684, 104, 22393, 5457, 2718, 4, 46624, 1640, 32863, 33302, 1215, 8727, 3293, 1215, 47992, 6, 83, 6454, 1215, 37133, 14645, 1215, 6725, 2620, 7205, 1723, 1215, 47992, 4397, 50118, 1437, 1437, 1437, 10759, 3031, 104, 22393, 5457, 2718, 4, 46624, 1640, 32863, 33302, 1215, 8727, 3293, 1215, 47992, 6, 37604, 14356, 4322, 1215, 48468, 4397, 50140, 1437, 1437, 1437, 266, 45985, 1640, 5521, 4376, 45985, 6, 421, 14484, 4376, 104, 22393, 6, 3031, 104, 22393, 4397, 50118, 1437, 1437, 1437, 266, 45985, 1640, 39472, 45985, 6, 421, 22429, 104, 22393, 6, 3031, 104, 22393, 4397, 50118, 1437, 1437, 1437, 266, 45985, 1640, 3463, 45598, 45985, 6, 421, 48684, 104, 22393, 6, 3031, 104, 22393, 4397, 50118, 1437, 35524, 50140, 1437, 609, 4, 44054, 1640, 7333, 45985, 17487, 112, 4832, 321, 4397, 50118, 24303, 2]', '[0, 7469, 7469, 7469, 7469, 7469, 7469, 7469, 7469, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7467, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 7468, 0]')]\n"
     ]
    }
   ],
   "source": [
    "with get_cursor() as cursor:\n",
    "    print(list(cursor.execute('select * from tokenized limit 1')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bef707b-169d-4458-9f8c-18e57fd47407",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('drop table if exists train_snippets')\n",
    "    cursor.execute('create table train_snippets as select * from snippets where idx < 650')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fda53217-c701-4ded-9c2b-c8f63e77a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_epoch(batch_size):\n",
    "    with get_cursor() as cursor:\n",
    "        epoch = list(cursor.execute('select count(*) from train_snippets'))[0][0]\n",
    "        cursor.execute('create table if not exists train_snippets_shuffled as select * from train_snippets ORDER BY RANDOM()')\n",
    "        for i in range(0, epoch, batch_size):\n",
    "            ids = ','.join(\n",
    "                str(x[0]) for x in cursor.execute('select id from train_snippets_shuffled limit ? offset ?', (batch_size, i))\n",
    "            )\n",
    "            tokens = cursor.execute(f'select input_ids, region_ids from tokenized where id in ({ids})')\n",
    "            tokens = [(json.loads(input_ids), json.loads(region_ids)) for (input_ids, region_ids) in tokens]\n",
    "            region_ids_str = ','.join(set(str(region_id) for (_, region_ids) in tokens for region_id in region_ids))\n",
    "            regions = dict((id, json.loads(vector)) for (id, vector) in\n",
    "                cursor.execute(f'select id, vector from region where id in ({region_ids_str})'))\n",
    "            batch = []\n",
    "            for (input_ids, region_ids) in tokens:\n",
    "                label_size = 768\n",
    "                size = len(input_ids)\n",
    "                if size > 512:\n",
    "                    print(size)\n",
    "                empties = 512 - size\n",
    "                null_vector = [0] * label_size\n",
    "                for r_id in region_ids:\n",
    "                    if r_id and not (r_id in regions):\n",
    "                        print(r_id)\n",
    "                label = [regions[r_id] if r_id else null_vector for r_id in region_ids]\n",
    "                label += [null_vector] * empties\n",
    "                input_ids += [0] * empties\n",
    "                attention = [1] * size + [0] * empties\n",
    "                batch.append((input_ids, attention, label))\n",
    "            yield tuple(torch.FloatTensor(t) for t in zip(*batch))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "df189e27-706e-4ee0-81e2-ef57d1522b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2525, -0.2023, -0.0671,  ..., -0.4390, -0.1260, -0.0548],\n",
      "        [ 0.2525, -0.2023, -0.0671,  ..., -0.4390, -0.1260, -0.0548],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "for batch in handle_epoch(16):\n",
    "    input_ids, attention, label = batch\n",
    "    print(label[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36fc9de3-2f66-4aa6-8237-7e38c094b329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 2}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict([(1, 2)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
