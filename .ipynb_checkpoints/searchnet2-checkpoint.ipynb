{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6db99f-03f8-4d9c-b270-11efa25b0cd7",
   "metadata": {},
   "source": [
    "# After getting files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8225e83-c777-4e68-8d88-1c2a82ca195f",
   "metadata": {},
   "source": [
    "## Insert into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00457c64-4536-4a2a-be83-04e1f1e66e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting dataset location\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "try:\n",
    "    dataset\n",
    "except:\n",
    "    print('getting dataset location')\n",
    "    dataset = Path('dataset').absolute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28b7e35-3703-4a56-9707-fadde8c3f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(dataset)\n",
    "\n",
    "js_dir = dataset / 'javascript'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40094612-df9b-4969-bdd3-94b0f201afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9c940ca-f75f-4f96-9960-3c301cdde7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3, json\n",
    "from tqdm import tqdm\n",
    "from contextlib import contextmanager\n",
    "\n",
    "train = js_dir / 'final' / 'jsonl' / 'train'\n",
    "os.chdir(train)\n",
    "\n",
    "@contextmanager\n",
    "def get_cursor(database_name='rsn_train'):\n",
    "    with sqlite3.connect(database_name) as conn:\n",
    "        yield conn.cursor()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b02a9475-3743-4131-bc5b-864a34c129c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "def simple_parse_xml(content, nested):\n",
    "    pattern = r'\\s*<([^\\s]*?)>\\s*'\n",
    "    open = re.search(pattern, content)\n",
    "    if not open:\n",
    "        return [content]\n",
    "    [open_start, open_end] = open.span()\n",
    "    before = content[:open_start]\n",
    "    tag = open.group(1)\n",
    "    inner_and_after = content[open_end:]\n",
    "    close = re.search(f'</{re.escape(tag)}>', inner_and_after)\n",
    "    if close:\n",
    "        [close_start, close_end] = close.span()\n",
    "    else:\n",
    "        close = re.search(pattern, inner_and_after)\n",
    "        if close:\n",
    "            [close_start, _] = close.span()\n",
    "        else:\n",
    "            close_start = len(inner_and_after)\n",
    "        close_end = close_start\n",
    "    inner = inner_and_after[:close_start]\n",
    "    after = inner_and_after[close_end:]\n",
    "    inner = simple_parse_xml(inner, nested) if nested else inner\n",
    "    return [[before, tag, inner], *simple_parse_xml(after, nested)]\n",
    "\n",
    "def atom_to_re(s):\n",
    "    tokens = [token.strip() for token in re.split(r'\\s+|(?=\\W)|(?<=\\W)', s.strip()) if token]\n",
    "    escaped = [regex.escape(token, special_only=True) for token in tokens]\n",
    "    return r'\\s*' + r'\\s*'.join(escaped) + r'\\s*'\n",
    "\n",
    "def str_to_re(s):\n",
    "    codes = re.split(r'\\s*(?://[^\\n]*(?:\\n|$)|/\\*.*?\\*/|\\.{3,})\\s*', s,  flags=re.DOTALL)\n",
    "    return '(?:.*?)'.join(atom_to_re(code) for code in codes)\n",
    "\n",
    "def node_to_re(node, c):\n",
    "    if type(node) == str:\n",
    "        return str_to_re(node), []\n",
    "    before, tag, content = node\n",
    "    before_re = str_to_re(before)\n",
    "    c[0] += 1\n",
    "    open_gr = c[0]\n",
    "    content_re, content_tags = make_regex(content, c)\n",
    "    c[0] += 1\n",
    "    close_gr = c[0]\n",
    "    open_re = '\\s*(|<'+re.escape(tag)+'>)\\s*'\n",
    "    close_re = '\\s*(|</'+re.escape(tag)+'>)\\s*'\n",
    "    return before_re+open_re+content_re+close_re, [(tag, open_gr, close_gr, content_tags)]\n",
    "\n",
    "\n",
    "def make_regex(tree, c):\n",
    "    regs, tags = zip(*(node_to_re(node, c) for node in tree))\n",
    "    return re.sub(r'(\\\\s\\*)+', r'\\\\s*', ''.join(regs)), [t for tag in tags for t in tag] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8904bebf-5087-4d02-ad73-5e4f5aac1cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re\n",
    "omitted = []\n",
    "output = []\n",
    "for text_file in js_dir.glob('thread2-*.txt'):\n",
    "    outfile = str.replace(str(text_file), '.txt', '.json')\n",
    "    if os.path.exists(outfile):\n",
    "        continue\n",
    "    with open(text_file, 'r') as file:\n",
    "        file_contents = file.read()\n",
    "    parsed = simple_parse_xml(file_contents, nested=False)\n",
    "    output.append({\n",
    "        'file': str(text_file),\n",
    "        'split': [p[0] for p in parsed[:-1]] + [parsed[-1]],\n",
    "        'len': len(parsed),\n",
    "        'ok': len(parsed) == 11\n",
    "    })\n",
    "    if len(parsed) != 11:\n",
    "        omitted.append(text_file)\n",
    "        continue\n",
    "    with open(outfile, 'w') as file:\n",
    "        file.write(json.dumps([p[1:] for p in parsed]))\n",
    "\n",
    "with open('parse3.log', 'w') as file:\n",
    "    file.write(json.dumps(output))\n",
    "for out in output:\n",
    "    if not out['ok']:\n",
    "        continue\n",
    "    print(out['file'])\n",
    "    print(out['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "709bdefe-5748-4a18-8be6-ae73ddaeead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('drop table if exists snippets')\n",
    "    cursor.execute('drop table if exists region')\n",
    "    cursor.execute('create table if not exists snippets (ID INTEGER PRIMARY KEY, idx INTEGER, code TEXT, locations JSON, regions JSON, SRP boolean)')\n",
    "    cursor.execute('create table if not exists region (ID INTEGER PRIMARY KEY, code TEXT, vector JSON)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8996c197-897d-4d14-bb18-1034b3a38b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_region(code):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute('insert into region (code) values (?)', (code, ))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "def insert_snippet(id, index, code, locations, regions, srp):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute(\n",
    "            'insert into snippets (ID, idx, code, locations, regions, SRP) values (?, ?, ?, ?, ?, ?)',\n",
    "            (id, index, code, json.dumps(locations), json.dumps(regions), srp))\n",
    "\n",
    "def flat_wrong_tags(tags, code, m):\n",
    "    clean_tags = []\n",
    "    for tag in tags:\n",
    "        clean_tags += flat_wrong_tag(tag, clean_tags, code, m)\n",
    "    return clean_tags\n",
    "\n",
    "def flat_wrong_tag(tag, clean_tags, code, m):\n",
    "    name, open, close, sub_tags = tag\n",
    "    clean_sub_tags = flat_wrong_tags(sub_tags, code, m)\n",
    "    if m.group(open):\n",
    "        if len(clean_tags):\n",
    "            pname, popen, pclose, psub_tags = clean_tags[-1]\n",
    "            if not m.group(pclose):\n",
    "                clean_tags[-1] = (pname, popen, close, psub_tags)\n",
    "        return clean_sub_tags\n",
    "    return [(name, open, close, clean_sub_tags)]\n",
    "\n",
    "def tag_to_json(tag, code, m, handle_region):\n",
    "    name, open, close, sub_tags = tag\n",
    "    start = m.span(open)[0]\n",
    "    regions, body, end = tags_to_json(start, sub_tags, code, m, handle_region)\n",
    "    body += code[end:m.span(close)[0]] \n",
    "    region_id = handle_region(f'function {name} () {{\\n{body}\\n}}')\n",
    "    regions = [(start, region_id)] + regions\n",
    "    return regions\n",
    "\n",
    "\n",
    "def tags_to_json(outer_index, tags, code, m, handle_region):\n",
    "    regions = []\n",
    "    outer_body = ''\n",
    "    for tag in tags:\n",
    "        name, open, close, _ = tag\n",
    "        outer_body += code[outer_index:m.span(open)[0]] + '\\n' + name + '();\\n'\n",
    "        outer_index = m.span(close)[0]\n",
    "        regions += tag_to_json(tag, code, m, handle_region)\n",
    "    return regions, outer_body, outer_index\n",
    "\n",
    "\n",
    "def to_json(tags, code, m, handle_region):\n",
    "    regions, body, end = tags_to_json(0, tags, code, m, handle_region)\n",
    "    body += code[end:] \n",
    "    region_id = handle_region(body)\n",
    "    regions = [(0, region_id)] + regions\n",
    "    if len(regions) > 1:\n",
    "        regions.append((end, region_id))\n",
    "    return regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194654e4-f22c-452e-9dfb-5a953fb6a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_js_comments(js_code):\n",
    "    js_code = re.sub(r'\\n?//.*?\\n', '\\n', js_code)\n",
    "    js_code = re.sub(r'/\\*.*?\\*/', '', js_code, flags=re.DOTALL)\n",
    "    return js_code    \n",
    "\n",
    "n_ok, n_all = 0, 0\n",
    "for text_file in tqdm(list(js_dir.glob('thread2-*.json'))):\n",
    "    index = int(re.match('.*thread2-(.*)\\.json', str(text_file)).group(1))\n",
    "    limit = 10\n",
    "    with get_cursor() as cursor:\n",
    "        codes = list(cursor.execute('select id, code from shuffled limit ? offset ?', (limit, limit * index + 1)))\n",
    "    with open(text_file, 'r') as file:\n",
    "        file_contents = file.read()\n",
    "    for (id, code), obj in zip(codes, json.loads(file_contents)):\n",
    "        original = strip_js_comments(code)\n",
    "        xml = obj[1]\n",
    "        reg_str, tags = make_regex(simple_parse_xml(xml, True), [0])\n",
    "        reg = regex.compile(reg_str, flags = regex.DOTALL)\n",
    "        m = reg.match(original)\n",
    "        n_all += 1\n",
    "        if m:\n",
    "            n_ok += 1\n",
    "            tags = flat_wrong_tags(tags, original, m)\n",
    "            if len(tags) == 1 and not len(tags[0][3]):\n",
    "                tags = []\n",
    "            regions = to_json(tags, original, m, insert_region)\n",
    "            insert_snippet(id, index, original, *zip(*regions), len(tags) == 0)\n",
    "            \n",
    "            \n",
    "print(f'{n_ok}/{n_all}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a62a65ab-ab7c-4efe-a4d3-2f0f5145c12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 567, 0), (0, 574, 1), (1, 2510, 0), (1, 2596, 1)]\n",
      "[(17037,)]\n"
     ]
    }
   ],
   "source": [
    "with get_cursor() as cursor:\n",
    "    print(list(cursor.execute('select idx < 650, count(*), SRP from snippets group by idx < 650, SRP')))\n",
    "    print(list(cursor.execute('select count(*) from region')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fee2198e-ec9b-46ee-a700-4a102de567d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8663b41d-56f6-4ebd-ba46-8efbfe5be2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_labels():\n",
    "    with get_cursor() as cursor:\n",
    "        regions = cursor.execute('select id, code from region')\n",
    "        for region in tqdm(list(regions)):\n",
    "            id, code = region\n",
    "            tokenized_inputs = tokenizer([code], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**tokenized_inputs)\n",
    "                last_hidden_states = outputs.last_hidden_state\n",
    "            average_hidden_states = last_hidden_states.mean(dim=1)[0]\n",
    "            cursor.execute('update region set vector = ? where id = ?', \n",
    "                           (json.dumps([float(x) for x in average_hidden_states.numpy()]), id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d6ff5-67f5-4119-a067-754c5dd9fa2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e81adab-31e7-478f-8f0c-9bc251b82e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5106\n"
     ]
    }
   ],
   "source": [
    "with get_cursor() as cursor:\n",
    "    print(next(cursor.execute('select count(*) from train_snippets'))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211bdf4d-2c35-4cd9-81b6-8f3775c59e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('drop table if exists tokenized')\n",
    "    cursor.execute('create table if not exists tokenized (ID INTEGER PRIMARY KEY, input_ids JSON, region_ids JSON)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655692b4-a8bc-4aeb-95f4-9a23fedb1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    snippets = list(cursor.execute('select id, code, locations, regions from snippets where idx < 650'))\n",
    "for (id, code, locations, regions) in tqdm(snippets):\n",
    "    tokens = tokenizer.encode_plus(code, truncation=True, return_offsets_mapping=True)\n",
    "    offset_mapping = tokens['offset_mapping']\n",
    "    regions, locations = json.loads(regions),json.loads(locations)\n",
    "    if not len(regions):\n",
    "        continue\n",
    "    locations.append(len(code))\n",
    "    i = 0\n",
    "    region_ids = []\n",
    "    for (start, end) in offset_mapping[1:-1]:\n",
    "        while start > locations[i+1]:\n",
    "            i += 1\n",
    "        region_ids.append(regions[i])\n",
    "    region_ids = [0] + region_ids + [0]\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute(\n",
    "            'insert into tokenized (ID, input_ids, region_ids) values (?, ?, ?)',\n",
    "            (id, json.dumps(tokens['input_ids']), json.dumps(region_ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bef707b-169d-4458-9f8c-18e57fd47407",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('drop table if exists train_snippets')\n",
    "    cursor.execute('create table train_snippets as select * from snippets where idx < 650')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5569bc6b-d6e2-453d-8dab-c16d2cca3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('create table if not exists current_batch(idx INTEGER)')\n",
    "    if not len(list(cursor.execute('select * from current_batch'))):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78bc3235-55f3-4130-bc94-40624dd4a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('create table if not exists last_batch(idx INTEGER)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fda53217-c701-4ded-9c2b-c8f63e77a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def set_last_batch(last_batch):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute('update last_batch set idx = ?', (last_batch,))\n",
    "\n",
    "def get_unsafe_last_batch(batches):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute('select * from last_batch')\n",
    "        item = cursor.fetchone()\n",
    "        if not item:\n",
    "            cursor.execute('insert into last_batch values (0)')\n",
    "            return 0\n",
    "        else:\n",
    "            return item[0]\n",
    "\n",
    "def get_last_batch(batches):\n",
    "    last_batch = get_unsafe_last_batch(batches)\n",
    "    if last_batch >= batches:\n",
    "        last_batch = 0\n",
    "        set_last_batch(0)\n",
    "    if last_batch == 0:\n",
    "        tee('recreating train_snippets_shuffled')\n",
    "        with get_cursor() as cursor:\n",
    "            cursor.execute('drop table if exists train_snippets_shuffled')\n",
    "            cursor.execute('create table train_snippets_shuffled as select * from train_snippets ORDER BY RANDOM()')\n",
    "    return last_batch\n",
    "        \n",
    "def get_batch_tokens(batch_size, i):\n",
    "    with get_cursor() as cursor:\n",
    "        ids = ','.join(\n",
    "            str(x[0]) for x in cursor.execute('select id from train_snippets_shuffled limit ? offset ?', (batch_size, i))\n",
    "        )\n",
    "        tokens = cursor.execute(f'select input_ids, region_ids from tokenized where id in ({ids})')\n",
    "        return [(json.loads(input_ids), json.loads(region_ids)) for (input_ids, region_ids) in tokens]\n",
    "        \n",
    "\n",
    "def get_batch_regions(tokens):\n",
    "    with get_cursor() as cursor:\n",
    "        region_ids_str = ','.join(set(str(region_id) for (_, region_ids) in tokens for region_id in region_ids))\n",
    "        return dict((id, json.loads(vector)) for (id, vector) in\n",
    "            cursor.execute(f'select id, vector from region where id in ({region_ids_str})'))\n",
    "\n",
    "def prepare_iteration(input_ids, region_ids, regions):\n",
    "    label_size = 768\n",
    "    size = len(input_ids)\n",
    "    if size > 512:\n",
    "        print(size)\n",
    "    empties = 512 - size\n",
    "    null_vector = [0] * label_size\n",
    "    for r_id in region_ids:\n",
    "        if r_id and not (r_id in regions):\n",
    "            print(r_id)\n",
    "    label = [regions[r_id] if r_id else null_vector for r_id in region_ids]\n",
    "    label += [null_vector] * empties\n",
    "    input_ids += [0] * empties\n",
    "    attention = [1] * size + [0] * empties\n",
    "    return input_ids, attention, label\n",
    "\n",
    "def get_batch(batch_size, i):\n",
    "    tokens = get_batch_tokens(batch_size, i)\n",
    "    regions = get_batch_regions(tokens)\n",
    "    batch = [prepare_iteration(input_ids, region_ids, regions) for (input_ids, region_ids) in tokens]\n",
    "    input_ids, attention, label = zip(*batch)\n",
    "    return torch.IntTensor(input_ids), torch.FloatTensor(attention), torch.FloatTensor(label)\n",
    "\n",
    "\n",
    "def get_epoch_part(i0, epoch, batch_size):\n",
    "    return ((i, get_batch(batch_size, i)) for i in range(i0, epoch, batch_size))\n",
    "\n",
    "def tee(text):\n",
    "    print(f\"\\r{text}\\r\")\n",
    "    with open('train-resnet.log', 'a') as file:\n",
    "        file.write(f\"{text}\\n\")\n",
    "\n",
    "def log(title, start_time, sizes, epoch, i, mean_loss, loss):\n",
    "    _, num_epochs, batches = sizes\n",
    "    i += 1\n",
    "    dt = time.time() - start_time\n",
    "    elapsed = timedelta(seconds=int(dt))\n",
    "    remaining = timedelta(seconds=int(dt*(batches-i)/i))\n",
    "    text = f\"{title}: {elapsed}<{remaining} Epoch {epoch+1}/{num_epochs} - Batch {i}/{batches}, Loss: {mean_loss:.4f} {loss.item():.4f}\"\n",
    "    tee(text)\n",
    "\n",
    "def save(epoch, i):\n",
    "    \n",
    "    torch.save(model.state_dict(), f'searchnet-dynmodel-{epoch}-{i}.pt')\n",
    "    torch.save(optimizer.state_dict(), f'searchnet-dynoptimizer-{epoch}-{i}.pt')\n",
    "    set_last_batch(i)\n",
    "    tee(f'saving searchnet-dynmodel-{epoch}-{i}.pt\\n')\n",
    "\n",
    "def handle_train_batch(i, epoch, batch, performance, sizes):\n",
    "    input_ids, attention, labels = (c.to(device) for c in batch)\n",
    "    min_alpha, alpha, running_loss, start_time = performance\n",
    "    train_batchs, num_epochs, batches = sizes\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids, attention_mask=attention)\n",
    "    loss = loss_fn(outputs.last_hidden_state, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    current_alpha = min_alpha + alpha\n",
    "    running_loss = (1 - current_alpha) * running_loss + current_alpha * loss.item()\n",
    "    alpha *= .5\n",
    "    \n",
    "    log('training', start_time, sizes, epoch, i, running_loss, loss)\n",
    "    if i % 100 == 0:\n",
    "        save(epoch, i)\n",
    "    return running_loss, alpha\n",
    "    \n",
    "def handle_val_batch(i0, i, epoch, batch, start_time, cum_loss, sizes):\n",
    "    input_ids, attention, labels = (c.to(device) for c in batch)\n",
    "    _, num_epochs, batches = sizes\n",
    "\n",
    "    with torch.no_grad():            \n",
    "        outputs = model(input_ids, attention_mask=attention)\n",
    "        loss = loss_fn(outputs.last_hidden_state, labels)\n",
    "    \n",
    "    cum_loss += loss.item()\n",
    "    mean_loss = cum_loss / (i - i0 + 1)\n",
    "    \n",
    "    log('validation', start_time, sizes, epoch, i, mean_loss, loss)\n",
    "    return cum_loss\n",
    "    \n",
    "\n",
    "def handle_epoch(batch_size, num_epochs, epoch):\n",
    "    with get_cursor() as cursor:\n",
    "        epoch_size = next(cursor.execute('select count(*) from train_snippets'))[0]\n",
    "    batches = (epoch_size - 1) // batch_size + 1\n",
    "    train_batchs = 4 * batches // 5\n",
    "    i0 = get_last_batch(train_batchs)\n",
    "    i = i0 - 1\n",
    "    min_alpha = .2\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    alpha = 1 - min_alpha\n",
    "\n",
    "    model.train()\n",
    "    sizes = (train_batchs, num_epochs, batches)\n",
    "    tee('\\nTraining\\n')\n",
    "    for (i, batch) in get_epoch_part(i0, train_batchs, 1):\n",
    "        performance = (min_alpha, alpha, running_loss, start_time)\n",
    "        running_loss, alpha = handle_train_batch(i, epoch, batch, performance, sizes)\n",
    "    \n",
    "    \n",
    "    i0 = i + 1\n",
    "    save(epoch, i0)\n",
    "    \n",
    "    model.eval()\n",
    "    cum_loss = 0\n",
    "    tee('\\nValidation:\\n')\n",
    "    for (i, batch) in get_epoch_part(i0, batches, 1):\n",
    "        cum_loss = handle_val_batch(i0, i, epoch, batch, start_time, cum_loss, sizes)\n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59af576d-6e6b-4f4d-910d-ff23dab8cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_last_batch(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df189e27-706e-4ee0-81e2-ef57d1522b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 160\n",
      "training: 0:00:00<0:01:01 Epoch 1/10 - Batch 2/160, Loss: 0.2829 0.2829\n",
      "training: 0:00:02<0:02:38 Epoch 1/10 - Batch 3/160, Loss: 0.1976 0.1408\n",
      "training: 0:00:02<0:01:51 Epoch 1/10 - Batch 4/160, Loss: 0.2202 0.2540\n",
      "training: 0:00:02<0:01:28 Epoch 1/10 - Batch 5/160, Loss: 0.2284 0.2475\n",
      "training: 0:00:02<0:01:13 Epoch 1/10 - Batch 6/160, Loss: 0.2265 0.2210\n",
      "training: 0:00:02<0:01:04 Epoch 1/10 - Batch 7/160, Loss: 0.2206 0.2003\n",
      "training: 0:00:02<0:00:57 Epoch 1/10 - Batch 8/160, Loss: 0.2228 0.2308\n",
      "training: 0:00:02<0:00:53 Epoch 1/10 - Batch 9/160, Loss: 0.2247 0.2319\n",
      "training: 0:00:02<0:00:49 Epoch 1/10 - Batch 10/160, Loss: 0.2079 0.1421\n",
      "training: 0:00:03<0:00:45 Epoch 1/10 - Batch 11/160, Loss: 0.2066 0.2017\n",
      "training: 0:00:03<0:00:42 Epoch 1/10 - Batch 12/160, Loss: 0.1990 0.1688\n",
      "training: 0:00:03<0:00:40 Epoch 1/10 - Batch 13/160, Loss: 0.1931 0.1694\n",
      "training: 0:00:03<0:00:38 Epoch 1/10 - Batch 14/160, Loss: 0.1898 0.1765\n",
      "training: 0:00:03<0:00:37 Epoch 1/10 - Batch 15/160, Loss: 0.1866 0.1738\n",
      "training: 0:00:03<0:00:36 Epoch 1/10 - Batch 16/160, Loss: 0.1781 0.1442\n",
      "training: 0:00:03<0:00:35 Epoch 1/10 - Batch 17/160, Loss: 0.1745 0.1601\n",
      "training: 0:00:04<0:00:34 Epoch 1/10 - Batch 18/160, Loss: 0.1727 0.1655\n",
      "training: 0:00:04<0:00:32 Epoch 1/10 - Batch 19/160, Loss: 0.1626 0.1220\n",
      "training: 0:00:04<0:00:32 Epoch 1/10 - Batch 20/160, Loss: 0.1586 0.1426\n",
      "training: 0:00:04<0:00:31 Epoch 1/10 - Batch 21/160, Loss: 0.1533 0.1321\n",
      "training: 0:00:04<0:00:30 Epoch 1/10 - Batch 22/160, Loss: 0.1534 0.1537\n",
      "training: 0:00:04<0:00:30 Epoch 1/10 - Batch 23/160, Loss: 0.1465 0.1193\n",
      "training: 0:00:05<0:00:29 Epoch 1/10 - Batch 24/160, Loss: 0.1388 0.1077\n",
      "training: 0:00:05<0:00:29 Epoch 1/10 - Batch 25/160, Loss: 0.1311 0.1007\n",
      "training: 0:00:05<0:00:29 Epoch 1/10 - Batch 26/160, Loss: 0.1262 0.1064\n",
      "training: 0:00:05<0:00:29 Epoch 1/10 - Batch 27/160, Loss: 0.1273 0.1315\n",
      "training: 0:00:05<0:00:28 Epoch 1/10 - Batch 28/160, Loss: 0.1267 0.1247\n",
      "training: 0:00:05<0:00:28 Epoch 1/10 - Batch 29/160, Loss: 0.1247 0.1166\n",
      "training: 0:00:06<0:00:27 Epoch 1/10 - Batch 30/160, Loss: 0.1242 0.1223\n",
      "training: 0:00:06<0:00:27 Epoch 1/10 - Batch 31/160, Loss: 0.1225 0.1156\n",
      "training: 0:00:06<0:00:27 Epoch 1/10 - Batch 32/160, Loss: 0.1188 0.1038\n",
      "training: 0:00:06<0:00:26 Epoch 1/10 - Batch 33/160, Loss: 0.1160 0.1048\n",
      "training: 0:00:06<0:00:26 Epoch 1/10 - Batch 34/160, Loss: 0.1139 0.1054\n",
      "training: 0:00:07<0:00:25 Epoch 1/10 - Batch 35/160, Loss: 0.1104 0.0968\n",
      "training: 0:00:07<0:00:25 Epoch 1/10 - Batch 36/160, Loss: 0.1085 0.1006\n",
      "training: 0:00:07<0:00:25 Epoch 1/10 - Batch 37/160, Loss: 0.1044 0.0880\n",
      "training: 0:00:07<0:00:25 Epoch 1/10 - Batch 38/160, Loss: 0.1056 0.1104\n",
      "training: 0:00:07<0:00:24 Epoch 1/10 - Batch 39/160, Loss: 0.1071 0.1130\n",
      "training: 0:00:07<0:00:24 Epoch 1/10 - Batch 40/160, Loss: 0.1088 0.1158\n",
      "training: 0:00:08<0:00:24 Epoch 1/10 - Batch 41/160, Loss: 0.1062 0.0959\n",
      "training: 0:00:08<0:00:23 Epoch 1/10 - Batch 42/160, Loss: 0.1012 0.0811\n",
      "training: 0:00:08<0:00:23 Epoch 1/10 - Batch 43/160, Loss: 0.1004 0.0973\n",
      "training: 0:00:08<0:00:23 Epoch 1/10 - Batch 44/160, Loss: 0.0993 0.0948\n",
      "training: 0:00:08<0:00:23 Epoch 1/10 - Batch 45/160, Loss: 0.0999 0.1024\n",
      "training: 0:00:09<0:00:23 Epoch 1/10 - Batch 46/160, Loss: 0.1006 0.1033\n",
      "training: 0:00:09<0:00:22 Epoch 1/10 - Batch 47/160, Loss: 0.0975 0.0849\n",
      "training: 0:00:09<0:00:22 Epoch 1/10 - Batch 48/160, Loss: 0.0968 0.0940\n",
      "training: 0:00:09<0:00:22 Epoch 1/10 - Batch 49/160, Loss: 0.0939 0.0822\n",
      "training: 0:00:09<0:00:22 Epoch 1/10 - Batch 50/160, Loss: 0.0945 0.0972\n",
      "training: 0:00:09<0:00:21 Epoch 1/10 - Batch 51/160, Loss: 0.0956 0.1000\n",
      "training: 0:00:10<0:00:21 Epoch 1/10 - Batch 52/160, Loss: 0.0938 0.0864\n",
      "training: 0:00:10<0:00:21 Epoch 1/10 - Batch 53/160, Loss: 0.0921 0.0856\n",
      "training: 0:00:10<0:00:21 Epoch 1/10 - Batch 54/160, Loss: 0.0934 0.0983\n",
      "training: 0:00:10<0:00:21 Epoch 1/10 - Batch 55/160, Loss: 0.0917 0.0852\n",
      "training: 0:00:10<0:00:20 Epoch 1/10 - Batch 56/160, Loss: 0.0910 0.0881\n",
      "training: 0:00:11<0:00:20 Epoch 1/10 - Batch 57/160, Loss: 0.0905 0.0884\n",
      "training: 0:00:11<0:00:20 Epoch 1/10 - Batch 58/160, Loss: 0.0894 0.0853\n",
      "training: 0:00:11<0:00:20 Epoch 1/10 - Batch 59/160, Loss: 0.0874 0.0791\n",
      "training: 0:00:11<0:00:19 Epoch 1/10 - Batch 60/160, Loss: 0.0856 0.0783\n",
      "training: 0:00:11<0:00:19 Epoch 1/10 - Batch 61/160, Loss: 0.0861 0.0884\n",
      "training: 0:00:12<0:00:19 Epoch 1/10 - Batch 62/160, Loss: 0.0858 0.0843\n",
      "training: 0:00:12<0:00:19 Epoch 1/10 - Batch 63/160, Loss: 0.0846 0.0799\n",
      "training: 0:00:12<0:00:19 Epoch 1/10 - Batch 64/160, Loss: 0.0842 0.0828\n",
      "training: 0:00:12<0:00:18 Epoch 1/10 - Batch 65/160, Loss: 0.0835 0.0807\n",
      "training: 0:00:12<0:00:18 Epoch 1/10 - Batch 66/160, Loss: 0.0810 0.0706\n",
      "training: 0:00:12<0:00:18 Epoch 1/10 - Batch 67/160, Loss: 0.0820 0.0863\n",
      "training: 0:00:13<0:00:18 Epoch 1/10 - Batch 68/160, Loss: 0.0828 0.0858\n",
      "training: 0:00:13<0:00:18 Epoch 1/10 - Batch 69/160, Loss: 0.0828 0.0827\n",
      "training: 0:00:13<0:00:17 Epoch 1/10 - Batch 70/160, Loss: 0.0834 0.0859\n",
      "training: 0:00:13<0:00:17 Epoch 1/10 - Batch 71/160, Loss: 0.0822 0.0774\n",
      "training: 0:00:13<0:00:17 Epoch 1/10 - Batch 72/160, Loss: 0.0809 0.0758\n",
      "training: 0:00:14<0:00:17 Epoch 1/10 - Batch 73/160, Loss: 0.0819 0.0858\n",
      "training: 0:00:14<0:00:17 Epoch 1/10 - Batch 74/160, Loss: 0.0798 0.0715\n",
      "training: 0:00:14<0:00:16 Epoch 1/10 - Batch 75/160, Loss: 0.0772 0.0667\n",
      "training: 0:00:14<0:00:16 Epoch 1/10 - Batch 76/160, Loss: 0.0773 0.0776\n",
      "training: 0:00:14<0:00:16 Epoch 1/10 - Batch 77/160, Loss: 0.0791 0.0862\n",
      "training: 0:00:15<0:00:16 Epoch 1/10 - Batch 78/160, Loss: 0.0773 0.0701\n",
      "training: 0:00:15<0:00:16 Epoch 1/10 - Batch 79/160, Loss: 0.0760 0.0711\n",
      "training: 0:00:15<0:00:15 Epoch 1/10 - Batch 80/160, Loss: 0.0766 0.0788\n",
      "training: 0:00:15<0:00:15 Epoch 1/10 - Batch 81/160, Loss: 0.0753 0.0702\n",
      "training: 0:00:15<0:00:15 Epoch 1/10 - Batch 82/160, Loss: 0.0754 0.0760\n",
      "training: 0:00:15<0:00:15 Epoch 1/10 - Batch 83/160, Loss: 0.0765 0.0810\n",
      "training: 0:00:16<0:00:15 Epoch 1/10 - Batch 84/160, Loss: 0.0747 0.0671\n",
      "training: 0:00:16<0:00:14 Epoch 1/10 - Batch 85/160, Loss: 0.0763 0.0828\n",
      "training: 0:00:16<0:00:14 Epoch 1/10 - Batch 86/160, Loss: 0.0767 0.0785\n",
      "training: 0:00:16<0:00:14 Epoch 1/10 - Batch 87/160, Loss: 0.0764 0.0750\n",
      "training: 0:00:16<0:00:14 Epoch 1/10 - Batch 88/160, Loss: 0.0778 0.0832\n",
      "training: 0:00:17<0:00:14 Epoch 1/10 - Batch 89/160, Loss: 0.0755 0.0666\n",
      "training: 0:00:17<0:00:13 Epoch 1/10 - Batch 90/160, Loss: 0.0763 0.0796\n",
      "training: 0:00:17<0:00:13 Epoch 1/10 - Batch 91/160, Loss: 0.0744 0.0665\n",
      "training: 0:00:17<0:00:13 Epoch 1/10 - Batch 92/160, Loss: 0.0732 0.0685\n",
      "training: 0:00:17<0:00:13 Epoch 1/10 - Batch 93/160, Loss: 0.0732 0.0731\n",
      "training: 0:00:18<0:00:13 Epoch 1/10 - Batch 94/160, Loss: 0.0735 0.0746\n",
      "training: 0:00:18<0:00:12 Epoch 1/10 - Batch 95/160, Loss: 0.0711 0.0614\n",
      "training: 0:00:18<0:00:12 Epoch 1/10 - Batch 96/160, Loss: 0.0693 0.0623\n",
      "training: 0:00:18<0:00:12 Epoch 1/10 - Batch 97/160, Loss: 0.0692 0.0687\n",
      "training: 0:00:18<0:00:12 Epoch 1/10 - Batch 98/160, Loss: 0.0685 0.0658\n",
      "training: 0:00:19<0:00:12 Epoch 1/10 - Batch 99/160, Loss: 0.0704 0.0778\n",
      "training: 0:00:19<0:00:11 Epoch 1/10 - Batch 100/160, Loss: 0.0698 0.0676\n",
      "training: 0:00:19<0:00:11 Epoch 1/10 - Batch 101/160, Loss: 0.0689 0.0650\n",
      "training: 0:00:19<0:00:11 Epoch 1/10 - Batch 102/160, Loss: 0.0704 0.0768\n",
      "training: 0:00:29<0:00:16 Epoch 1/10 - Batch 103/160, Loss: 0.0725 0.0806\n",
      "training: 0:00:29<0:00:16 Epoch 1/10 - Batch 104/160, Loss: 0.0722 0.0710\n",
      "training: 0:00:29<0:00:16 Epoch 1/10 - Batch 105/160, Loss: 0.0692 0.0571\n",
      "training: 0:00:30<0:00:15 Epoch 1/10 - Batch 106/160, Loss: 0.0715 0.0809\n",
      "training: 0:00:30<0:00:15 Epoch 1/10 - Batch 107/160, Loss: 0.0694 0.0610\n",
      "training: 0:00:30<0:00:15 Epoch 1/10 - Batch 108/160, Loss: 0.0682 0.0632\n",
      "training: 0:00:30<0:00:14 Epoch 1/10 - Batch 109/160, Loss: 0.0696 0.0755\n",
      "training: 0:00:30<0:00:14 Epoch 1/10 - Batch 110/160, Loss: 0.0710 0.0766\n",
      "training: 0:00:31<0:00:14 Epoch 1/10 - Batch 111/160, Loss: 0.0685 0.0583\n",
      "training: 0:00:31<0:00:13 Epoch 1/10 - Batch 112/160, Loss: 0.0693 0.0727\n",
      "training: 0:00:31<0:00:13 Epoch 1/10 - Batch 113/160, Loss: 0.0690 0.0676\n",
      "training: 0:00:31<0:00:13 Epoch 1/10 - Batch 114/160, Loss: 0.0654 0.0511\n",
      "training: 0:00:31<0:00:12 Epoch 1/10 - Batch 115/160, Loss: 0.0670 0.0735\n",
      "training: 0:00:31<0:00:12 Epoch 1/10 - Batch 116/160, Loss: 0.0686 0.0751\n",
      "training: 0:00:32<0:00:12 Epoch 1/10 - Batch 117/160, Loss: 0.0665 0.0579\n",
      "training: 0:00:32<0:00:11 Epoch 1/10 - Batch 118/160, Loss: 0.0654 0.0610\n",
      "training: 0:00:32<0:00:11 Epoch 1/10 - Batch 119/160, Loss: 0.0668 0.0723\n",
      "training: 0:00:32<0:00:11 Epoch 1/10 - Batch 120/160, Loss: 0.0665 0.0655\n",
      "training: 0:00:32<0:00:10 Epoch 1/10 - Batch 121/160, Loss: 0.0672 0.0701\n",
      "training: 0:00:32<0:00:10 Epoch 1/10 - Batch 122/160, Loss: 0.0662 0.0622\n",
      "training: 0:00:33<0:00:10 Epoch 1/10 - Batch 123/160, Loss: 0.0690 0.0802\n",
      "training: 0:00:33<0:00:10 Epoch 1/10 - Batch 124/160, Loss: 0.0680 0.0636\n",
      "training: 0:00:33<0:00:09 Epoch 1/10 - Batch 125/160, Loss: 0.0675 0.0659\n",
      "training: 0:00:33<0:00:09 Epoch 1/10 - Batch 126/160, Loss: 0.0684 0.0717\n",
      "training: 0:00:33<0:00:09 Epoch 1/10 - Batch 127/160, Loss: 0.0698 0.0753\n",
      "training: 0:00:33<0:00:08 Epoch 1/10 - Batch 128/160, Loss: 0.0712 0.0769\n",
      "training: 0:00:34<0:00:08 Epoch 1/10 - Batch 129/160, Loss: 0.0726 0.0784\n",
      "validation: 0:00:52<0:00:12 Epoch 1/10 - Batch 130/160, Loss: 0.0299 0.0299\n",
      "validation: 0:00:52<0:00:12 Epoch 1/10 - Batch 131/160, Loss: 0.0281 0.0263\n",
      "validation: 0:00:52<0:00:11 Epoch 1/10 - Batch 132/160, Loss: 0.0275 0.0263\n",
      "validation: 0:00:52<0:00:11 Epoch 1/10 - Batch 133/160, Loss: 0.0309 0.0409\n",
      "validation: 0:00:52<0:00:10 Epoch 1/10 - Batch 134/160, Loss: 0.0294 0.0233\n",
      "validation: 0:00:52<0:00:10 Epoch 1/10 - Batch 135/160, Loss: 0.0284 0.0236\n",
      "validation: 0:00:52<0:00:09 Epoch 1/10 - Batch 136/160, Loss: 0.0287 0.0302\n",
      "validation: 0:00:52<0:00:09 Epoch 1/10 - Batch 137/160, Loss: 0.0290 0.0310\n",
      "validation: 0:00:52<0:00:08 Epoch 1/10 - Batch 138/160, Loss: 0.0288 0.0277\n",
      "validation: 0:00:53<0:00:08 Epoch 1/10 - Batch 139/160, Loss: 0.0295 0.0357\n",
      "validation: 0:00:53<0:00:08 Epoch 1/10 - Batch 140/160, Loss: 0.0302 0.0375\n",
      "validation: 0:00:53<0:00:07 Epoch 1/10 - Batch 141/160, Loss: 0.0297 0.0233\n",
      "validation: 0:00:53<0:00:07 Epoch 1/10 - Batch 142/160, Loss: 0.0292 0.0242\n",
      "validation: 0:00:53<0:00:06 Epoch 1/10 - Batch 143/160, Loss: 0.0289 0.0241\n",
      "validation: 0:00:53<0:00:06 Epoch 1/10 - Batch 144/160, Loss: 0.0295 0.0378\n",
      "validation: 0:00:53<0:00:05 Epoch 1/10 - Batch 145/160, Loss: 0.0293 0.0276\n",
      "validation: 0:00:53<0:00:05 Epoch 1/10 - Batch 146/160, Loss: 0.0294 0.0308\n",
      "validation: 0:00:53<0:00:05 Epoch 1/10 - Batch 147/160, Loss: 0.0293 0.0262\n",
      "validation: 0:00:53<0:00:04 Epoch 1/10 - Batch 148/160, Loss: 0.0293 0.0296\n",
      "validation: 0:00:53<0:00:04 Epoch 1/10 - Batch 149/160, Loss: 0.0302 0.0478\n",
      "validation: 0:00:53<0:00:03 Epoch 1/10 - Batch 150/160, Loss: 0.0300 0.0251\n",
      "validation: 0:00:53<0:00:03 Epoch 1/10 - Batch 151/160, Loss: 0.0301 0.0327\n",
      "validation: 0:00:53<0:00:03 Epoch 1/10 - Batch 152/160, Loss: 0.0298 0.0234\n",
      "validation: 0:00:53<0:00:02 Epoch 1/10 - Batch 153/160, Loss: 0.0298 0.0299\n",
      "validation: 0:00:54<0:00:02 Epoch 1/10 - Batch 154/160, Loss: 0.0297 0.0265\n",
      "validation: 0:00:54<0:00:02 Epoch 1/10 - Batch 155/160, Loss: 0.0295 0.0257\n",
      "validation: 0:00:54<0:00:01 Epoch 1/10 - Batch 156/160, Loss: 0.0293 0.0239\n",
      "validation: 0:00:54<0:00:01 Epoch 1/10 - Batch 157/160, Loss: 0.0293 0.0286\n",
      "validation: 0:00:54<0:00:01 Epoch 1/10 - Batch 158/160, Loss: 0.0291 0.0235\n",
      "validation: 0:00:54<0:00:00 Epoch 1/10 - Batch 159/160, Loss: 0.0289 0.0236\n",
      "validation: 0:00:54<0:00:00 Epoch 1/10 - Batch 160/160, Loss: 0.0287 0.0231\n",
      "validation: 0:00:54<0:00:00 Epoch 1/10 - Batch 161/160, Loss: 0.0285 0.0233\n",
      "128 160\n",
      "training: 0:00:00<0:00:33 Epoch 2/10 - Batch 2/160, Loss: 0.0742 0.0742\n",
      "training: 0:00:01<0:01:59 Epoch 2/10 - Batch 3/160, Loss: 0.0718 0.0703\n",
      "training: 0:00:01<0:01:32 Epoch 2/10 - Batch 4/160, Loss: 0.0693 0.0654\n",
      "training: 0:00:02<0:01:32 Epoch 2/10 - Batch 5/160, Loss: 0.0709 0.0747\n",
      "training: 0:00:02<0:01:21 Epoch 2/10 - Batch 6/160, Loss: 0.0693 0.0644\n",
      "training: 0:00:02<0:01:13 Epoch 2/10 - Batch 7/160, Loss: 0.0675 0.0614\n",
      "training: 0:00:03<0:01:08 Epoch 2/10 - Batch 8/160, Loss: 0.0681 0.0701\n",
      "training: 0:00:03<0:01:04 Epoch 2/10 - Batch 9/160, Loss: 0.0676 0.0657\n",
      "training: 0:00:03<0:00:59 Epoch 2/10 - Batch 10/160, Loss: 0.0660 0.0597\n",
      "training: 0:00:03<0:00:56 Epoch 2/10 - Batch 11/160, Loss: 0.0654 0.0629\n",
      "training: 0:00:04<0:00:54 Epoch 2/10 - Batch 12/160, Loss: 0.0667 0.0720\n",
      "training: 0:00:04<0:00:52 Epoch 2/10 - Batch 13/160, Loss: 0.0663 0.0647\n",
      "training: 0:00:04<0:00:51 Epoch 2/10 - Batch 14/160, Loss: 0.0660 0.0646\n",
      "training: 0:00:04<0:00:50 Epoch 2/10 - Batch 15/160, Loss: 0.0662 0.0672\n",
      "training: 0:00:05<0:00:48 Epoch 2/10 - Batch 16/160, Loss: 0.0653 0.0616\n",
      "training: 0:00:05<0:00:47 Epoch 2/10 - Batch 17/160, Loss: 0.0663 0.0701\n",
      "training: 0:00:05<0:00:46 Epoch 2/10 - Batch 18/160, Loss: 0.0678 0.0741\n",
      "training: 0:00:05<0:00:45 Epoch 2/10 - Batch 19/160, Loss: 0.0680 0.0686\n",
      "training: 0:00:06<0:00:45 Epoch 2/10 - Batch 20/160, Loss: 0.0677 0.0664\n",
      "training: 0:00:06<0:00:43 Epoch 2/10 - Batch 21/160, Loss: 0.0673 0.0656\n",
      "training: 0:00:07<0:00:46 Epoch 2/10 - Batch 22/160, Loss: 0.0643 0.0526\n",
      "training: 0:00:07<0:00:45 Epoch 2/10 - Batch 23/160, Loss: 0.0624 0.0546\n",
      "training: 0:00:13<0:01:21 Epoch 2/10 - Batch 24/160, Loss: 0.0637 0.0688\n",
      "training: 0:00:15<0:01:28 Epoch 2/10 - Batch 25/160, Loss: 0.0649 0.0700\n",
      "training: 0:00:15<0:01:25 Epoch 2/10 - Batch 26/160, Loss: 0.0656 0.0685\n",
      "training: 0:00:15<0:01:22 Epoch 2/10 - Batch 27/160, Loss: 0.0646 0.0606\n",
      "training: 0:00:16<0:01:19 Epoch 2/10 - Batch 28/160, Loss: 0.0629 0.0560\n",
      "training: 0:00:16<0:01:17 Epoch 2/10 - Batch 29/160, Loss: 0.0626 0.0613\n",
      "training: 0:00:16<0:01:14 Epoch 2/10 - Batch 30/160, Loss: 0.0633 0.0660\n",
      "training: 0:00:16<0:01:12 Epoch 2/10 - Batch 31/160, Loss: 0.0633 0.0633\n",
      "training: 0:00:16<0:01:10 Epoch 2/10 - Batch 32/160, Loss: 0.0639 0.0664\n",
      "training: 0:00:17<0:01:08 Epoch 2/10 - Batch 33/160, Loss: 0.0638 0.0634\n",
      "training: 0:00:17<0:01:06 Epoch 2/10 - Batch 34/160, Loss: 0.0648 0.0687\n",
      "training: 0:00:17<0:01:05 Epoch 2/10 - Batch 35/160, Loss: 0.0650 0.0660\n",
      "training: 0:00:17<0:01:03 Epoch 2/10 - Batch 36/160, Loss: 0.0642 0.0609\n",
      "training: 0:00:17<0:01:01 Epoch 2/10 - Batch 37/160, Loss: 0.0644 0.0651\n",
      "training: 0:00:18<0:01:00 Epoch 2/10 - Batch 38/160, Loss: 0.0641 0.0632\n",
      "training: 0:00:18<0:00:58 Epoch 2/10 - Batch 39/160, Loss: 0.0646 0.0664\n",
      "training: 0:00:18<0:00:57 Epoch 2/10 - Batch 40/160, Loss: 0.0629 0.0561\n",
      "training: 0:00:18<0:00:56 Epoch 2/10 - Batch 41/160, Loss: 0.0614 0.0556\n",
      "training: 0:00:18<0:00:55 Epoch 2/10 - Batch 42/160, Loss: 0.0602 0.0553\n",
      "training: 0:00:19<0:00:53 Epoch 2/10 - Batch 43/160, Loss: 0.0593 0.0559\n",
      "training: 0:00:19<0:00:52 Epoch 2/10 - Batch 44/160, Loss: 0.0571 0.0481\n",
      "training: 0:00:19<0:00:51 Epoch 2/10 - Batch 45/160, Loss: 0.0561 0.0521\n",
      "training: 0:00:19<0:00:50 Epoch 2/10 - Batch 46/160, Loss: 0.0577 0.0640\n",
      "training: 0:00:19<0:00:49 Epoch 2/10 - Batch 47/160, Loss: 0.0594 0.0661\n",
      "training: 0:00:20<0:00:48 Epoch 2/10 - Batch 48/160, Loss: 0.0592 0.0586\n",
      "training: 0:00:20<0:00:47 Epoch 2/10 - Batch 49/160, Loss: 0.0575 0.0508\n",
      "training: 0:00:20<0:00:46 Epoch 2/10 - Batch 50/160, Loss: 0.0590 0.0649\n",
      "training: 0:00:20<0:00:45 Epoch 2/10 - Batch 51/160, Loss: 0.0596 0.0619\n",
      "training: 0:00:20<0:00:44 Epoch 2/10 - Batch 52/160, Loss: 0.0633 0.0781\n",
      "training: 0:00:21<0:00:43 Epoch 2/10 - Batch 53/160, Loss: 0.0626 0.0598\n",
      "training: 0:00:21<0:00:42 Epoch 2/10 - Batch 54/160, Loss: 0.0633 0.0664\n",
      "training: 0:00:21<0:00:42 Epoch 2/10 - Batch 55/160, Loss: 0.0621 0.0573\n",
      "training: 0:00:21<0:00:41 Epoch 2/10 - Batch 56/160, Loss: 0.0611 0.0572\n",
      "training: 0:00:21<0:00:40 Epoch 2/10 - Batch 57/160, Loss: 0.0605 0.0580\n",
      "training: 0:00:22<0:00:39 Epoch 2/10 - Batch 58/160, Loss: 0.0594 0.0548\n",
      "training: 0:00:22<0:00:39 Epoch 2/10 - Batch 59/160, Loss: 0.0590 0.0577\n",
      "training: 0:00:22<0:00:38 Epoch 2/10 - Batch 60/160, Loss: 0.0581 0.0542\n",
      "training: 0:00:22<0:00:37 Epoch 2/10 - Batch 61/160, Loss: 0.0575 0.0554\n",
      "training: 0:00:22<0:00:36 Epoch 2/10 - Batch 62/160, Loss: 0.0589 0.0645\n",
      "training: 0:00:22<0:00:36 Epoch 2/10 - Batch 63/160, Loss: 0.0596 0.0620\n",
      "training: 0:00:23<0:00:35 Epoch 2/10 - Batch 64/160, Loss: 0.0595 0.0593\n",
      "training: 0:00:23<0:00:35 Epoch 2/10 - Batch 65/160, Loss: 0.0618 0.0710\n",
      "training: 0:00:23<0:00:34 Epoch 2/10 - Batch 66/160, Loss: 0.0611 0.0585\n",
      "training: 0:00:23<0:00:33 Epoch 2/10 - Batch 67/160, Loss: 0.0604 0.0572\n",
      "training: 0:00:24<0:00:33 Epoch 2/10 - Batch 68/160, Loss: 0.0602 0.0597\n",
      "training: 0:00:24<0:00:32 Epoch 2/10 - Batch 69/160, Loss: 0.0611 0.0648\n",
      "training: 0:00:24<0:00:32 Epoch 2/10 - Batch 70/160, Loss: 0.0584 0.0474\n",
      "training: 0:00:24<0:00:31 Epoch 2/10 - Batch 71/160, Loss: 0.0580 0.0562\n",
      "training: 0:00:24<0:00:31 Epoch 2/10 - Batch 72/160, Loss: 0.0578 0.0573\n",
      "training: 0:00:24<0:00:30 Epoch 2/10 - Batch 73/160, Loss: 0.0611 0.0742\n",
      "training: 0:00:25<0:00:30 Epoch 2/10 - Batch 74/160, Loss: 0.0614 0.0626\n",
      "training: 0:00:25<0:00:29 Epoch 2/10 - Batch 75/160, Loss: 0.0638 0.0734\n",
      "training: 0:00:25<0:00:28 Epoch 2/10 - Batch 76/160, Loss: 0.0641 0.0654\n",
      "training: 0:00:25<0:00:28 Epoch 2/10 - Batch 77/160, Loss: 0.0636 0.0614\n",
      "training: 0:00:25<0:00:27 Epoch 2/10 - Batch 78/160, Loss: 0.0617 0.0541\n",
      "training: 0:00:26<0:00:27 Epoch 2/10 - Batch 79/160, Loss: 0.0600 0.0535\n",
      "training: 0:00:26<0:00:26 Epoch 2/10 - Batch 80/160, Loss: 0.0581 0.0503\n",
      "training: 0:00:26<0:00:26 Epoch 2/10 - Batch 81/160, Loss: 0.0557 0.0459\n",
      "training: 0:00:26<0:00:25 Epoch 2/10 - Batch 82/160, Loss: 0.0559 0.0567\n",
      "training: 0:00:26<0:00:25 Epoch 2/10 - Batch 83/160, Loss: 0.0535 0.0438\n",
      "training: 0:00:27<0:00:25 Epoch 2/10 - Batch 84/160, Loss: 0.0542 0.0573\n",
      "training: 0:00:27<0:00:24 Epoch 2/10 - Batch 85/160, Loss: 0.0546 0.0560\n",
      "training: 0:00:27<0:00:24 Epoch 2/10 - Batch 86/160, Loss: 0.0548 0.0555\n",
      "training: 0:00:27<0:00:23 Epoch 2/10 - Batch 87/160, Loss: 0.0558 0.0601\n",
      "training: 0:00:27<0:00:23 Epoch 2/10 - Batch 88/160, Loss: 0.0574 0.0636\n",
      "training: 0:00:28<0:00:22 Epoch 2/10 - Batch 89/160, Loss: 0.0550 0.0456\n",
      "training: 0:00:28<0:00:22 Epoch 2/10 - Batch 90/160, Loss: 0.0553 0.0564\n",
      "training: 0:00:28<0:00:22 Epoch 2/10 - Batch 91/160, Loss: 0.0559 0.0585\n",
      "training: 0:00:28<0:00:21 Epoch 2/10 - Batch 92/160, Loss: 0.0558 0.0553\n",
      "training: 0:00:28<0:00:21 Epoch 2/10 - Batch 93/160, Loss: 0.0563 0.0582\n",
      "training: 0:00:28<0:00:20 Epoch 2/10 - Batch 94/160, Loss: 0.0550 0.0499\n",
      "training: 0:00:29<0:00:20 Epoch 2/10 - Batch 95/160, Loss: 0.0527 0.0435\n",
      "training: 0:00:29<0:00:20 Epoch 2/10 - Batch 96/160, Loss: 0.0551 0.0645\n",
      "training: 0:00:29<0:00:19 Epoch 2/10 - Batch 97/160, Loss: 0.0562 0.0610\n",
      "training: 0:00:29<0:00:19 Epoch 2/10 - Batch 98/160, Loss: 0.0563 0.0566\n",
      "training: 0:00:29<0:00:18 Epoch 2/10 - Batch 99/160, Loss: 0.0541 0.0453\n",
      "training: 0:00:30<0:00:18 Epoch 2/10 - Batch 100/160, Loss: 0.0542 0.0546\n",
      "training: 0:00:30<0:00:18 Epoch 2/10 - Batch 101/160, Loss: 0.0521 0.0436\n",
      "training: 0:00:30<0:00:17 Epoch 2/10 - Batch 102/160, Loss: 0.0565 0.0741\n",
      "training: 0:00:52<0:00:29 Epoch 2/10 - Batch 103/160, Loss: 0.0568 0.0582\n",
      "training: 0:00:52<0:00:28 Epoch 2/10 - Batch 104/160, Loss: 0.0566 0.0557\n",
      "training: 0:00:52<0:00:28 Epoch 2/10 - Batch 105/160, Loss: 0.0576 0.0616\n",
      "training: 0:00:52<0:00:27 Epoch 2/10 - Batch 106/160, Loss: 0.0572 0.0556\n",
      "training: 0:00:52<0:00:26 Epoch 2/10 - Batch 107/160, Loss: 0.0558 0.0501\n",
      "training: 0:00:53<0:00:26 Epoch 2/10 - Batch 108/160, Loss: 0.0567 0.0604\n",
      "training: 0:00:53<0:00:25 Epoch 2/10 - Batch 109/160, Loss: 0.0567 0.0569\n",
      "training: 0:00:53<0:00:25 Epoch 2/10 - Batch 110/160, Loss: 0.0579 0.0624\n",
      "training: 0:00:53<0:00:24 Epoch 2/10 - Batch 111/160, Loss: 0.0585 0.0608\n",
      "training: 0:00:53<0:00:23 Epoch 2/10 - Batch 112/160, Loss: 0.0590 0.0614\n",
      "training: 0:00:54<0:00:23 Epoch 2/10 - Batch 113/160, Loss: 0.0562 0.0447\n",
      "training: 0:00:54<0:00:22 Epoch 2/10 - Batch 114/160, Loss: 0.0560 0.0550\n",
      "training: 0:00:54<0:00:21 Epoch 2/10 - Batch 115/160, Loss: 0.0572 0.0623\n",
      "training: 0:00:54<0:00:21 Epoch 2/10 - Batch 116/160, Loss: 0.0579 0.0605\n",
      "training: 0:00:54<0:00:20 Epoch 2/10 - Batch 117/160, Loss: 0.0579 0.0579\n",
      "training: 0:00:54<0:00:20 Epoch 2/10 - Batch 118/160, Loss: 0.0581 0.0590\n",
      "training: 0:00:55<0:00:19 Epoch 2/10 - Batch 119/160, Loss: 0.0595 0.0649\n",
      "training: 0:00:55<0:00:19 Epoch 2/10 - Batch 120/160, Loss: 0.0577 0.0505\n",
      "training: 0:00:55<0:00:18 Epoch 2/10 - Batch 121/160, Loss: 0.0546 0.0422\n",
      "training: 0:00:55<0:00:17 Epoch 2/10 - Batch 122/160, Loss: 0.0528 0.0456\n",
      "training: 0:00:55<0:00:17 Epoch 2/10 - Batch 123/160, Loss: 0.0517 0.0474\n",
      "training: 0:00:56<0:00:16 Epoch 2/10 - Batch 124/160, Loss: 0.0513 0.0496\n",
      "training: 0:00:56<0:00:16 Epoch 2/10 - Batch 125/160, Loss: 0.0513 0.0514\n",
      "training: 0:00:56<0:00:15 Epoch 2/10 - Batch 126/160, Loss: 0.0524 0.0570\n",
      "training: 0:00:56<0:00:15 Epoch 2/10 - Batch 127/160, Loss: 0.0535 0.0578\n",
      "training: 0:00:56<0:00:14 Epoch 2/10 - Batch 128/160, Loss: 0.0523 0.0475\n",
      "training: 0:00:57<0:00:14 Epoch 2/10 - Batch 129/160, Loss: 0.0505 0.0433\n",
      "validation: 0:01:13<0:00:17 Epoch 2/10 - Batch 130/160, Loss: 0.0130 0.0130\n",
      "validation: 0:01:13<0:00:17 Epoch 2/10 - Batch 131/160, Loss: 0.0129 0.0127\n",
      "validation: 0:01:13<0:00:16 Epoch 2/10 - Batch 132/160, Loss: 0.0148 0.0187\n",
      "validation: 0:01:14<0:00:15 Epoch 2/10 - Batch 133/160, Loss: 0.0150 0.0155\n",
      "validation: 0:01:14<0:00:15 Epoch 2/10 - Batch 134/160, Loss: 0.0148 0.0143\n",
      "validation: 0:01:14<0:00:14 Epoch 2/10 - Batch 135/160, Loss: 0.0144 0.0121\n",
      "validation: 0:01:14<0:00:13 Epoch 2/10 - Batch 136/160, Loss: 0.0146 0.0158\n",
      "validation: 0:01:14<0:00:13 Epoch 2/10 - Batch 137/160, Loss: 0.0148 0.0166\n",
      "validation: 0:01:14<0:00:12 Epoch 2/10 - Batch 138/160, Loss: 0.0148 0.0145\n",
      "validation: 0:01:14<0:00:11 Epoch 2/10 - Batch 139/160, Loss: 0.0154 0.0212\n",
      "validation: 0:01:14<0:00:11 Epoch 2/10 - Batch 140/160, Loss: 0.0173 0.0360\n",
      "validation: 0:01:14<0:00:10 Epoch 2/10 - Batch 141/160, Loss: 0.0172 0.0156\n",
      "validation: 0:01:14<0:00:10 Epoch 2/10 - Batch 142/160, Loss: 0.0174 0.0203\n",
      "validation: 0:01:14<0:00:09 Epoch 2/10 - Batch 143/160, Loss: 0.0172 0.0144\n",
      "validation: 0:01:14<0:00:08 Epoch 2/10 - Batch 144/160, Loss: 0.0169 0.0126\n",
      "validation: 0:01:14<0:00:08 Epoch 2/10 - Batch 145/160, Loss: 0.0168 0.0151\n",
      "validation: 0:01:14<0:00:07 Epoch 2/10 - Batch 146/160, Loss: 0.0167 0.0153\n",
      "validation: 0:01:14<0:00:07 Epoch 2/10 - Batch 147/160, Loss: 0.0175 0.0309\n",
      "validation: 0:01:14<0:00:06 Epoch 2/10 - Batch 148/160, Loss: 0.0182 0.0306\n",
      "validation: 0:01:14<0:00:06 Epoch 2/10 - Batch 149/160, Loss: 0.0180 0.0154\n",
      "validation: 0:01:15<0:00:05 Epoch 2/10 - Batch 150/160, Loss: 0.0184 0.0253\n",
      "validation: 0:01:15<0:00:05 Epoch 2/10 - Batch 151/160, Loss: 0.0182 0.0137\n",
      "validation: 0:01:15<0:00:04 Epoch 2/10 - Batch 152/160, Loss: 0.0179 0.0120\n",
      "validation: 0:01:15<0:00:03 Epoch 2/10 - Batch 153/160, Loss: 0.0179 0.0167\n",
      "validation: 0:01:15<0:00:03 Epoch 2/10 - Batch 154/160, Loss: 0.0177 0.0131\n",
      "validation: 0:01:15<0:00:02 Epoch 2/10 - Batch 155/160, Loss: 0.0177 0.0187\n",
      "validation: 0:01:15<0:00:02 Epoch 2/10 - Batch 156/160, Loss: 0.0177 0.0177\n",
      "validation: 0:01:15<0:00:01 Epoch 2/10 - Batch 157/160, Loss: 0.0175 0.0116\n",
      "validation: 0:01:15<0:00:01 Epoch 2/10 - Batch 158/160, Loss: 0.0174 0.0137\n",
      "validation: 0:01:15<0:00:00 Epoch 2/10 - Batch 159/160, Loss: 0.0176 0.0245\n",
      "validation: 0:01:15<0:00:00 Epoch 2/10 - Batch 160/160, Loss: 0.0174 0.0117\n",
      "validation: 0:01:15<0:00:00 Epoch 2/10 - Batch 161/160, Loss: 0.0181 0.0402\n",
      "128 160\n",
      "training: 0:00:00<0:00:34 Epoch 3/10 - Batch 2/160, Loss: 0.0657 0.0657\n",
      "training: 0:00:17<0:23:28 Epoch 3/10 - Batch 3/160, Loss: 0.0566 0.0505\n",
      "training: 0:00:18<0:15:42 Epoch 3/10 - Batch 4/160, Loss: 0.0550 0.0526\n",
      "training: 0:00:18<0:11:49 Epoch 3/10 - Batch 5/160, Loss: 0.0559 0.0581\n",
      "training: 0:00:18<0:09:29 Epoch 3/10 - Batch 6/160, Loss: 0.0543 0.0493\n",
      "training: 0:00:18<0:07:56 Epoch 3/10 - Batch 7/160, Loss: 0.0533 0.0498\n",
      "training: 0:00:18<0:06:49 Epoch 3/10 - Batch 8/160, Loss: 0.0539 0.0564\n",
      "training: 0:00:18<0:05:59 Epoch 3/10 - Batch 9/160, Loss: 0.0543 0.0555\n",
      "training: 0:00:19<0:05:20 Epoch 3/10 - Batch 10/160, Loss: 0.0547 0.0565\n",
      "training: 0:00:19<0:04:49 Epoch 3/10 - Batch 11/160, Loss: 0.0568 0.0651\n",
      "training: 0:00:19<0:04:23 Epoch 3/10 - Batch 12/160, Loss: 0.0555 0.0504\n",
      "training: 0:00:19<0:04:02 Epoch 3/10 - Batch 13/160, Loss: 0.0539 0.0472\n",
      "training: 0:00:19<0:03:44 Epoch 3/10 - Batch 14/160, Loss: 0.0528 0.0483\n",
      "training: 0:00:20<0:03:28 Epoch 3/10 - Batch 15/160, Loss: 0.0544 0.0610\n",
      "training: 0:00:20<0:03:15 Epoch 3/10 - Batch 16/160, Loss: 0.0563 0.0638\n",
      "training: 0:00:20<0:03:03 Epoch 3/10 - Batch 17/160, Loss: 0.0557 0.0531\n",
      "training: 0:00:20<0:02:53 Epoch 3/10 - Batch 18/160, Loss: 0.0532 0.0432\n",
      "training: 0:00:20<0:02:43 Epoch 3/10 - Batch 19/160, Loss: 0.0504 0.0395\n",
      "training: 0:00:20<0:02:35 Epoch 3/10 - Batch 20/160, Loss: 0.0514 0.0552\n",
      "training: 0:00:21<0:02:28 Epoch 3/10 - Batch 21/160, Loss: 0.0526 0.0577\n",
      "training: 0:00:21<0:02:21 Epoch 3/10 - Batch 22/160, Loss: 0.0531 0.0548\n",
      "training: 0:00:21<0:02:15 Epoch 3/10 - Batch 23/160, Loss: 0.0538 0.0566\n",
      "training: 0:00:21<0:02:09 Epoch 3/10 - Batch 24/160, Loss: 0.0562 0.0657\n",
      "training: 0:00:21<0:02:04 Epoch 3/10 - Batch 25/160, Loss: 0.0542 0.0465\n",
      "training: 0:00:22<0:01:59 Epoch 3/10 - Batch 26/160, Loss: 0.0518 0.0423\n",
      "training: 0:00:22<0:01:54 Epoch 3/10 - Batch 27/160, Loss: 0.0516 0.0506\n",
      "training: 0:00:22<0:01:50 Epoch 3/10 - Batch 28/160, Loss: 0.0512 0.0496\n",
      "training: 0:00:22<0:01:46 Epoch 3/10 - Batch 29/160, Loss: 0.0530 0.0604\n",
      "training: 0:00:22<0:01:43 Epoch 3/10 - Batch 30/160, Loss: 0.0505 0.0406\n",
      "training: 0:00:23<0:01:39 Epoch 3/10 - Batch 31/160, Loss: 0.0503 0.0493\n",
      "training: 0:00:23<0:01:36 Epoch 3/10 - Batch 32/160, Loss: 0.0507 0.0523\n",
      "training: 0:00:23<0:01:33 Epoch 3/10 - Batch 33/160, Loss: 0.0506 0.0503\n",
      "training: 0:00:23<0:01:30 Epoch 3/10 - Batch 34/160, Loss: 0.0515 0.0551\n",
      "training: 0:00:23<0:01:28 Epoch 3/10 - Batch 35/160, Loss: 0.0508 0.0482\n",
      "training: 0:00:23<0:01:25 Epoch 3/10 - Batch 36/160, Loss: 0.0510 0.0515\n",
      "training: 0:00:24<0:01:23 Epoch 3/10 - Batch 37/160, Loss: 0.0487 0.0396\n",
      "training: 0:00:24<0:01:20 Epoch 3/10 - Batch 38/160, Loss: 0.0470 0.0402\n",
      "training: 0:00:24<0:01:18 Epoch 3/10 - Batch 39/160, Loss: 0.0473 0.0483\n",
      "training: 0:00:24<0:01:16 Epoch 3/10 - Batch 40/160, Loss: 0.0466 0.0439\n",
      "training: 0:00:24<0:01:14 Epoch 3/10 - Batch 41/160, Loss: 0.0476 0.0514\n",
      "training: 0:00:25<0:01:12 Epoch 3/10 - Batch 42/160, Loss: 0.0479 0.0491\n",
      "training: 0:00:25<0:01:11 Epoch 3/10 - Batch 43/160, Loss: 0.0479 0.0479\n",
      "training: 0:00:25<0:01:09 Epoch 3/10 - Batch 44/160, Loss: 0.0486 0.0513\n",
      "training: 0:00:25<0:01:07 Epoch 3/10 - Batch 45/160, Loss: 0.0492 0.0520\n",
      "training: 0:00:25<0:01:06 Epoch 3/10 - Batch 46/160, Loss: 0.0483 0.0444\n",
      "training: 0:00:26<0:01:04 Epoch 3/10 - Batch 47/160, Loss: 0.0473 0.0435\n",
      "training: 0:00:26<0:01:03 Epoch 3/10 - Batch 48/160, Loss: 0.0464 0.0427\n",
      "training: 0:00:26<0:01:01 Epoch 3/10 - Batch 49/160, Loss: 0.0464 0.0465\n",
      "training: 0:00:26<0:01:00 Epoch 3/10 - Batch 50/160, Loss: 0.0473 0.0509\n",
      "training: 0:00:26<0:00:58 Epoch 3/10 - Batch 51/160, Loss: 0.0490 0.0558\n",
      "training: 0:00:26<0:00:57 Epoch 3/10 - Batch 52/160, Loss: 0.0479 0.0434\n",
      "training: 0:00:27<0:00:56 Epoch 3/10 - Batch 53/160, Loss: 0.0484 0.0503\n",
      "training: 0:00:27<0:00:55 Epoch 3/10 - Batch 54/160, Loss: 0.0480 0.0468\n",
      "training: 0:00:27<0:00:54 Epoch 3/10 - Batch 55/160, Loss: 0.0483 0.0494\n",
      "training: 0:00:27<0:00:53 Epoch 3/10 - Batch 56/160, Loss: 0.0475 0.0442\n",
      "training: 0:00:27<0:00:51 Epoch 3/10 - Batch 57/160, Loss: 0.0453 0.0367\n",
      "training: 0:00:28<0:00:50 Epoch 3/10 - Batch 58/160, Loss: 0.0448 0.0426\n",
      "training: 0:00:28<0:00:49 Epoch 3/10 - Batch 59/160, Loss: 0.0462 0.0519\n",
      "training: 0:00:28<0:00:48 Epoch 3/10 - Batch 60/160, Loss: 0.0460 0.0450\n",
      "training: 0:00:28<0:00:47 Epoch 3/10 - Batch 61/160, Loss: 0.0458 0.0449\n",
      "training: 0:00:28<0:00:46 Epoch 3/10 - Batch 62/160, Loss: 0.0476 0.0547\n",
      "training: 0:00:29<0:00:45 Epoch 3/10 - Batch 63/160, Loss: 0.0492 0.0560\n",
      "training: 0:00:29<0:00:45 Epoch 3/10 - Batch 64/160, Loss: 0.0508 0.0570\n",
      "training: 0:00:29<0:00:44 Epoch 3/10 - Batch 65/160, Loss: 0.0504 0.0488\n",
      "training: 0:00:29<0:00:43 Epoch 3/10 - Batch 66/160, Loss: 0.0509 0.0530\n",
      "training: 0:00:29<0:00:42 Epoch 3/10 - Batch 67/160, Loss: 0.0502 0.0472\n",
      "training: 0:00:30<0:00:41 Epoch 3/10 - Batch 68/160, Loss: 0.0487 0.0427\n",
      "training: 0:00:30<0:00:40 Epoch 3/10 - Batch 69/160, Loss: 0.0493 0.0520\n",
      "training: 0:00:30<0:00:40 Epoch 3/10 - Batch 70/160, Loss: 0.0483 0.0442\n",
      "training: 0:00:30<0:00:39 Epoch 3/10 - Batch 71/160, Loss: 0.0484 0.0489\n",
      "training: 0:00:30<0:00:38 Epoch 3/10 - Batch 72/160, Loss: 0.0482 0.0474\n",
      "training: 0:00:30<0:00:37 Epoch 3/10 - Batch 73/160, Loss: 0.0494 0.0544\n",
      "training: 0:00:31<0:00:37 Epoch 3/10 - Batch 74/160, Loss: 0.0491 0.0475\n",
      "training: 0:00:31<0:00:36 Epoch 3/10 - Batch 75/160, Loss: 0.0481 0.0444\n",
      "training: 0:00:31<0:00:35 Epoch 3/10 - Batch 76/160, Loss: 0.0472 0.0435\n",
      "training: 0:00:31<0:00:35 Epoch 3/10 - Batch 77/160, Loss: 0.0491 0.0567\n",
      "training: 0:00:31<0:00:34 Epoch 3/10 - Batch 78/160, Loss: 0.0483 0.0451\n",
      "training: 0:00:32<0:00:33 Epoch 3/10 - Batch 79/160, Loss: 0.0478 0.0456\n",
      "training: 0:00:32<0:00:33 Epoch 3/10 - Batch 80/160, Loss: 0.0468 0.0428\n",
      "training: 0:00:32<0:00:32 Epoch 3/10 - Batch 81/160, Loss: 0.0477 0.0515\n",
      "training: 0:00:32<0:00:31 Epoch 3/10 - Batch 82/160, Loss: 0.0478 0.0481\n",
      "training: 0:00:32<0:00:31 Epoch 3/10 - Batch 83/160, Loss: 0.0508 0.0626\n",
      "training: 0:00:32<0:00:30 Epoch 3/10 - Batch 84/160, Loss: 0.0503 0.0484\n",
      "training: 0:00:33<0:00:29 Epoch 3/10 - Batch 85/160, Loss: 0.0498 0.0481\n",
      "training: 0:00:33<0:00:29 Epoch 3/10 - Batch 86/160, Loss: 0.0508 0.0545\n",
      "training: 0:00:33<0:00:28 Epoch 3/10 - Batch 87/160, Loss: 0.0515 0.0544\n",
      "training: 0:00:33<0:00:28 Epoch 3/10 - Batch 88/160, Loss: 0.0523 0.0555\n",
      "training: 0:00:33<0:00:27 Epoch 3/10 - Batch 89/160, Loss: 0.0529 0.0552\n",
      "training: 0:00:34<0:00:27 Epoch 3/10 - Batch 90/160, Loss: 0.0512 0.0446\n",
      "training: 0:00:34<0:00:26 Epoch 3/10 - Batch 91/160, Loss: 0.0492 0.0412\n",
      "training: 0:00:34<0:00:26 Epoch 3/10 - Batch 92/160, Loss: 0.0496 0.0512\n",
      "training: 0:00:34<0:00:25 Epoch 3/10 - Batch 93/160, Loss: 0.0502 0.0523\n",
      "training: 0:00:34<0:00:25 Epoch 3/10 - Batch 94/160, Loss: 0.0499 0.0490\n",
      "training: 0:00:35<0:00:24 Epoch 3/10 - Batch 95/160, Loss: 0.0514 0.0572\n",
      "training: 0:00:35<0:00:24 Epoch 3/10 - Batch 96/160, Loss: 0.0509 0.0492\n",
      "training: 0:00:35<0:00:23 Epoch 3/10 - Batch 97/160, Loss: 0.0502 0.0473\n",
      "training: 0:00:35<0:00:23 Epoch 3/10 - Batch 98/160, Loss: 0.0477 0.0376\n",
      "training: 0:00:35<0:00:22 Epoch 3/10 - Batch 99/160, Loss: 0.0477 0.0479\n",
      "training: 0:00:36<0:00:22 Epoch 3/10 - Batch 100/160, Loss: 0.0497 0.0574\n",
      "training: 0:00:36<0:00:21 Epoch 3/10 - Batch 101/160, Loss: 0.0506 0.0541\n",
      "training: 0:00:36<0:00:21 Epoch 3/10 - Batch 102/160, Loss: 0.0482 0.0388\n",
      "training: 0:00:55<0:00:31 Epoch 3/10 - Batch 103/160, Loss: 0.0477 0.0456\n",
      "training: 0:00:55<0:00:30 Epoch 3/10 - Batch 104/160, Loss: 0.0462 0.0401\n",
      "training: 0:00:55<0:00:29 Epoch 3/10 - Batch 105/160, Loss: 0.0475 0.0527\n",
      "training: 0:00:55<0:00:29 Epoch 3/10 - Batch 106/160, Loss: 0.0491 0.0555\n",
      "training: 0:00:55<0:00:28 Epoch 3/10 - Batch 107/160, Loss: 0.0479 0.0433\n",
      "training: 0:00:56<0:00:27 Epoch 3/10 - Batch 108/160, Loss: 0.0484 0.0500\n",
      "training: 0:00:56<0:00:27 Epoch 3/10 - Batch 109/160, Loss: 0.0480 0.0465\n",
      "training: 0:00:56<0:00:26 Epoch 3/10 - Batch 110/160, Loss: 0.0475 0.0458\n",
      "training: 0:00:56<0:00:25 Epoch 3/10 - Batch 111/160, Loss: 0.0483 0.0514\n",
      "training: 0:00:56<0:00:25 Epoch 3/10 - Batch 112/160, Loss: 0.0505 0.0595\n",
      "training: 0:00:57<0:00:24 Epoch 3/10 - Batch 113/160, Loss: 0.0507 0.0511\n",
      "training: 0:00:57<0:00:23 Epoch 3/10 - Batch 114/160, Loss: 0.0516 0.0555\n",
      "training: 0:00:57<0:00:23 Epoch 3/10 - Batch 115/160, Loss: 0.0514 0.0504\n",
      "training: 0:00:57<0:00:22 Epoch 3/10 - Batch 116/160, Loss: 0.0498 0.0434\n",
      "training: 0:00:57<0:00:21 Epoch 3/10 - Batch 117/160, Loss: 0.0490 0.0457\n",
      "training: 0:00:57<0:00:21 Epoch 3/10 - Batch 118/160, Loss: 0.0496 0.0520\n",
      "training: 0:00:58<0:00:20 Epoch 3/10 - Batch 119/160, Loss: 0.0491 0.0474\n",
      "training: 0:00:58<0:00:20 Epoch 3/10 - Batch 120/160, Loss: 0.0477 0.0422\n",
      "training: 0:00:58<0:00:19 Epoch 3/10 - Batch 121/160, Loss: 0.0465 0.0415\n",
      "training: 0:00:58<0:00:18 Epoch 3/10 - Batch 122/160, Loss: 0.0465 0.0467\n",
      "training: 0:00:58<0:00:18 Epoch 3/10 - Batch 123/160, Loss: 0.0476 0.0518\n",
      "training: 0:00:59<0:00:17 Epoch 3/10 - Batch 124/160, Loss: 0.0487 0.0531\n",
      "training: 0:00:59<0:00:17 Epoch 3/10 - Batch 125/160, Loss: 0.0484 0.0472\n",
      "training: 0:00:59<0:00:16 Epoch 3/10 - Batch 126/160, Loss: 0.0487 0.0497\n",
      "training: 0:00:59<0:00:16 Epoch 3/10 - Batch 127/160, Loss: 0.0481 0.0457\n",
      "training: 0:00:59<0:00:15 Epoch 3/10 - Batch 128/160, Loss: 0.0476 0.0455\n",
      "training: 0:01:00<0:00:15 Epoch 3/10 - Batch 129/160, Loss: 0.0477 0.0481\n",
      "validation: 0:01:17<0:00:18 Epoch 3/10 - Batch 130/160, Loss: 0.0097 0.0097\n",
      "validation: 0:01:17<0:00:17 Epoch 3/10 - Batch 131/160, Loss: 0.0095 0.0094\n",
      "validation: 0:01:17<0:00:17 Epoch 3/10 - Batch 132/160, Loss: 0.0129 0.0195\n",
      "validation: 0:01:17<0:00:16 Epoch 3/10 - Batch 133/160, Loss: 0.0130 0.0133\n",
      "validation: 0:01:17<0:00:15 Epoch 3/10 - Batch 134/160, Loss: 0.0121 0.0087\n",
      "validation: 0:01:17<0:00:15 Epoch 3/10 - Batch 135/160, Loss: 0.0129 0.0168\n",
      "validation: 0:01:17<0:00:14 Epoch 3/10 - Batch 136/160, Loss: 0.0132 0.0154\n",
      "validation: 0:01:17<0:00:13 Epoch 3/10 - Batch 137/160, Loss: 0.0131 0.0124\n",
      "validation: 0:01:17<0:00:13 Epoch 3/10 - Batch 138/160, Loss: 0.0128 0.0103\n",
      "validation: 0:01:17<0:00:12 Epoch 3/10 - Batch 139/160, Loss: 0.0126 0.0107\n",
      "validation: 0:01:17<0:00:11 Epoch 3/10 - Batch 140/160, Loss: 0.0125 0.0112\n",
      "validation: 0:01:17<0:00:11 Epoch 3/10 - Batch 141/160, Loss: 0.0124 0.0115\n",
      "validation: 0:01:18<0:00:10 Epoch 3/10 - Batch 142/160, Loss: 0.0124 0.0130\n",
      "validation: 0:01:18<0:00:09 Epoch 3/10 - Batch 143/160, Loss: 0.0121 0.0079\n",
      "validation: 0:01:18<0:00:09 Epoch 3/10 - Batch 144/160, Loss: 0.0119 0.0094\n",
      "validation: 0:01:18<0:00:08 Epoch 3/10 - Batch 145/160, Loss: 0.0117 0.0078\n",
      "validation: 0:01:18<0:00:08 Epoch 3/10 - Batch 146/160, Loss: 0.0115 0.0094\n",
      "validation: 0:01:18<0:00:07 Epoch 3/10 - Batch 147/160, Loss: 0.0116 0.0117\n",
      "validation: 0:01:18<0:00:06 Epoch 3/10 - Batch 148/160, Loss: 0.0118 0.0170\n",
      "validation: 0:01:18<0:00:06 Epoch 3/10 - Batch 149/160, Loss: 0.0119 0.0121\n",
      "validation: 0:01:18<0:00:05 Epoch 3/10 - Batch 150/160, Loss: 0.0117 0.0094\n",
      "validation: 0:01:18<0:00:05 Epoch 3/10 - Batch 151/160, Loss: 0.0117 0.0104\n",
      "validation: 0:01:18<0:00:04 Epoch 3/10 - Batch 152/160, Loss: 0.0117 0.0117\n",
      "validation: 0:01:18<0:00:04 Epoch 3/10 - Batch 153/160, Loss: 0.0115 0.0082\n",
      "validation: 0:01:18<0:00:03 Epoch 3/10 - Batch 154/160, Loss: 0.0114 0.0084\n",
      "validation: 0:01:18<0:00:03 Epoch 3/10 - Batch 155/160, Loss: 0.0113 0.0083\n",
      "validation: 0:01:18<0:00:02 Epoch 3/10 - Batch 156/160, Loss: 0.0114 0.0140\n",
      "validation: 0:01:18<0:00:02 Epoch 3/10 - Batch 157/160, Loss: 0.0114 0.0125\n",
      "validation: 0:01:18<0:00:01 Epoch 3/10 - Batch 158/160, Loss: 0.0113 0.0083\n",
      "validation: 0:01:19<0:00:01 Epoch 3/10 - Batch 159/160, Loss: 0.0115 0.0154\n",
      "validation: 0:01:19<0:00:00 Epoch 3/10 - Batch 160/160, Loss: 0.0114 0.0112\n",
      "validation: 0:01:19<0:00:00 Epoch 3/10 - Batch 161/160, Loss: 0.0114 0.0104\n",
      "128 160\n",
      "training: 0:00:00<0:00:29 Epoch 4/10 - Batch 2/160, Loss: 0.0450 0.0450\n",
      "training: 0:00:01<0:01:50 Epoch 4/10 - Batch 3/160, Loss: 0.0509 0.0548\n",
      "training: 0:00:01<0:01:42 Epoch 4/10 - Batch 4/160, Loss: 0.0490 0.0462\n",
      "training: 0:00:02<0:01:24 Epoch 4/10 - Batch 5/160, Loss: 0.0525 0.0606\n",
      "training: 0:00:02<0:01:13 Epoch 4/10 - Batch 6/160, Loss: 0.0494 0.0404\n",
      "training: 0:00:02<0:01:06 Epoch 4/10 - Batch 7/160, Loss: 0.0494 0.0493\n",
      "training: 0:00:02<0:01:01 Epoch 4/10 - Batch 8/160, Loss: 0.0493 0.0488\n",
      "training: 0:00:03<0:00:57 Epoch 4/10 - Batch 9/160, Loss: 0.0472 0.0392\n",
      "training: 0:00:03<0:00:53 Epoch 4/10 - Batch 10/160, Loss: 0.0482 0.0520\n",
      "training: 0:00:03<0:00:51 Epoch 4/10 - Batch 11/160, Loss: 0.0472 0.0434\n",
      "training: 0:00:03<0:00:48 Epoch 4/10 - Batch 12/160, Loss: 0.0465 0.0435\n",
      "training: 0:00:03<0:00:46 Epoch 4/10 - Batch 13/160, Loss: 0.0450 0.0391\n",
      "training: 0:00:04<0:00:45 Epoch 4/10 - Batch 14/160, Loss: 0.0457 0.0485\n",
      "training: 0:00:04<0:00:43 Epoch 4/10 - Batch 15/160, Loss: 0.0444 0.0394\n",
      "training: 0:00:04<0:00:42 Epoch 4/10 - Batch 16/160, Loss: 0.0446 0.0453\n",
      "training: 0:00:04<0:00:41 Epoch 4/10 - Batch 17/160, Loss: 0.0431 0.0370\n",
      "training: 0:00:04<0:00:39 Epoch 4/10 - Batch 18/160, Loss: 0.0441 0.0480\n",
      "training: 0:00:04<0:00:38 Epoch 4/10 - Batch 19/160, Loss: 0.0438 0.0427\n",
      "training: 0:00:05<0:00:37 Epoch 4/10 - Batch 20/160, Loss: 0.0439 0.0442\n",
      "training: 0:00:05<0:00:37 Epoch 4/10 - Batch 21/160, Loss: 0.0441 0.0449\n",
      "training: 0:00:05<0:00:36 Epoch 4/10 - Batch 22/160, Loss: 0.0449 0.0484\n",
      "training: 0:00:05<0:00:35 Epoch 4/10 - Batch 23/160, Loss: 0.0458 0.0494\n",
      "training: 0:00:05<0:00:34 Epoch 4/10 - Batch 24/160, Loss: 0.0446 0.0396\n",
      "training: 0:00:06<0:00:34 Epoch 4/10 - Batch 25/160, Loss: 0.0448 0.0456\n",
      "training: 0:00:06<0:00:33 Epoch 4/10 - Batch 26/160, Loss: 0.0440 0.0411\n",
      "training: 0:00:06<0:00:33 Epoch 4/10 - Batch 27/160, Loss: 0.0441 0.0443\n",
      "training: 0:00:21<0:01:46 Epoch 4/10 - Batch 28/160, Loss: 0.0461 0.0540\n",
      "training: 0:00:21<0:01:43 Epoch 4/10 - Batch 29/160, Loss: 0.0469 0.0503\n",
      "training: 0:00:22<0:01:40 Epoch 4/10 - Batch 30/160, Loss: 0.0469 0.0470\n",
      "training: 0:00:22<0:01:37 Epoch 4/10 - Batch 31/160, Loss: 0.0493 0.0588\n",
      "training: 0:00:22<0:01:34 Epoch 4/10 - Batch 32/160, Loss: 0.0499 0.0522\n",
      "training: 0:00:22<0:01:31 Epoch 4/10 - Batch 33/160, Loss: 0.0502 0.0515\n",
      "training: 0:00:23<0:01:28 Epoch 4/10 - Batch 34/160, Loss: 0.0492 0.0453\n",
      "training: 0:00:23<0:01:26 Epoch 4/10 - Batch 35/160, Loss: 0.0484 0.0451\n",
      "training: 0:00:23<0:01:24 Epoch 4/10 - Batch 36/160, Loss: 0.0491 0.0517\n",
      "training: 0:00:23<0:01:21 Epoch 4/10 - Batch 37/160, Loss: 0.0475 0.0414\n",
      "training: 0:00:23<0:01:19 Epoch 4/10 - Batch 38/160, Loss: 0.0466 0.0427\n",
      "training: 0:00:24<0:01:17 Epoch 4/10 - Batch 39/160, Loss: 0.0451 0.0394\n",
      "training: 0:00:24<0:01:15 Epoch 4/10 - Batch 40/160, Loss: 0.0439 0.0391\n",
      "training: 0:00:24<0:01:13 Epoch 4/10 - Batch 41/160, Loss: 0.0428 0.0385\n",
      "training: 0:00:24<0:01:11 Epoch 4/10 - Batch 42/160, Loss: 0.0426 0.0418\n",
      "training: 0:00:24<0:01:09 Epoch 4/10 - Batch 43/160, Loss: 0.0432 0.0457\n",
      "training: 0:00:25<0:01:08 Epoch 4/10 - Batch 44/160, Loss: 0.0452 0.0529\n",
      "training: 0:00:25<0:01:06 Epoch 4/10 - Batch 45/160, Loss: 0.0445 0.0417\n",
      "training: 0:00:25<0:01:04 Epoch 4/10 - Batch 46/160, Loss: 0.0437 0.0404\n",
      "training: 0:00:25<0:01:03 Epoch 4/10 - Batch 47/160, Loss: 0.0451 0.0509\n",
      "training: 0:00:25<0:01:02 Epoch 4/10 - Batch 48/160, Loss: 0.0445 0.0419\n",
      "training: 0:00:25<0:01:00 Epoch 4/10 - Batch 49/160, Loss: 0.0446 0.0452\n",
      "training: 0:00:26<0:00:59 Epoch 4/10 - Batch 50/160, Loss: 0.0464 0.0535\n",
      "training: 0:00:26<0:00:57 Epoch 4/10 - Batch 51/160, Loss: 0.0468 0.0481\n",
      "training: 0:00:26<0:00:56 Epoch 4/10 - Batch 52/160, Loss: 0.0468 0.0471\n",
      "training: 0:00:26<0:00:55 Epoch 4/10 - Batch 53/160, Loss: 0.0465 0.0451\n",
      "training: 0:00:26<0:00:54 Epoch 4/10 - Batch 54/160, Loss: 0.0454 0.0410\n",
      "training: 0:00:27<0:00:53 Epoch 4/10 - Batch 55/160, Loss: 0.0471 0.0539\n",
      "training: 0:00:27<0:00:52 Epoch 4/10 - Batch 56/160, Loss: 0.0472 0.0475\n",
      "training: 0:00:27<0:00:50 Epoch 4/10 - Batch 57/160, Loss: 0.0446 0.0345\n",
      "training: 0:00:27<0:00:49 Epoch 4/10 - Batch 58/160, Loss: 0.0463 0.0527\n",
      "training: 0:00:27<0:00:48 Epoch 4/10 - Batch 59/160, Loss: 0.0468 0.0490\n",
      "training: 0:00:27<0:00:47 Epoch 4/10 - Batch 60/160, Loss: 0.0462 0.0439\n",
      "training: 0:00:28<0:00:46 Epoch 4/10 - Batch 61/160, Loss: 0.0449 0.0398\n",
      "training: 0:00:28<0:00:46 Epoch 4/10 - Batch 62/160, Loss: 0.0436 0.0381\n",
      "training: 0:00:28<0:00:45 Epoch 4/10 - Batch 63/160, Loss: 0.0438 0.0448\n",
      "training: 0:00:28<0:00:44 Epoch 4/10 - Batch 64/160, Loss: 0.0438 0.0436\n",
      "training: 0:00:28<0:00:43 Epoch 4/10 - Batch 65/160, Loss: 0.0439 0.0446\n",
      "training: 0:00:29<0:00:42 Epoch 4/10 - Batch 66/160, Loss: 0.0426 0.0373\n",
      "training: 0:00:29<0:00:41 Epoch 4/10 - Batch 67/160, Loss: 0.0424 0.0415\n",
      "training: 0:00:29<0:00:40 Epoch 4/10 - Batch 68/160, Loss: 0.0424 0.0426\n",
      "training: 0:00:29<0:00:40 Epoch 4/10 - Batch 69/160, Loss: 0.0436 0.0484\n",
      "training: 0:00:29<0:00:39 Epoch 4/10 - Batch 70/160, Loss: 0.0456 0.0538\n",
      "training: 0:00:30<0:00:38 Epoch 4/10 - Batch 71/160, Loss: 0.0457 0.0460\n",
      "training: 0:00:30<0:00:37 Epoch 4/10 - Batch 72/160, Loss: 0.0462 0.0480\n",
      "training: 0:00:30<0:00:37 Epoch 4/10 - Batch 73/160, Loss: 0.0449 0.0397\n",
      "training: 0:00:30<0:00:36 Epoch 4/10 - Batch 74/160, Loss: 0.0425 0.0332\n",
      "training: 0:00:30<0:00:35 Epoch 4/10 - Batch 75/160, Loss: 0.0431 0.0453\n",
      "training: 0:00:30<0:00:35 Epoch 4/10 - Batch 76/160, Loss: 0.0439 0.0469\n",
      "training: 0:00:31<0:00:34 Epoch 4/10 - Batch 77/160, Loss: 0.0437 0.0430\n",
      "training: 0:00:31<0:00:33 Epoch 4/10 - Batch 78/160, Loss: 0.0437 0.0437\n",
      "training: 0:00:31<0:00:33 Epoch 4/10 - Batch 79/160, Loss: 0.0430 0.0404\n",
      "training: 0:00:31<0:00:32 Epoch 4/10 - Batch 80/160, Loss: 0.0438 0.0466\n",
      "training: 0:00:31<0:00:31 Epoch 4/10 - Batch 81/160, Loss: 0.0436 0.0430\n",
      "training: 0:00:32<0:00:31 Epoch 4/10 - Batch 82/160, Loss: 0.0458 0.0548\n",
      "training: 0:00:32<0:00:30 Epoch 4/10 - Batch 83/160, Loss: 0.0445 0.0389\n",
      "training: 0:00:32<0:00:30 Epoch 4/10 - Batch 84/160, Loss: 0.0454 0.0493\n",
      "training: 0:00:32<0:00:29 Epoch 4/10 - Batch 85/160, Loss: 0.0441 0.0390\n",
      "training: 0:00:32<0:00:29 Epoch 4/10 - Batch 86/160, Loss: 0.0434 0.0405\n",
      "training: 0:00:33<0:00:28 Epoch 4/10 - Batch 87/160, Loss: 0.0424 0.0382\n",
      "training: 0:00:33<0:00:27 Epoch 4/10 - Batch 88/160, Loss: 0.0446 0.0534\n",
      "training: 0:00:33<0:00:27 Epoch 4/10 - Batch 89/160, Loss: 0.0434 0.0386\n",
      "training: 0:00:33<0:00:26 Epoch 4/10 - Batch 90/160, Loss: 0.0431 0.0419\n",
      "training: 0:00:33<0:00:26 Epoch 4/10 - Batch 91/160, Loss: 0.0425 0.0404\n",
      "training: 0:00:34<0:00:25 Epoch 4/10 - Batch 92/160, Loss: 0.0441 0.0506\n",
      "training: 0:00:34<0:00:25 Epoch 4/10 - Batch 93/160, Loss: 0.0421 0.0338\n",
      "training: 0:00:34<0:00:24 Epoch 4/10 - Batch 94/160, Loss: 0.0433 0.0481\n",
      "training: 0:00:34<0:00:24 Epoch 4/10 - Batch 95/160, Loss: 0.0421 0.0372\n",
      "training: 0:00:34<0:00:23 Epoch 4/10 - Batch 96/160, Loss: 0.0418 0.0406\n",
      "training: 0:00:35<0:00:23 Epoch 4/10 - Batch 97/160, Loss: 0.0418 0.0418\n",
      "training: 0:00:35<0:00:22 Epoch 4/10 - Batch 98/160, Loss: 0.0414 0.0401\n",
      "training: 0:00:35<0:00:22 Epoch 4/10 - Batch 99/160, Loss: 0.0441 0.0548\n",
      "training: 0:00:35<0:00:21 Epoch 4/10 - Batch 100/160, Loss: 0.0437 0.0423\n",
      "training: 0:00:35<0:00:21 Epoch 4/10 - Batch 101/160, Loss: 0.0430 0.0402\n",
      "training: 0:00:35<0:00:21 Epoch 4/10 - Batch 102/160, Loss: 0.0419 0.0373\n",
      "training: 0:00:57<0:00:32 Epoch 4/10 - Batch 103/160, Loss: 0.0437 0.0509\n",
      "training: 0:00:58<0:00:32 Epoch 4/10 - Batch 104/160, Loss: 0.0440 0.0455\n",
      "training: 0:00:58<0:00:31 Epoch 4/10 - Batch 105/160, Loss: 0.0437 0.0421\n",
      "training: 0:00:58<0:00:30 Epoch 4/10 - Batch 106/160, Loss: 0.0436 0.0433\n",
      "training: 0:00:58<0:00:29 Epoch 4/10 - Batch 107/160, Loss: 0.0438 0.0445\n",
      "training: 0:00:58<0:00:29 Epoch 4/10 - Batch 108/160, Loss: 0.0440 0.0448\n",
      "training: 0:00:58<0:00:28 Epoch 4/10 - Batch 109/160, Loss: 0.0435 0.0415\n",
      "training: 0:00:59<0:00:27 Epoch 4/10 - Batch 110/160, Loss: 0.0445 0.0483\n",
      "training: 0:00:59<0:00:26 Epoch 4/10 - Batch 111/160, Loss: 0.0445 0.0447\n",
      "training: 0:00:59<0:00:26 Epoch 4/10 - Batch 112/160, Loss: 0.0454 0.0491\n",
      "training: 0:00:59<0:00:25 Epoch 4/10 - Batch 113/160, Loss: 0.0442 0.0395\n",
      "training: 0:00:59<0:00:24 Epoch 4/10 - Batch 114/160, Loss: 0.0449 0.0474\n",
      "training: 0:01:00<0:00:24 Epoch 4/10 - Batch 115/160, Loss: 0.0475 0.0580\n",
      "training: 0:01:00<0:00:23 Epoch 4/10 - Batch 116/160, Loss: 0.0459 0.0395\n",
      "training: 0:01:00<0:00:22 Epoch 4/10 - Batch 117/160, Loss: 0.0449 0.0407\n",
      "training: 0:01:00<0:00:22 Epoch 4/10 - Batch 118/160, Loss: 0.0451 0.0461\n",
      "training: 0:01:00<0:00:21 Epoch 4/10 - Batch 119/160, Loss: 0.0460 0.0495\n",
      "training: 0:01:01<0:00:21 Epoch 4/10 - Batch 120/160, Loss: 0.0443 0.0375\n",
      "training: 0:01:01<0:00:20 Epoch 4/10 - Batch 121/160, Loss: 0.0456 0.0507\n",
      "training: 0:01:01<0:00:19 Epoch 4/10 - Batch 122/160, Loss: 0.0450 0.0425\n",
      "training: 0:01:01<0:00:19 Epoch 4/10 - Batch 123/160, Loss: 0.0460 0.0500\n",
      "training: 0:01:01<0:00:18 Epoch 4/10 - Batch 124/160, Loss: 0.0449 0.0404\n",
      "training: 0:01:01<0:00:17 Epoch 4/10 - Batch 125/160, Loss: 0.0449 0.0452\n",
      "training: 0:01:02<0:00:17 Epoch 4/10 - Batch 126/160, Loss: 0.0435 0.0376\n",
      "training: 0:01:02<0:00:16 Epoch 4/10 - Batch 127/160, Loss: 0.0426 0.0393\n",
      "training: 0:01:02<0:00:16 Epoch 4/10 - Batch 128/160, Loss: 0.0429 0.0441\n",
      "training: 0:01:02<0:00:15 Epoch 4/10 - Batch 129/160, Loss: 0.0437 0.0470\n",
      "validation: 0:01:25<0:00:20 Epoch 4/10 - Batch 130/160, Loss: 0.0082 0.0082\n",
      "validation: 0:01:25<0:00:19 Epoch 4/10 - Batch 131/160, Loss: 0.0088 0.0093\n",
      "validation: 0:01:25<0:00:18 Epoch 4/10 - Batch 132/160, Loss: 0.0090 0.0095\n",
      "validation: 0:01:25<0:00:18 Epoch 4/10 - Batch 133/160, Loss: 0.0101 0.0136\n",
      "validation: 0:01:25<0:00:17 Epoch 4/10 - Batch 134/160, Loss: 0.0113 0.0161\n",
      "validation: 0:01:25<0:00:16 Epoch 4/10 - Batch 135/160, Loss: 0.0112 0.0106\n",
      "validation: 0:01:25<0:00:15 Epoch 4/10 - Batch 136/160, Loss: 0.0122 0.0181\n",
      "validation: 0:01:25<0:00:15 Epoch 4/10 - Batch 137/160, Loss: 0.0116 0.0073\n",
      "validation: 0:01:25<0:00:14 Epoch 4/10 - Batch 138/160, Loss: 0.0113 0.0093\n",
      "validation: 0:01:25<0:00:13 Epoch 4/10 - Batch 139/160, Loss: 0.0118 0.0163\n",
      "validation: 0:01:25<0:00:12 Epoch 4/10 - Batch 140/160, Loss: 0.0117 0.0102\n",
      "validation: 0:01:25<0:00:12 Epoch 4/10 - Batch 141/160, Loss: 0.0123 0.0190\n",
      "validation: 0:01:25<0:00:11 Epoch 4/10 - Batch 142/160, Loss: 0.0122 0.0108\n",
      "validation: 0:01:25<0:00:10 Epoch 4/10 - Batch 143/160, Loss: 0.0119 0.0080\n",
      "validation: 0:01:25<0:00:10 Epoch 4/10 - Batch 144/160, Loss: 0.0117 0.0097\n",
      "validation: 0:01:26<0:00:09 Epoch 4/10 - Batch 145/160, Loss: 0.0117 0.0118\n",
      "validation: 0:01:26<0:00:08 Epoch 4/10 - Batch 146/160, Loss: 0.0116 0.0095\n",
      "validation: 0:01:26<0:00:08 Epoch 4/10 - Batch 147/160, Loss: 0.0117 0.0127\n",
      "validation: 0:01:26<0:00:07 Epoch 4/10 - Batch 148/160, Loss: 0.0115 0.0084\n",
      "validation: 0:01:26<0:00:06 Epoch 4/10 - Batch 149/160, Loss: 0.0119 0.0189\n",
      "validation: 0:01:26<0:00:06 Epoch 4/10 - Batch 150/160, Loss: 0.0119 0.0121\n",
      "validation: 0:01:26<0:00:05 Epoch 4/10 - Batch 151/160, Loss: 0.0117 0.0080\n",
      "validation: 0:01:26<0:00:05 Epoch 4/10 - Batch 152/160, Loss: 0.0115 0.0081\n",
      "validation: 0:01:26<0:00:04 Epoch 4/10 - Batch 153/160, Loss: 0.0114 0.0091\n",
      "validation: 0:01:26<0:00:03 Epoch 4/10 - Batch 154/160, Loss: 0.0113 0.0083\n",
      "validation: 0:01:26<0:00:03 Epoch 4/10 - Batch 155/160, Loss: 0.0117 0.0215\n",
      "validation: 0:01:26<0:00:02 Epoch 4/10 - Batch 156/160, Loss: 0.0116 0.0090\n",
      "validation: 0:01:26<0:00:02 Epoch 4/10 - Batch 157/160, Loss: 0.0115 0.0094\n",
      "validation: 0:01:26<0:00:01 Epoch 4/10 - Batch 158/160, Loss: 0.0114 0.0089\n",
      "validation: 0:01:26<0:00:01 Epoch 4/10 - Batch 159/160, Loss: 0.0117 0.0198\n",
      "validation: 0:01:26<0:00:00 Epoch 4/10 - Batch 160/160, Loss: 0.0119 0.0167\n",
      "validation: 0:01:26<0:00:00 Epoch 4/10 - Batch 161/160, Loss: 0.0118 0.0093\n",
      "128 160\n",
      "training: 0:00:00<0:00:18 Epoch 5/10 - Batch 2/160, Loss: 0.0421 0.0421\n",
      "training: 0:00:22<0:30:11 Epoch 5/10 - Batch 3/160, Loss: 0.0392 0.0372\n",
      "training: 0:00:23<0:20:10 Epoch 5/10 - Batch 4/160, Loss: 0.0378 0.0358\n",
      "training: 0:00:23<0:15:08 Epoch 5/10 - Batch 5/160, Loss: 0.0401 0.0453\n",
      "training: 0:00:23<0:12:07 Epoch 5/10 - Batch 6/160, Loss: 0.0401 0.0402\n",
      "training: 0:00:23<0:10:07 Epoch 5/10 - Batch 7/160, Loss: 0.0402 0.0403\n",
      "training: 0:00:23<0:08:41 Epoch 5/10 - Batch 8/160, Loss: 0.0411 0.0444\n",
      "training: 0:00:24<0:07:36 Epoch 5/10 - Batch 9/160, Loss: 0.0420 0.0457\n",
      "training: 0:00:24<0:06:46 Epoch 5/10 - Batch 10/160, Loss: 0.0402 0.0331\n",
      "training: 0:00:24<0:06:05 Epoch 5/10 - Batch 11/160, Loss: 0.0404 0.0414\n",
      "training: 0:00:24<0:05:32 Epoch 5/10 - Batch 12/160, Loss: 0.0407 0.0417\n",
      "training: 0:00:24<0:05:05 Epoch 5/10 - Batch 13/160, Loss: 0.0409 0.0418\n",
      "training: 0:00:24<0:04:42 Epoch 5/10 - Batch 14/160, Loss: 0.0411 0.0417\n",
      "training: 0:00:25<0:04:22 Epoch 5/10 - Batch 15/160, Loss: 0.0421 0.0462\n",
      "training: 0:00:25<0:04:04 Epoch 5/10 - Batch 16/160, Loss: 0.0413 0.0383\n",
      "training: 0:00:25<0:03:49 Epoch 5/10 - Batch 17/160, Loss: 0.0427 0.0482\n",
      "training: 0:00:25<0:03:36 Epoch 5/10 - Batch 18/160, Loss: 0.0435 0.0469\n",
      "training: 0:00:25<0:03:24 Epoch 5/10 - Batch 19/160, Loss: 0.0437 0.0445\n",
      "training: 0:00:26<0:03:13 Epoch 5/10 - Batch 20/160, Loss: 0.0433 0.0416\n",
      "training: 0:00:26<0:03:03 Epoch 5/10 - Batch 21/160, Loss: 0.0439 0.0464\n",
      "training: 0:00:26<0:02:54 Epoch 5/10 - Batch 22/160, Loss: 0.0459 0.0539\n",
      "training: 0:00:26<0:02:46 Epoch 5/10 - Batch 23/160, Loss: 0.0461 0.0470\n",
      "training: 0:00:26<0:02:39 Epoch 5/10 - Batch 24/160, Loss: 0.0454 0.0424\n",
      "training: 0:00:26<0:02:32 Epoch 5/10 - Batch 25/160, Loss: 0.0449 0.0428\n",
      "training: 0:00:27<0:02:26 Epoch 5/10 - Batch 26/160, Loss: 0.0455 0.0481\n",
      "training: 0:00:27<0:02:20 Epoch 5/10 - Batch 27/160, Loss: 0.0450 0.0427\n",
      "training: 0:00:27<0:02:15 Epoch 5/10 - Batch 28/160, Loss: 0.0468 0.0541\n",
      "training: 0:00:27<0:02:10 Epoch 5/10 - Batch 29/160, Loss: 0.0462 0.0437\n",
      "training: 0:00:27<0:02:06 Epoch 5/10 - Batch 30/160, Loss: 0.0462 0.0464\n",
      "training: 0:00:28<0:02:01 Epoch 5/10 - Batch 31/160, Loss: 0.0454 0.0422\n",
      "training: 0:00:28<0:01:57 Epoch 5/10 - Batch 32/160, Loss: 0.0444 0.0404\n",
      "training: 0:00:28<0:01:53 Epoch 5/10 - Batch 33/160, Loss: 0.0450 0.0476\n",
      "training: 0:00:28<0:01:50 Epoch 5/10 - Batch 34/160, Loss: 0.0441 0.0404\n",
      "training: 0:00:28<0:01:46 Epoch 5/10 - Batch 35/160, Loss: 0.0443 0.0451\n",
      "training: 0:00:28<0:01:43 Epoch 5/10 - Batch 36/160, Loss: 0.0464 0.0549\n",
      "training: 0:00:29<0:01:40 Epoch 5/10 - Batch 37/160, Loss: 0.0453 0.0406\n",
      "training: 0:00:29<0:01:37 Epoch 5/10 - Batch 38/160, Loss: 0.0453 0.0452\n",
      "training: 0:00:29<0:01:34 Epoch 5/10 - Batch 39/160, Loss: 0.0451 0.0445\n",
      "training: 0:00:29<0:01:32 Epoch 5/10 - Batch 40/160, Loss: 0.0450 0.0445\n",
      "training: 0:00:29<0:01:29 Epoch 5/10 - Batch 41/160, Loss: 0.0447 0.0433\n",
      "training: 0:00:30<0:01:27 Epoch 5/10 - Batch 42/160, Loss: 0.0440 0.0412\n",
      "training: 0:00:30<0:01:25 Epoch 5/10 - Batch 43/160, Loss: 0.0424 0.0359\n",
      "training: 0:00:30<0:01:22 Epoch 5/10 - Batch 44/160, Loss: 0.0443 0.0522\n",
      "training: 0:00:30<0:01:20 Epoch 5/10 - Batch 45/160, Loss: 0.0440 0.0427\n",
      "training: 0:00:30<0:01:18 Epoch 5/10 - Batch 46/160, Loss: 0.0431 0.0392\n",
      "training: 0:00:31<0:01:16 Epoch 5/10 - Batch 47/160, Loss: 0.0422 0.0389\n",
      "training: 0:00:31<0:01:15 Epoch 5/10 - Batch 48/160, Loss: 0.0403 0.0327\n",
      "training: 0:00:31<0:01:13 Epoch 5/10 - Batch 49/160, Loss: 0.0425 0.0513\n",
      "training: 0:00:31<0:01:11 Epoch 5/10 - Batch 50/160, Loss: 0.0426 0.0431\n",
      "training: 0:00:31<0:01:10 Epoch 5/10 - Batch 51/160, Loss: 0.0416 0.0376\n",
      "training: 0:00:32<0:01:08 Epoch 5/10 - Batch 52/160, Loss: 0.0415 0.0408\n",
      "training: 0:00:32<0:01:07 Epoch 5/10 - Batch 53/160, Loss: 0.0421 0.0447\n",
      "training: 0:00:32<0:01:05 Epoch 5/10 - Batch 54/160, Loss: 0.0431 0.0471\n",
      "training: 0:00:32<0:01:04 Epoch 5/10 - Batch 55/160, Loss: 0.0433 0.0443\n",
      "training: 0:00:33<0:01:03 Epoch 5/10 - Batch 56/160, Loss: 0.0442 0.0476\n",
      "training: 0:00:33<0:01:01 Epoch 5/10 - Batch 57/160, Loss: 0.0444 0.0451\n",
      "training: 0:00:33<0:01:00 Epoch 5/10 - Batch 58/160, Loss: 0.0452 0.0485\n",
      "training: 0:00:33<0:00:59 Epoch 5/10 - Batch 59/160, Loss: 0.0454 0.0461\n",
      "training: 0:00:33<0:00:57 Epoch 5/10 - Batch 60/160, Loss: 0.0476 0.0564\n",
      "training: 0:00:34<0:00:56 Epoch 5/10 - Batch 61/160, Loss: 0.0480 0.0494\n",
      "training: 0:00:34<0:00:55 Epoch 5/10 - Batch 62/160, Loss: 0.0476 0.0463\n",
      "training: 0:00:34<0:00:54 Epoch 5/10 - Batch 63/160, Loss: 0.0467 0.0429\n",
      "training: 0:00:34<0:00:53 Epoch 5/10 - Batch 64/160, Loss: 0.0444 0.0355\n",
      "training: 0:00:34<0:00:52 Epoch 5/10 - Batch 65/160, Loss: 0.0445 0.0446\n",
      "training: 0:00:35<0:00:51 Epoch 5/10 - Batch 66/160, Loss: 0.0437 0.0408\n",
      "training: 0:00:35<0:00:50 Epoch 5/10 - Batch 67/160, Loss: 0.0421 0.0356\n",
      "training: 0:00:35<0:00:49 Epoch 5/10 - Batch 68/160, Loss: 0.0413 0.0382\n",
      "training: 0:00:35<0:00:48 Epoch 5/10 - Batch 69/160, Loss: 0.0398 0.0337\n",
      "training: 0:00:35<0:00:47 Epoch 5/10 - Batch 70/160, Loss: 0.0416 0.0489\n",
      "training: 0:00:36<0:00:46 Epoch 5/10 - Batch 71/160, Loss: 0.0408 0.0375\n",
      "training: 0:00:36<0:00:45 Epoch 5/10 - Batch 72/160, Loss: 0.0423 0.0482\n",
      "training: 0:00:36<0:00:44 Epoch 5/10 - Batch 73/160, Loss: 0.0425 0.0435\n",
      "training: 0:00:36<0:00:43 Epoch 5/10 - Batch 74/160, Loss: 0.0431 0.0453\n",
      "training: 0:00:36<0:00:42 Epoch 5/10 - Batch 75/160, Loss: 0.0423 0.0393\n",
      "training: 0:00:37<0:00:42 Epoch 5/10 - Batch 76/160, Loss: 0.0428 0.0449\n",
      "training: 0:00:37<0:00:41 Epoch 5/10 - Batch 77/160, Loss: 0.0443 0.0499\n",
      "training: 0:00:37<0:00:40 Epoch 5/10 - Batch 78/160, Loss: 0.0457 0.0515\n",
      "training: 0:00:37<0:00:39 Epoch 5/10 - Batch 79/160, Loss: 0.0441 0.0377\n",
      "training: 0:00:37<0:00:38 Epoch 5/10 - Batch 80/160, Loss: 0.0443 0.0449\n",
      "training: 0:00:38<0:00:38 Epoch 5/10 - Batch 81/160, Loss: 0.0444 0.0451\n",
      "training: 0:00:38<0:00:37 Epoch 5/10 - Batch 82/160, Loss: 0.0465 0.0550\n",
      "training: 0:00:38<0:00:36 Epoch 5/10 - Batch 83/160, Loss: 0.0459 0.0436\n",
      "training: 0:00:38<0:00:35 Epoch 5/10 - Batch 84/160, Loss: 0.0456 0.0441\n",
      "training: 0:00:38<0:00:35 Epoch 5/10 - Batch 85/160, Loss: 0.0460 0.0477\n",
      "training: 0:00:39<0:00:34 Epoch 5/10 - Batch 86/160, Loss: 0.0444 0.0381\n",
      "training: 0:00:39<0:00:33 Epoch 5/10 - Batch 87/160, Loss: 0.0460 0.0523\n",
      "training: 0:00:39<0:00:33 Epoch 5/10 - Batch 88/160, Loss: 0.0461 0.0464\n",
      "training: 0:00:39<0:00:32 Epoch 5/10 - Batch 89/160, Loss: 0.0440 0.0357\n",
      "training: 0:00:39<0:00:31 Epoch 5/10 - Batch 90/160, Loss: 0.0443 0.0455\n",
      "training: 0:00:40<0:00:31 Epoch 5/10 - Batch 91/160, Loss: 0.0452 0.0489\n",
      "training: 0:00:40<0:00:30 Epoch 5/10 - Batch 92/160, Loss: 0.0453 0.0457\n",
      "training: 0:00:40<0:00:29 Epoch 5/10 - Batch 93/160, Loss: 0.0454 0.0456\n",
      "training: 0:00:40<0:00:29 Epoch 5/10 - Batch 94/160, Loss: 0.0472 0.0544\n",
      "training: 0:00:40<0:00:28 Epoch 5/10 - Batch 95/160, Loss: 0.0461 0.0417\n",
      "training: 0:00:41<0:00:28 Epoch 5/10 - Batch 96/160, Loss: 0.0472 0.0515\n",
      "training: 0:00:41<0:00:27 Epoch 5/10 - Batch 97/160, Loss: 0.0453 0.0379\n",
      "training: 0:00:41<0:00:26 Epoch 5/10 - Batch 98/160, Loss: 0.0437 0.0373\n",
      "training: 0:00:41<0:00:26 Epoch 5/10 - Batch 99/160, Loss: 0.0442 0.0461\n",
      "training: 0:00:41<0:00:25 Epoch 5/10 - Batch 100/160, Loss: 0.0433 0.0399\n",
      "training: 0:00:42<0:00:25 Epoch 5/10 - Batch 101/160, Loss: 0.0439 0.0465\n",
      "training: 0:00:42<0:00:24 Epoch 5/10 - Batch 102/160, Loss: 0.0432 0.0401\n",
      "training: 0:01:10<0:00:39 Epoch 5/10 - Batch 103/160, Loss: 0.0418 0.0364\n",
      "training: 0:01:10<0:00:38 Epoch 5/10 - Batch 104/160, Loss: 0.0427 0.0460\n",
      "training: 0:01:10<0:00:37 Epoch 5/10 - Batch 105/160, Loss: 0.0427 0.0428\n",
      "training: 0:01:10<0:00:36 Epoch 5/10 - Batch 106/160, Loss: 0.0426 0.0423\n",
      "training: 0:01:10<0:00:36 Epoch 5/10 - Batch 107/160, Loss: 0.0420 0.0395\n",
      "training: 0:01:10<0:00:35 Epoch 5/10 - Batch 108/160, Loss: 0.0416 0.0401\n",
      "training: 0:01:10<0:00:34 Epoch 5/10 - Batch 109/160, Loss: 0.0413 0.0402\n",
      "training: 0:01:11<0:00:33 Epoch 5/10 - Batch 110/160, Loss: 0.0420 0.0448\n",
      "training: 0:01:11<0:00:32 Epoch 5/10 - Batch 111/160, Loss: 0.0465 0.0644\n",
      "training: 0:01:11<0:00:31 Epoch 5/10 - Batch 112/160, Loss: 0.0466 0.0468\n",
      "training: 0:01:11<0:00:30 Epoch 5/10 - Batch 113/160, Loss: 0.0459 0.0430\n",
      "training: 0:01:11<0:00:29 Epoch 5/10 - Batch 114/160, Loss: 0.0442 0.0378\n",
      "training: 0:01:11<0:00:28 Epoch 5/10 - Batch 115/160, Loss: 0.0440 0.0431\n",
      "training: 0:01:11<0:00:28 Epoch 5/10 - Batch 116/160, Loss: 0.0431 0.0396\n",
      "training: 0:01:11<0:00:27 Epoch 5/10 - Batch 117/160, Loss: 0.0421 0.0378\n",
      "training: 0:01:11<0:00:26 Epoch 5/10 - Batch 118/160, Loss: 0.0422 0.0427\n",
      "training: 0:01:11<0:00:25 Epoch 5/10 - Batch 119/160, Loss: 0.0428 0.0452\n",
      "training: 0:01:12<0:00:24 Epoch 5/10 - Batch 120/160, Loss: 0.0424 0.0410\n",
      "training: 0:01:12<0:00:24 Epoch 5/10 - Batch 121/160, Loss: 0.0429 0.0449\n",
      "training: 0:01:12<0:00:23 Epoch 5/10 - Batch 122/160, Loss: 0.0433 0.0446\n",
      "training: 0:01:12<0:00:22 Epoch 5/10 - Batch 123/160, Loss: 0.0432 0.0431\n",
      "training: 0:01:12<0:00:21 Epoch 5/10 - Batch 124/160, Loss: 0.0428 0.0413\n",
      "training: 0:01:12<0:00:21 Epoch 5/10 - Batch 125/160, Loss: 0.0440 0.0489\n",
      "training: 0:01:12<0:00:20 Epoch 5/10 - Batch 126/160, Loss: 0.0442 0.0447\n",
      "training: 0:01:12<0:00:19 Epoch 5/10 - Batch 127/160, Loss: 0.0427 0.0368\n",
      "training: 0:01:12<0:00:18 Epoch 5/10 - Batch 128/160, Loss: 0.0430 0.0443\n",
      "training: 0:01:13<0:00:18 Epoch 5/10 - Batch 129/160, Loss: 0.0421 0.0383\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "model = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\").to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    handle_epoch(batch_size, num_epochs, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67a9af82-b160-42d2-a468-448199d7573b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recreating train_snippets_shuffled\n",
      "\n",
      "Validation\n",
      "\n",
      "training: 0:00:00<0:01:16 Epoch 11/10 - Batch 1/160, Loss: 0.0323 0.0323\n",
      "saving searchnet-model-10-0.pt\n",
      "\n",
      "training: 0:00:02<0:02:59 Epoch 11/10 - Batch 2/160, Loss: 0.0397 0.0446\n",
      "training: 0:00:02<0:02:08 Epoch 11/10 - Batch 3/160, Loss: 0.0408 0.0424\n",
      "training: 0:00:02<0:01:42 Epoch 11/10 - Batch 4/160, Loss: 0.0385 0.0331\n",
      "training: 0:00:02<0:01:27 Epoch 11/10 - Batch 5/160, Loss: 0.0373 0.0337\n",
      "training: 0:00:02<0:01:16 Epoch 11/10 - Batch 6/160, Loss: 0.0376 0.0388\n",
      "training: 0:00:03<0:01:09 Epoch 11/10 - Batch 7/160, Loss: 0.0368 0.0337\n",
      "training: 0:00:03<0:01:03 Epoch 11/10 - Batch 8/160, Loss: 0.0356 0.0312\n",
      "training: 0:00:03<0:00:59 Epoch 11/10 - Batch 9/160, Loss: 0.0354 0.0347\n",
      "training: 0:00:03<0:00:55 Epoch 11/10 - Batch 10/160, Loss: 0.0353 0.0349\n",
      "training: 0:00:03<0:00:52 Epoch 11/10 - Batch 11/160, Loss: 0.0348 0.0327\n",
      "training: 0:00:04<0:00:50 Epoch 11/10 - Batch 12/160, Loss: 0.0361 0.0412\n",
      "training: 0:00:04<0:00:48 Epoch 11/10 - Batch 13/160, Loss: 0.0366 0.0386\n",
      "training: 0:00:04<0:00:46 Epoch 11/10 - Batch 14/160, Loss: 0.0366 0.0365\n",
      "training: 0:00:04<0:00:44 Epoch 11/10 - Batch 15/160, Loss: 0.0372 0.0399\n",
      "training: 0:00:04<0:00:43 Epoch 11/10 - Batch 16/160, Loss: 0.0380 0.0411\n",
      "training: 0:00:04<0:00:41 Epoch 11/10 - Batch 17/160, Loss: 0.0384 0.0399\n",
      "training: 0:00:05<0:00:40 Epoch 11/10 - Batch 18/160, Loss: 0.0379 0.0360\n",
      "training: 0:00:05<0:00:39 Epoch 11/10 - Batch 19/160, Loss: 0.0396 0.0464\n",
      "training: 0:00:05<0:00:38 Epoch 11/10 - Batch 20/160, Loss: 0.0394 0.0385\n",
      "training: 0:00:05<0:00:37 Epoch 11/10 - Batch 21/160, Loss: 0.0388 0.0366\n",
      "training: 0:00:05<0:00:36 Epoch 11/10 - Batch 22/160, Loss: 0.0389 0.0393\n",
      "training: 0:00:06<0:00:36 Epoch 11/10 - Batch 23/160, Loss: 0.0398 0.0434\n",
      "training: 0:00:06<0:00:35 Epoch 11/10 - Batch 24/160, Loss: 0.0390 0.0355\n",
      "training: 0:00:06<0:00:34 Epoch 11/10 - Batch 25/160, Loss: 0.0377 0.0325\n",
      "training: 0:00:06<0:00:34 Epoch 11/10 - Batch 26/160, Loss: 0.0393 0.0456\n",
      "training: 0:00:06<0:00:33 Epoch 11/10 - Batch 27/160, Loss: 0.0384 0.0349\n",
      "training: 0:00:06<0:00:32 Epoch 11/10 - Batch 28/160, Loss: 0.0383 0.0380\n",
      "training: 0:00:07<0:00:32 Epoch 11/10 - Batch 29/160, Loss: 0.0390 0.0415\n",
      "training: 0:00:07<0:00:32 Epoch 11/10 - Batch 30/160, Loss: 0.0367 0.0277\n",
      "training: 0:00:07<0:00:32 Epoch 11/10 - Batch 31/160, Loss: 0.0363 0.0347\n",
      "training: 0:00:07<0:00:31 Epoch 11/10 - Batch 32/160, Loss: 0.0371 0.0405\n",
      "training: 0:00:08<0:00:31 Epoch 11/10 - Batch 33/160, Loss: 0.0366 0.0346\n",
      "training: 0:00:08<0:00:30 Epoch 11/10 - Batch 34/160, Loss: 0.0362 0.0344\n",
      "training: 0:00:08<0:00:30 Epoch 11/10 - Batch 35/160, Loss: 0.0366 0.0383\n",
      "training: 0:00:08<0:00:29 Epoch 11/10 - Batch 36/160, Loss: 0.0376 0.0414\n",
      "training: 0:00:08<0:00:29 Epoch 11/10 - Batch 37/160, Loss: 0.0371 0.0355\n",
      "training: 0:00:09<0:00:29 Epoch 11/10 - Batch 38/160, Loss: 0.0359 0.0311\n",
      "training: 0:00:09<0:00:28 Epoch 11/10 - Batch 39/160, Loss: 0.0371 0.0417\n",
      "training: 0:00:09<0:00:28 Epoch 11/10 - Batch 40/160, Loss: 0.0365 0.0342\n",
      "training: 0:00:09<0:00:27 Epoch 11/10 - Batch 41/160, Loss: 0.0366 0.0369\n",
      "training: 0:00:09<0:00:27 Epoch 11/10 - Batch 42/160, Loss: 0.0349 0.0282\n",
      "training: 0:00:09<0:00:27 Epoch 11/10 - Batch 43/160, Loss: 0.0347 0.0338\n",
      "training: 0:00:10<0:00:26 Epoch 11/10 - Batch 44/160, Loss: 0.0352 0.0373\n",
      "training: 0:00:10<0:00:26 Epoch 11/10 - Batch 45/160, Loss: 0.0345 0.0315\n",
      "training: 0:00:10<0:00:25 Epoch 11/10 - Batch 46/160, Loss: 0.0362 0.0430\n",
      "training: 0:00:10<0:00:25 Epoch 11/10 - Batch 47/160, Loss: 0.0363 0.0368\n",
      "training: 0:00:10<0:00:25 Epoch 11/10 - Batch 48/160, Loss: 0.0379 0.0444\n",
      "training: 0:00:11<0:00:25 Epoch 11/10 - Batch 49/160, Loss: 0.0405 0.0508\n",
      "training: 0:00:11<0:00:24 Epoch 11/10 - Batch 50/160, Loss: 0.0412 0.0442\n",
      "training: 0:00:11<0:00:24 Epoch 11/10 - Batch 51/160, Loss: 0.0401 0.0354\n",
      "training: 0:00:11<0:00:24 Epoch 11/10 - Batch 52/160, Loss: 0.0384 0.0320\n",
      "training: 0:00:11<0:00:23 Epoch 11/10 - Batch 53/160, Loss: 0.0378 0.0354\n",
      "training: 0:00:11<0:00:23 Epoch 11/10 - Batch 54/160, Loss: 0.0387 0.0421\n",
      "training: 0:00:12<0:00:23 Epoch 11/10 - Batch 55/160, Loss: 0.0367 0.0287\n",
      "training: 0:00:12<0:00:22 Epoch 11/10 - Batch 56/160, Loss: 0.0341 0.0239\n",
      "training: 0:00:12<0:00:22 Epoch 11/10 - Batch 57/160, Loss: 0.0354 0.0406\n",
      "training: 0:00:12<0:00:22 Epoch 11/10 - Batch 58/160, Loss: 0.0350 0.0331\n",
      "training: 0:00:12<0:00:22 Epoch 11/10 - Batch 59/160, Loss: 0.0338 0.0292\n",
      "training: 0:00:13<0:00:21 Epoch 11/10 - Batch 60/160, Loss: 0.0338 0.0338\n",
      "training: 0:00:13<0:00:21 Epoch 11/10 - Batch 61/160, Loss: 0.0332 0.0306\n",
      "training: 0:00:13<0:00:21 Epoch 11/10 - Batch 62/160, Loss: 0.0337 0.0359\n",
      "training: 0:00:13<0:00:20 Epoch 11/10 - Batch 63/160, Loss: 0.0324 0.0270\n",
      "training: 0:00:13<0:00:20 Epoch 11/10 - Batch 64/160, Loss: 0.0335 0.0381\n",
      "training: 0:00:13<0:00:20 Epoch 11/10 - Batch 65/160, Loss: 0.0326 0.0288\n",
      "training: 0:00:14<0:00:20 Epoch 11/10 - Batch 66/160, Loss: 0.0338 0.0388\n",
      "training: 0:00:14<0:00:19 Epoch 11/10 - Batch 67/160, Loss: 0.0345 0.0371\n",
      "training: 0:00:14<0:00:19 Epoch 11/10 - Batch 68/160, Loss: 0.0352 0.0379\n",
      "training: 0:00:14<0:00:19 Epoch 11/10 - Batch 69/160, Loss: 0.0358 0.0383\n",
      "training: 0:00:14<0:00:19 Epoch 11/10 - Batch 70/160, Loss: 0.0360 0.0368\n",
      "training: 0:00:15<0:00:18 Epoch 11/10 - Batch 71/160, Loss: 0.0357 0.0347\n",
      "training: 0:00:15<0:00:18 Epoch 11/10 - Batch 72/160, Loss: 0.0352 0.0332\n",
      "training: 0:00:15<0:00:18 Epoch 11/10 - Batch 73/160, Loss: 0.0348 0.0334\n",
      "training: 0:00:15<0:00:18 Epoch 11/10 - Batch 74/160, Loss: 0.0347 0.0339\n",
      "training: 0:00:15<0:00:17 Epoch 11/10 - Batch 75/160, Loss: 0.0344 0.0333\n",
      "training: 0:00:15<0:00:17 Epoch 11/10 - Batch 76/160, Loss: 0.0346 0.0355\n",
      "training: 0:00:16<0:00:17 Epoch 11/10 - Batch 77/160, Loss: 0.0360 0.0414\n",
      "training: 0:00:16<0:00:17 Epoch 11/10 - Batch 78/160, Loss: 0.0363 0.0377\n",
      "training: 0:00:16<0:00:16 Epoch 11/10 - Batch 79/160, Loss: 0.0364 0.0364\n",
      "training: 0:00:16<0:00:16 Epoch 11/10 - Batch 80/160, Loss: 0.0343 0.0260\n",
      "training: 0:00:16<0:00:16 Epoch 11/10 - Batch 81/160, Loss: 0.0350 0.0377\n",
      "training: 0:00:17<0:00:16 Epoch 11/10 - Batch 82/160, Loss: 0.0359 0.0399\n",
      "training: 0:00:17<0:00:16 Epoch 11/10 - Batch 83/160, Loss: 0.0352 0.0322\n",
      "training: 0:00:17<0:00:15 Epoch 11/10 - Batch 84/160, Loss: 0.0365 0.0415\n",
      "training: 0:00:17<0:00:15 Epoch 11/10 - Batch 85/160, Loss: 0.0355 0.0318\n",
      "training: 0:00:17<0:00:15 Epoch 11/10 - Batch 86/160, Loss: 0.0339 0.0274\n",
      "training: 0:00:17<0:00:15 Epoch 11/10 - Batch 87/160, Loss: 0.0345 0.0368\n",
      "training: 0:00:18<0:00:14 Epoch 11/10 - Batch 88/160, Loss: 0.0345 0.0347\n",
      "training: 0:00:18<0:00:14 Epoch 11/10 - Batch 89/160, Loss: 0.0348 0.0356\n",
      "training: 0:00:18<0:00:14 Epoch 11/10 - Batch 90/160, Loss: 0.0363 0.0422\n",
      "training: 0:00:18<0:00:14 Epoch 11/10 - Batch 91/160, Loss: 0.0366 0.0378\n",
      "training: 0:00:18<0:00:13 Epoch 11/10 - Batch 92/160, Loss: 0.0379 0.0433\n",
      "training: 0:00:19<0:00:13 Epoch 11/10 - Batch 93/160, Loss: 0.0370 0.0334\n",
      "training: 0:00:19<0:00:13 Epoch 11/10 - Batch 94/160, Loss: 0.0361 0.0323\n",
      "training: 0:00:19<0:00:13 Epoch 11/10 - Batch 95/160, Loss: 0.0370 0.0407\n",
      "training: 0:00:19<0:00:13 Epoch 11/10 - Batch 96/160, Loss: 0.0371 0.0376\n",
      "training: 0:00:19<0:00:12 Epoch 11/10 - Batch 97/160, Loss: 0.0381 0.0418\n",
      "training: 0:00:20<0:00:12 Epoch 11/10 - Batch 98/160, Loss: 0.0405 0.0502\n",
      "training: 0:00:20<0:00:12 Epoch 11/10 - Batch 99/160, Loss: 0.0410 0.0431\n",
      "training: 0:00:20<0:00:12 Epoch 11/10 - Batch 100/160, Loss: 0.0404 0.0379\n",
      "training: 0:00:20<0:00:12 Epoch 11/10 - Batch 101/160, Loss: 0.0385 0.0310\n",
      "saving searchnet-model-10-100.pt\n",
      "\n",
      "training: 0:00:23<0:00:13 Epoch 11/10 - Batch 102/160, Loss: 0.0376 0.0341\n",
      "training: 0:00:23<0:00:13 Epoch 11/10 - Batch 103/160, Loss: 0.0374 0.0367\n",
      "training: 0:00:23<0:00:12 Epoch 11/10 - Batch 104/160, Loss: 0.0395 0.0477\n",
      "training: 0:00:23<0:00:12 Epoch 11/10 - Batch 105/160, Loss: 0.0394 0.0392\n",
      "training: 0:00:24<0:00:12 Epoch 11/10 - Batch 106/160, Loss: 0.0398 0.0411\n",
      "training: 0:00:24<0:00:12 Epoch 11/10 - Batch 107/160, Loss: 0.0388 0.0350\n",
      "training: 0:00:24<0:00:11 Epoch 11/10 - Batch 108/160, Loss: 0.0378 0.0340\n",
      "training: 0:00:24<0:00:11 Epoch 11/10 - Batch 109/160, Loss: 0.0365 0.0309\n",
      "training: 0:00:24<0:00:11 Epoch 11/10 - Batch 110/160, Loss: 0.0401 0.0547\n",
      "training: 0:00:25<0:00:11 Epoch 11/10 - Batch 111/160, Loss: 0.0424 0.0517\n",
      "training: 0:00:25<0:00:10 Epoch 11/10 - Batch 112/160, Loss: 0.0400 0.0302\n",
      "training: 0:00:25<0:00:10 Epoch 11/10 - Batch 113/160, Loss: 0.0392 0.0363\n",
      "training: 0:00:25<0:00:10 Epoch 11/10 - Batch 114/160, Loss: 0.0386 0.0363\n",
      "training: 0:00:25<0:00:10 Epoch 11/10 - Batch 115/160, Loss: 0.0380 0.0353\n",
      "training: 0:00:26<0:00:09 Epoch 11/10 - Batch 116/160, Loss: 0.0391 0.0437\n",
      "training: 0:00:26<0:00:09 Epoch 11/10 - Batch 117/160, Loss: 0.0392 0.0396\n",
      "training: 0:00:26<0:00:09 Epoch 11/10 - Batch 118/160, Loss: 0.0385 0.0357\n",
      "training: 0:00:26<0:00:09 Epoch 11/10 - Batch 119/160, Loss: 0.0377 0.0343\n",
      "training: 0:00:26<0:00:08 Epoch 11/10 - Batch 120/160, Loss: 0.0381 0.0400\n",
      "training: 0:00:26<0:00:08 Epoch 11/10 - Batch 121/160, Loss: 0.0376 0.0354\n",
      "training: 0:00:27<0:00:08 Epoch 11/10 - Batch 122/160, Loss: 0.0386 0.0426\n",
      "training: 0:00:27<0:00:08 Epoch 11/10 - Batch 123/160, Loss: 0.0365 0.0283\n",
      "training: 0:00:27<0:00:07 Epoch 11/10 - Batch 124/160, Loss: 0.0376 0.0417\n",
      "training: 0:00:27<0:00:07 Epoch 11/10 - Batch 125/160, Loss: 0.0375 0.0374\n",
      "training: 0:00:27<0:00:07 Epoch 11/10 - Batch 126/160, Loss: 0.0376 0.0377\n",
      "training: 0:00:28<0:00:07 Epoch 11/10 - Batch 127/160, Loss: 0.0375 0.0373\n",
      "training: 0:00:28<0:00:07 Epoch 11/10 - Batch 128/160, Loss: 0.0377 0.0384\n",
      "saving searchnet-model-10-128.pt\n",
      "\n",
      "\n",
      "Validation:\n",
      "\n",
      "validation: 0:00:30<0:00:07 Epoch 11/10 - Batch 129/160, Loss: 0.0083 0.0083\n",
      "validation: 0:00:31<0:00:07 Epoch 11/10 - Batch 130/160, Loss: 0.0094 0.0104\n",
      "validation: 0:00:31<0:00:06 Epoch 11/10 - Batch 131/160, Loss: 0.0087 0.0074\n",
      "validation: 0:00:31<0:00:06 Epoch 11/10 - Batch 132/160, Loss: 0.0094 0.0113\n",
      "validation: 0:00:31<0:00:06 Epoch 11/10 - Batch 133/160, Loss: 0.0096 0.0107\n",
      "validation: 0:00:31<0:00:06 Epoch 11/10 - Batch 134/160, Loss: 0.0092 0.0070\n",
      "validation: 0:00:31<0:00:05 Epoch 11/10 - Batch 135/160, Loss: 0.0094 0.0106\n",
      "validation: 0:00:31<0:00:05 Epoch 11/10 - Batch 136/160, Loss: 0.0094 0.0091\n",
      "validation: 0:00:31<0:00:05 Epoch 11/10 - Batch 137/160, Loss: 0.0102 0.0166\n",
      "validation: 0:00:31<0:00:05 Epoch 11/10 - Batch 138/160, Loss: 0.0101 0.0096\n",
      "validation: 0:00:31<0:00:04 Epoch 11/10 - Batch 139/160, Loss: 0.0098 0.0065\n",
      "validation: 0:00:31<0:00:04 Epoch 11/10 - Batch 140/160, Loss: 0.0096 0.0074\n",
      "validation: 0:00:31<0:00:04 Epoch 11/10 - Batch 141/160, Loss: 0.0094 0.0071\n",
      "validation: 0:00:31<0:00:04 Epoch 11/10 - Batch 142/160, Loss: 0.0093 0.0086\n",
      "validation: 0:00:31<0:00:03 Epoch 11/10 - Batch 143/160, Loss: 0.0092 0.0070\n",
      "validation: 0:00:31<0:00:03 Epoch 11/10 - Batch 144/160, Loss: 0.0091 0.0078\n",
      "validation: 0:00:31<0:00:03 Epoch 11/10 - Batch 145/160, Loss: 0.0090 0.0072\n",
      "validation: 0:00:31<0:00:03 Epoch 11/10 - Batch 146/160, Loss: 0.0090 0.0095\n",
      "validation: 0:00:32<0:00:02 Epoch 11/10 - Batch 147/160, Loss: 0.0094 0.0160\n",
      "validation: 0:00:32<0:00:02 Epoch 11/10 - Batch 148/160, Loss: 0.0094 0.0095\n",
      "validation: 0:00:32<0:00:02 Epoch 11/10 - Batch 149/160, Loss: 0.0094 0.0089\n",
      "validation: 0:00:32<0:00:02 Epoch 11/10 - Batch 150/160, Loss: 0.0093 0.0074\n",
      "validation: 0:00:32<0:00:01 Epoch 11/10 - Batch 151/160, Loss: 0.0092 0.0068\n",
      "validation: 0:00:32<0:00:01 Epoch 11/10 - Batch 152/160, Loss: 0.0094 0.0143\n",
      "validation: 0:00:32<0:00:01 Epoch 11/10 - Batch 153/160, Loss: 0.0093 0.0068\n",
      "validation: 0:00:32<0:00:01 Epoch 11/10 - Batch 154/160, Loss: 0.0092 0.0086\n",
      "validation: 0:00:32<0:00:01 Epoch 11/10 - Batch 155/160, Loss: 0.0092 0.0080\n",
      "validation: 0:00:32<0:00:00 Epoch 11/10 - Batch 156/160, Loss: 0.0092 0.0102\n",
      "validation: 0:00:32<0:00:00 Epoch 11/10 - Batch 157/160, Loss: 0.0093 0.0116\n",
      "validation: 0:00:32<0:00:00 Epoch 11/10 - Batch 158/160, Loss: 0.0092 0.0069\n",
      "validation: 0:00:32<0:00:00 Epoch 11/10 - Batch 159/160, Loss: 0.0092 0.0069\n",
      "validation: 0:00:32<0:00:00 Epoch 11/10 - Batch 160/160, Loss: 0.0093 0.0140\n",
      "recreating train_snippets_shuffled\n",
      "\n",
      "Validation\n",
      "\n",
      "training: 0:00:00<0:00:29 Epoch 12/10 - Batch 1/160, Loss: 0.0335 0.0335\n",
      "saving searchnet-model-11-0.pt\n",
      "\n",
      "training: 0:00:03<0:04:31 Epoch 12/10 - Batch 2/160, Loss: 0.0364 0.0384\n",
      "training: 0:00:03<0:03:09 Epoch 12/10 - Batch 3/160, Loss: 0.0359 0.0352\n",
      "training: 0:00:03<0:02:28 Epoch 12/10 - Batch 4/160, Loss: 0.0356 0.0348\n",
      "training: 0:00:03<0:02:03 Epoch 12/10 - Batch 5/160, Loss: 0.0355 0.0350\n",
      "training: 0:00:04<0:01:46 Epoch 12/10 - Batch 6/160, Loss: 0.0332 0.0256\n",
      "training: 0:00:04<0:01:35 Epoch 12/10 - Batch 7/160, Loss: 0.0350 0.0417\n",
      "training: 0:00:04<0:01:26 Epoch 12/10 - Batch 8/160, Loss: 0.0353 0.0364\n",
      "training: 0:00:04<0:01:19 Epoch 12/10 - Batch 9/160, Loss: 0.0365 0.0411\n",
      "training: 0:00:04<0:01:13 Epoch 12/10 - Batch 10/160, Loss: 0.0364 0.0359\n",
      "training: 0:00:05<0:01:08 Epoch 12/10 - Batch 11/160, Loss: 0.0376 0.0424\n",
      "training: 0:00:05<0:01:04 Epoch 12/10 - Batch 12/160, Loss: 0.0395 0.0470\n",
      "training: 0:00:05<0:01:01 Epoch 12/10 - Batch 13/160, Loss: 0.0385 0.0347\n",
      "training: 0:00:05<0:00:58 Epoch 12/10 - Batch 14/160, Loss: 0.0369 0.0306\n",
      "training: 0:00:05<0:00:56 Epoch 12/10 - Batch 15/160, Loss: 0.0362 0.0331\n",
      "training: 0:00:05<0:00:53 Epoch 12/10 - Batch 16/160, Loss: 0.0367 0.0387\n",
      "training: 0:00:06<0:00:51 Epoch 12/10 - Batch 17/160, Loss: 0.0347 0.0269\n",
      "training: 0:00:06<0:00:50 Epoch 12/10 - Batch 18/160, Loss: 0.0345 0.0335\n",
      "training: 0:00:06<0:00:48 Epoch 12/10 - Batch 19/160, Loss: 0.0358 0.0408\n",
      "training: 0:00:06<0:00:47 Epoch 12/10 - Batch 20/160, Loss: 0.0371 0.0423\n",
      "training: 0:00:06<0:00:45 Epoch 12/10 - Batch 21/160, Loss: 0.0365 0.0341\n",
      "training: 0:00:07<0:00:44 Epoch 12/10 - Batch 22/160, Loss: 0.0363 0.0357\n",
      "training: 0:00:07<0:00:43 Epoch 12/10 - Batch 23/160, Loss: 0.0369 0.0391\n",
      "training: 0:00:07<0:00:42 Epoch 12/10 - Batch 24/160, Loss: 0.0365 0.0351\n",
      "training: 0:00:07<0:00:41 Epoch 12/10 - Batch 25/160, Loss: 0.0368 0.0380\n",
      "training: 0:00:07<0:00:40 Epoch 12/10 - Batch 26/160, Loss: 0.0365 0.0350\n",
      "training: 0:00:08<0:00:39 Epoch 12/10 - Batch 27/160, Loss: 0.0352 0.0301\n",
      "training: 0:00:08<0:00:39 Epoch 12/10 - Batch 28/160, Loss: 0.0360 0.0394\n",
      "training: 0:00:08<0:00:39 Epoch 12/10 - Batch 29/160, Loss: 0.0378 0.0449\n",
      "training: 0:00:08<0:00:38 Epoch 12/10 - Batch 30/160, Loss: 0.0367 0.0325\n",
      "training: 0:00:09<0:00:37 Epoch 12/10 - Batch 31/160, Loss: 0.0377 0.0418\n",
      "training: 0:00:09<0:00:36 Epoch 12/10 - Batch 32/160, Loss: 0.0378 0.0382\n",
      "training: 0:00:09<0:00:36 Epoch 12/10 - Batch 33/160, Loss: 0.0370 0.0336\n",
      "training: 0:00:09<0:00:35 Epoch 12/10 - Batch 34/160, Loss: 0.0349 0.0266\n",
      "training: 0:00:09<0:00:34 Epoch 12/10 - Batch 35/160, Loss: 0.0348 0.0346\n",
      "training: 0:00:09<0:00:34 Epoch 12/10 - Batch 36/160, Loss: 0.0343 0.0320\n",
      "training: 0:00:10<0:00:33 Epoch 12/10 - Batch 37/160, Loss: 0.0349 0.0372\n",
      "training: 0:00:10<0:00:33 Epoch 12/10 - Batch 38/160, Loss: 0.0360 0.0403\n",
      "training: 0:00:10<0:00:32 Epoch 12/10 - Batch 39/160, Loss: 0.0371 0.0416\n",
      "training: 0:00:10<0:00:32 Epoch 12/10 - Batch 40/160, Loss: 0.0382 0.0426\n",
      "training: 0:00:10<0:00:31 Epoch 12/10 - Batch 41/160, Loss: 0.0369 0.0317\n",
      "training: 0:00:11<0:00:31 Epoch 12/10 - Batch 42/160, Loss: 0.0366 0.0354\n",
      "training: 0:00:11<0:00:30 Epoch 12/10 - Batch 43/160, Loss: 0.0373 0.0402\n",
      "training: 0:00:11<0:00:30 Epoch 12/10 - Batch 44/160, Loss: 0.0384 0.0429\n",
      "training: 0:00:11<0:00:29 Epoch 12/10 - Batch 45/160, Loss: 0.0381 0.0365\n",
      "training: 0:00:11<0:00:29 Epoch 12/10 - Batch 46/160, Loss: 0.0375 0.0351\n",
      "training: 0:00:11<0:00:28 Epoch 12/10 - Batch 47/160, Loss: 0.0374 0.0371\n",
      "training: 0:00:12<0:00:28 Epoch 12/10 - Batch 48/160, Loss: 0.0356 0.0286\n",
      "training: 0:00:12<0:00:27 Epoch 12/10 - Batch 49/160, Loss: 0.0344 0.0296\n",
      "training: 0:00:12<0:00:27 Epoch 12/10 - Batch 50/160, Loss: 0.0349 0.0366\n",
      "training: 0:00:12<0:00:27 Epoch 12/10 - Batch 51/160, Loss: 0.0354 0.0377\n",
      "training: 0:00:12<0:00:26 Epoch 12/10 - Batch 52/160, Loss: 0.0374 0.0454\n",
      "training: 0:00:13<0:00:26 Epoch 12/10 - Batch 53/160, Loss: 0.0379 0.0399\n",
      "training: 0:00:13<0:00:25 Epoch 12/10 - Batch 54/160, Loss: 0.0388 0.0424\n",
      "training: 0:00:13<0:00:25 Epoch 12/10 - Batch 55/160, Loss: 0.0386 0.0376\n",
      "training: 0:00:13<0:00:25 Epoch 12/10 - Batch 56/160, Loss: 0.0378 0.0347\n",
      "training: 0:00:13<0:00:24 Epoch 12/10 - Batch 57/160, Loss: 0.0359 0.0283\n",
      "training: 0:00:13<0:00:24 Epoch 12/10 - Batch 58/160, Loss: 0.0364 0.0384\n",
      "training: 0:00:14<0:00:24 Epoch 12/10 - Batch 59/160, Loss: 0.0367 0.0378\n",
      "training: 0:00:14<0:00:23 Epoch 12/10 - Batch 60/160, Loss: 0.0363 0.0347\n",
      "training: 0:00:14<0:00:23 Epoch 12/10 - Batch 61/160, Loss: 0.0350 0.0296\n",
      "training: 0:00:14<0:00:23 Epoch 12/10 - Batch 62/160, Loss: 0.0360 0.0399\n",
      "training: 0:00:14<0:00:22 Epoch 12/10 - Batch 63/160, Loss: 0.0354 0.0333\n",
      "training: 0:00:15<0:00:22 Epoch 12/10 - Batch 64/160, Loss: 0.0349 0.0330\n",
      "training: 0:00:15<0:00:22 Epoch 12/10 - Batch 65/160, Loss: 0.0367 0.0439\n",
      "training: 0:00:15<0:00:21 Epoch 12/10 - Batch 66/160, Loss: 0.0365 0.0357\n",
      "training: 0:00:15<0:00:21 Epoch 12/10 - Batch 67/160, Loss: 0.0365 0.0365\n",
      "training: 0:00:15<0:00:21 Epoch 12/10 - Batch 68/160, Loss: 0.0365 0.0365\n",
      "training: 0:00:15<0:00:21 Epoch 12/10 - Batch 69/160, Loss: 0.0361 0.0343\n",
      "training: 0:00:16<0:00:20 Epoch 12/10 - Batch 70/160, Loss: 0.0351 0.0312\n",
      "training: 0:00:16<0:00:20 Epoch 12/10 - Batch 71/160, Loss: 0.0350 0.0344\n",
      "training: 0:00:16<0:00:20 Epoch 12/10 - Batch 72/160, Loss: 0.0350 0.0353\n",
      "training: 0:00:16<0:00:19 Epoch 12/10 - Batch 73/160, Loss: 0.0346 0.0330\n",
      "training: 0:00:16<0:00:19 Epoch 12/10 - Batch 74/160, Loss: 0.0357 0.0398\n",
      "training: 0:00:17<0:00:19 Epoch 12/10 - Batch 75/160, Loss: 0.0365 0.0399\n",
      "training: 0:00:17<0:00:19 Epoch 12/10 - Batch 76/160, Loss: 0.0363 0.0352\n",
      "training: 0:00:17<0:00:18 Epoch 12/10 - Batch 77/160, Loss: 0.0364 0.0369\n",
      "training: 0:00:17<0:00:18 Epoch 12/10 - Batch 78/160, Loss: 0.0361 0.0351\n",
      "training: 0:00:17<0:00:18 Epoch 12/10 - Batch 79/160, Loss: 0.0363 0.0367\n",
      "training: 0:00:17<0:00:17 Epoch 12/10 - Batch 80/160, Loss: 0.0357 0.0337\n",
      "training: 0:00:18<0:00:17 Epoch 12/10 - Batch 81/160, Loss: 0.0356 0.0352\n",
      "training: 0:00:18<0:00:17 Epoch 12/10 - Batch 82/160, Loss: 0.0349 0.0320\n",
      "training: 0:00:18<0:00:17 Epoch 12/10 - Batch 83/160, Loss: 0.0361 0.0409\n",
      "training: 0:00:18<0:00:16 Epoch 12/10 - Batch 84/160, Loss: 0.0374 0.0425\n",
      "training: 0:00:18<0:00:16 Epoch 12/10 - Batch 85/160, Loss: 0.0382 0.0417\n",
      "training: 0:00:19<0:00:16 Epoch 12/10 - Batch 86/160, Loss: 0.0375 0.0346\n",
      "training: 0:00:19<0:00:16 Epoch 12/10 - Batch 87/160, Loss: 0.0373 0.0365\n",
      "training: 0:00:19<0:00:15 Epoch 12/10 - Batch 88/160, Loss: 0.0359 0.0304\n",
      "training: 0:00:19<0:00:15 Epoch 12/10 - Batch 89/160, Loss: 0.0349 0.0310\n",
      "training: 0:00:19<0:00:15 Epoch 12/10 - Batch 90/160, Loss: 0.0352 0.0361\n",
      "training: 0:00:19<0:00:15 Epoch 12/10 - Batch 91/160, Loss: 0.0342 0.0303\n",
      "training: 0:00:20<0:00:14 Epoch 12/10 - Batch 92/160, Loss: 0.0353 0.0399\n",
      "training: 0:00:20<0:00:14 Epoch 12/10 - Batch 93/160, Loss: 0.0348 0.0328\n",
      "training: 0:00:20<0:00:14 Epoch 12/10 - Batch 94/160, Loss: 0.0337 0.0293\n",
      "training: 0:00:20<0:00:14 Epoch 12/10 - Batch 95/160, Loss: 0.0337 0.0337\n",
      "training: 0:00:20<0:00:13 Epoch 12/10 - Batch 96/160, Loss: 0.0347 0.0386\n",
      "training: 0:00:21<0:00:13 Epoch 12/10 - Batch 97/160, Loss: 0.0355 0.0389\n",
      "training: 0:00:21<0:00:13 Epoch 12/10 - Batch 98/160, Loss: 0.0339 0.0274\n",
      "training: 0:00:21<0:00:13 Epoch 12/10 - Batch 99/160, Loss: 0.0349 0.0387\n",
      "training: 0:00:21<0:00:12 Epoch 12/10 - Batch 100/160, Loss: 0.0347 0.0338\n",
      "training: 0:00:21<0:00:12 Epoch 12/10 - Batch 101/160, Loss: 0.0346 0.0345\n",
      "saving searchnet-model-11-100.pt\n",
      "\n",
      "training: 0:00:24<0:00:13 Epoch 12/10 - Batch 102/160, Loss: 0.0337 0.0298\n",
      "training: 0:00:24<0:00:13 Epoch 12/10 - Batch 103/160, Loss: 0.0340 0.0352\n",
      "training: 0:00:24<0:00:13 Epoch 12/10 - Batch 104/160, Loss: 0.0345 0.0365\n",
      "training: 0:00:24<0:00:12 Epoch 12/10 - Batch 105/160, Loss: 0.0366 0.0452\n",
      "training: 0:00:24<0:00:12 Epoch 12/10 - Batch 106/160, Loss: 0.0350 0.0283\n",
      "training: 0:00:25<0:00:12 Epoch 12/10 - Batch 107/160, Loss: 0.0343 0.0315\n",
      "training: 0:00:25<0:00:12 Epoch 12/10 - Batch 108/160, Loss: 0.0344 0.0352\n",
      "training: 0:00:25<0:00:11 Epoch 12/10 - Batch 109/160, Loss: 0.0344 0.0342\n",
      "training: 0:00:25<0:00:11 Epoch 12/10 - Batch 110/160, Loss: 0.0349 0.0369\n",
      "training: 0:00:25<0:00:11 Epoch 12/10 - Batch 111/160, Loss: 0.0355 0.0380\n",
      "training: 0:00:25<0:00:11 Epoch 12/10 - Batch 112/160, Loss: 0.0357 0.0364\n",
      "training: 0:00:26<0:00:10 Epoch 12/10 - Batch 113/160, Loss: 0.0361 0.0379\n",
      "training: 0:00:26<0:00:10 Epoch 12/10 - Batch 114/160, Loss: 0.0375 0.0431\n",
      "training: 0:00:26<0:00:10 Epoch 12/10 - Batch 115/160, Loss: 0.0376 0.0380\n",
      "training: 0:00:26<0:00:10 Epoch 12/10 - Batch 116/160, Loss: 0.0365 0.0321\n",
      "training: 0:00:26<0:00:09 Epoch 12/10 - Batch 117/160, Loss: 0.0356 0.0320\n",
      "training: 0:00:27<0:00:09 Epoch 12/10 - Batch 118/160, Loss: 0.0366 0.0406\n",
      "training: 0:00:27<0:00:09 Epoch 12/10 - Batch 119/160, Loss: 0.0369 0.0382\n",
      "training: 0:00:27<0:00:09 Epoch 12/10 - Batch 120/160, Loss: 0.0371 0.0376\n",
      "training: 0:00:27<0:00:08 Epoch 12/10 - Batch 121/160, Loss: 0.0365 0.0341\n",
      "training: 0:00:27<0:00:08 Epoch 12/10 - Batch 122/160, Loss: 0.0367 0.0376\n",
      "training: 0:00:27<0:00:08 Epoch 12/10 - Batch 123/160, Loss: 0.0369 0.0379\n",
      "training: 0:00:28<0:00:08 Epoch 12/10 - Batch 124/160, Loss: 0.0375 0.0398\n",
      "training: 0:00:28<0:00:07 Epoch 12/10 - Batch 125/160, Loss: 0.0382 0.0411\n",
      "training: 0:00:28<0:00:07 Epoch 12/10 - Batch 126/160, Loss: 0.0384 0.0391\n",
      "training: 0:00:28<0:00:07 Epoch 12/10 - Batch 127/160, Loss: 0.0380 0.0363\n",
      "training: 0:00:29<0:00:07 Epoch 12/10 - Batch 128/160, Loss: 0.0381 0.0388\n",
      "saving searchnet-model-11-128.pt\n",
      "\n",
      "\n",
      "Validation:\n",
      "\n",
      "validation: 0:00:30<0:00:07 Epoch 12/10 - Batch 129/160, Loss: 0.0078 0.0078\n",
      "validation: 0:00:30<0:00:07 Epoch 12/10 - Batch 130/160, Loss: 0.0075 0.0072\n",
      "validation: 0:00:31<0:00:06 Epoch 12/10 - Batch 131/160, Loss: 0.0111 0.0184\n",
      "validation: 0:00:31<0:00:06 Epoch 12/10 - Batch 132/160, Loss: 0.0105 0.0085\n",
      "validation: 0:00:31<0:00:06 Epoch 12/10 - Batch 133/160, Loss: 0.0108 0.0120\n",
      "validation: 0:00:31<0:00:06 Epoch 12/10 - Batch 134/160, Loss: 0.0106 0.0097\n",
      "validation: 0:00:31<0:00:05 Epoch 12/10 - Batch 135/160, Loss: 0.0104 0.0090\n",
      "validation: 0:00:31<0:00:05 Epoch 12/10 - Batch 136/160, Loss: 0.0100 0.0070\n",
      "validation: 0:00:31<0:00:05 Epoch 12/10 - Batch 137/160, Loss: 0.0097 0.0074\n",
      "validation: 0:00:31<0:00:05 Epoch 12/10 - Batch 138/160, Loss: 0.0096 0.0093\n",
      "validation: 0:00:31<0:00:04 Epoch 12/10 - Batch 139/160, Loss: 0.0096 0.0093\n",
      "validation: 0:00:31<0:00:04 Epoch 12/10 - Batch 140/160, Loss: 0.0099 0.0134\n",
      "validation: 0:00:31<0:00:04 Epoch 12/10 - Batch 141/160, Loss: 0.0101 0.0128\n",
      "validation: 0:00:31<0:00:04 Epoch 12/10 - Batch 142/160, Loss: 0.0101 0.0096\n",
      "validation: 0:00:31<0:00:03 Epoch 12/10 - Batch 143/160, Loss: 0.0100 0.0093\n",
      "validation: 0:00:31<0:00:03 Epoch 12/10 - Batch 144/160, Loss: 0.0098 0.0057\n",
      "validation: 0:00:31<0:00:03 Epoch 12/10 - Batch 145/160, Loss: 0.0096 0.0070\n",
      "validation: 0:00:31<0:00:03 Epoch 12/10 - Batch 146/160, Loss: 0.0098 0.0125\n",
      "validation: 0:00:31<0:00:02 Epoch 12/10 - Batch 147/160, Loss: 0.0096 0.0064\n",
      "validation: 0:00:32<0:00:02 Epoch 12/10 - Batch 148/160, Loss: 0.0100 0.0174\n",
      "validation: 0:00:32<0:00:02 Epoch 12/10 - Batch 149/160, Loss: 0.0101 0.0131\n",
      "validation: 0:00:32<0:00:02 Epoch 12/10 - Batch 150/160, Loss: 0.0101 0.0096\n",
      "validation: 0:00:32<0:00:01 Epoch 12/10 - Batch 151/160, Loss: 0.0102 0.0109\n",
      "validation: 0:00:32<0:00:01 Epoch 12/10 - Batch 152/160, Loss: 0.0101 0.0093\n",
      "validation: 0:00:32<0:00:01 Epoch 12/10 - Batch 153/160, Loss: 0.0101 0.0108\n",
      "validation: 0:00:32<0:00:01 Epoch 12/10 - Batch 154/160, Loss: 0.0100 0.0059\n",
      "validation: 0:00:32<0:00:01 Epoch 12/10 - Batch 155/160, Loss: 0.0099 0.0069\n",
      "validation: 0:00:32<0:00:00 Epoch 12/10 - Batch 156/160, Loss: 0.0099 0.0099\n",
      "validation: 0:00:32<0:00:00 Epoch 12/10 - Batch 157/160, Loss: 0.0103 0.0220\n",
      "validation: 0:00:32<0:00:00 Epoch 12/10 - Batch 158/160, Loss: 0.0103 0.0103\n",
      "validation: 0:00:32<0:00:00 Epoch 12/10 - Batch 159/160, Loss: 0.0103 0.0103\n",
      "validation: 0:00:32<0:00:00 Epoch 12/10 - Batch 160/160, Loss: 0.0103 0.0114\n",
      "recreating train_snippets_shuffled\n",
      "\n",
      "Validation\n",
      "\n",
      "training: 0:00:00<0:00:29 Epoch 13/10 - Batch 1/160, Loss: 0.0340 0.0340\n",
      "saving searchnet-model-12-0.pt\n",
      "\n",
      "training: 0:00:03<0:04:34 Epoch 13/10 - Batch 2/160, Loss: 0.0307 0.0285\n",
      "training: 0:00:05<0:05:10 Epoch 13/10 - Batch 3/160, Loss: 0.0346 0.0403\n",
      "training: 0:00:06<0:03:58 Epoch 13/10 - Batch 4/160, Loss: 0.0351 0.0363\n",
      "training: 0:00:06<0:03:23 Epoch 13/10 - Batch 5/160, Loss: 0.0348 0.0341\n",
      "training: 0:00:06<0:02:52 Epoch 13/10 - Batch 6/160, Loss: 0.0342 0.0322\n",
      "training: 0:00:06<0:02:31 Epoch 13/10 - Batch 7/160, Loss: 0.0326 0.0264\n",
      "training: 0:00:07<0:02:14 Epoch 13/10 - Batch 8/160, Loss: 0.0325 0.0323\n",
      "training: 0:00:07<0:02:02 Epoch 13/10 - Batch 9/160, Loss: 0.0337 0.0383\n",
      "training: 0:00:07<0:01:51 Epoch 13/10 - Batch 10/160, Loss: 0.0357 0.0437\n",
      "training: 0:00:07<0:01:43 Epoch 13/10 - Batch 11/160, Loss: 0.0365 0.0395\n",
      "training: 0:00:07<0:01:36 Epoch 13/10 - Batch 12/160, Loss: 0.0354 0.0311\n",
      "training: 0:00:08<0:01:30 Epoch 13/10 - Batch 13/160, Loss: 0.0354 0.0354\n",
      "training: 0:00:08<0:01:25 Epoch 13/10 - Batch 14/160, Loss: 0.0339 0.0279\n",
      "training: 0:00:08<0:01:20 Epoch 13/10 - Batch 15/160, Loss: 0.0339 0.0340\n",
      "training: 0:00:08<0:01:16 Epoch 13/10 - Batch 16/160, Loss: 0.0327 0.0276\n",
      "training: 0:00:08<0:01:13 Epoch 13/10 - Batch 17/160, Loss: 0.0329 0.0337\n",
      "training: 0:00:08<0:01:10 Epoch 13/10 - Batch 18/160, Loss: 0.0345 0.0409\n",
      "training: 0:00:09<0:01:07 Epoch 13/10 - Batch 19/160, Loss: 0.0351 0.0374\n",
      "training: 0:00:09<0:01:04 Epoch 13/10 - Batch 20/160, Loss: 0.0358 0.0388\n",
      "training: 0:00:09<0:01:02 Epoch 13/10 - Batch 21/160, Loss: 0.0347 0.0305\n",
      "training: 0:00:09<0:01:00 Epoch 13/10 - Batch 22/160, Loss: 0.0346 0.0340\n",
      "training: 0:00:09<0:00:58 Epoch 13/10 - Batch 23/160, Loss: 0.0348 0.0359\n",
      "training: 0:00:10<0:00:56 Epoch 13/10 - Batch 24/160, Loss: 0.0341 0.0313\n",
      "training: 0:00:10<0:00:55 Epoch 13/10 - Batch 25/160, Loss: 0.0335 0.0308\n",
      "training: 0:00:10<0:00:53 Epoch 13/10 - Batch 26/160, Loss: 0.0336 0.0343\n",
      "training: 0:00:10<0:00:51 Epoch 13/10 - Batch 27/160, Loss: 0.0332 0.0315\n",
      "training: 0:00:10<0:00:50 Epoch 13/10 - Batch 28/160, Loss: 0.0340 0.0370\n",
      "training: 0:00:10<0:00:49 Epoch 13/10 - Batch 29/160, Loss: 0.0339 0.0338\n",
      "training: 0:00:11<0:00:48 Epoch 13/10 - Batch 30/160, Loss: 0.0333 0.0306\n",
      "training: 0:00:11<0:00:46 Epoch 13/10 - Batch 31/160, Loss: 0.0333 0.0334\n",
      "training: 0:00:11<0:00:45 Epoch 13/10 - Batch 32/160, Loss: 0.0332 0.0330\n",
      "training: 0:00:11<0:00:44 Epoch 13/10 - Batch 33/160, Loss: 0.0318 0.0259\n",
      "training: 0:00:11<0:00:43 Epoch 13/10 - Batch 34/160, Loss: 0.0329 0.0372\n",
      "training: 0:00:12<0:00:42 Epoch 13/10 - Batch 35/160, Loss: 0.0327 0.0321\n",
      "training: 0:00:12<0:00:42 Epoch 13/10 - Batch 36/160, Loss: 0.0346 0.0420\n",
      "training: 0:00:12<0:00:41 Epoch 13/10 - Batch 37/160, Loss: 0.0358 0.0408\n",
      "training: 0:00:12<0:00:40 Epoch 13/10 - Batch 38/160, Loss: 0.0362 0.0376\n",
      "training: 0:00:12<0:00:39 Epoch 13/10 - Batch 39/160, Loss: 0.0363 0.0366\n",
      "training: 0:00:12<0:00:38 Epoch 13/10 - Batch 40/160, Loss: 0.0369 0.0393\n",
      "training: 0:00:13<0:00:38 Epoch 13/10 - Batch 41/160, Loss: 0.0361 0.0332\n",
      "training: 0:00:13<0:00:37 Epoch 13/10 - Batch 42/160, Loss: 0.0355 0.0330\n",
      "training: 0:00:13<0:00:36 Epoch 13/10 - Batch 43/160, Loss: 0.0346 0.0307\n",
      "training: 0:00:13<0:00:36 Epoch 13/10 - Batch 44/160, Loss: 0.0333 0.0282\n",
      "training: 0:00:13<0:00:35 Epoch 13/10 - Batch 45/160, Loss: 0.0325 0.0296\n",
      "training: 0:00:14<0:00:34 Epoch 13/10 - Batch 46/160, Loss: 0.0330 0.0346\n",
      "training: 0:00:14<0:00:34 Epoch 13/10 - Batch 47/160, Loss: 0.0333 0.0347\n",
      "training: 0:00:14<0:00:33 Epoch 13/10 - Batch 48/160, Loss: 0.0349 0.0415\n",
      "training: 0:00:14<0:00:33 Epoch 13/10 - Batch 49/160, Loss: 0.0367 0.0436\n",
      "training: 0:00:14<0:00:32 Epoch 13/10 - Batch 50/160, Loss: 0.0351 0.0286\n",
      "training: 0:00:14<0:00:31 Epoch 13/10 - Batch 51/160, Loss: 0.0345 0.0323\n",
      "training: 0:00:15<0:00:31 Epoch 13/10 - Batch 52/160, Loss: 0.0338 0.0309\n",
      "training: 0:00:15<0:00:30 Epoch 13/10 - Batch 53/160, Loss: 0.0344 0.0369\n",
      "training: 0:00:15<0:00:30 Epoch 13/10 - Batch 54/160, Loss: 0.0358 0.0413\n",
      "training: 0:00:15<0:00:29 Epoch 13/10 - Batch 55/160, Loss: 0.0372 0.0430\n",
      "training: 0:00:15<0:00:29 Epoch 13/10 - Batch 56/160, Loss: 0.0370 0.0360\n",
      "training: 0:00:16<0:00:28 Epoch 13/10 - Batch 57/160, Loss: 0.0357 0.0305\n",
      "training: 0:00:16<0:00:28 Epoch 13/10 - Batch 58/160, Loss: 0.0355 0.0346\n",
      "training: 0:00:16<0:00:28 Epoch 13/10 - Batch 59/160, Loss: 0.0362 0.0390\n",
      "training: 0:00:16<0:00:27 Epoch 13/10 - Batch 60/160, Loss: 0.0365 0.0380\n",
      "training: 0:00:16<0:00:27 Epoch 13/10 - Batch 61/160, Loss: 0.0374 0.0410\n",
      "training: 0:00:16<0:00:26 Epoch 13/10 - Batch 62/160, Loss: 0.0374 0.0373\n",
      "training: 0:00:17<0:00:26 Epoch 13/10 - Batch 63/160, Loss: 0.0375 0.0377\n",
      "training: 0:00:17<0:00:25 Epoch 13/10 - Batch 64/160, Loss: 0.0381 0.0407\n",
      "training: 0:00:17<0:00:25 Epoch 13/10 - Batch 65/160, Loss: 0.0387 0.0411\n",
      "training: 0:00:17<0:00:25 Epoch 13/10 - Batch 66/160, Loss: 0.0373 0.0319\n",
      "training: 0:00:17<0:00:24 Epoch 13/10 - Batch 67/160, Loss: 0.0365 0.0332\n",
      "training: 0:00:18<0:00:24 Epoch 13/10 - Batch 68/160, Loss: 0.0361 0.0343\n",
      "training: 0:00:18<0:00:24 Epoch 13/10 - Batch 69/160, Loss: 0.0363 0.0374\n",
      "training: 0:00:18<0:00:23 Epoch 13/10 - Batch 70/160, Loss: 0.0370 0.0396\n",
      "training: 0:00:18<0:00:23 Epoch 13/10 - Batch 71/160, Loss: 0.0371 0.0377\n",
      "training: 0:00:18<0:00:22 Epoch 13/10 - Batch 72/160, Loss: 0.0360 0.0315\n",
      "training: 0:00:18<0:00:22 Epoch 13/10 - Batch 73/160, Loss: 0.0362 0.0372\n",
      "training: 0:00:19<0:00:22 Epoch 13/10 - Batch 74/160, Loss: 0.0362 0.0359\n",
      "training: 0:00:19<0:00:21 Epoch 13/10 - Batch 75/160, Loss: 0.0348 0.0295\n",
      "training: 0:00:19<0:00:21 Epoch 13/10 - Batch 76/160, Loss: 0.0349 0.0354\n",
      "training: 0:00:19<0:00:21 Epoch 13/10 - Batch 77/160, Loss: 0.0330 0.0253\n",
      "training: 0:00:19<0:00:20 Epoch 13/10 - Batch 78/160, Loss: 0.0332 0.0342\n",
      "training: 0:00:20<0:00:20 Epoch 13/10 - Batch 79/160, Loss: 0.0342 0.0381\n",
      "training: 0:00:20<0:00:20 Epoch 13/10 - Batch 80/160, Loss: 0.0339 0.0326\n",
      "training: 0:00:20<0:00:19 Epoch 13/10 - Batch 81/160, Loss: 0.0333 0.0310\n",
      "training: 0:00:20<0:00:19 Epoch 13/10 - Batch 82/160, Loss: 0.0350 0.0419\n",
      "training: 0:00:20<0:00:19 Epoch 13/10 - Batch 83/160, Loss: 0.0351 0.0356\n",
      "training: 0:00:20<0:00:18 Epoch 13/10 - Batch 84/160, Loss: 0.0358 0.0385\n",
      "training: 0:00:21<0:00:18 Epoch 13/10 - Batch 85/160, Loss: 0.0362 0.0377\n",
      "training: 0:00:21<0:00:18 Epoch 13/10 - Batch 86/160, Loss: 0.0361 0.0357\n",
      "training: 0:00:21<0:00:18 Epoch 13/10 - Batch 87/160, Loss: 0.0372 0.0418\n",
      "training: 0:00:21<0:00:17 Epoch 13/10 - Batch 88/160, Loss: 0.0380 0.0410\n",
      "training: 0:00:21<0:00:17 Epoch 13/10 - Batch 89/160, Loss: 0.0377 0.0365\n",
      "training: 0:00:22<0:00:17 Epoch 13/10 - Batch 90/160, Loss: 0.0363 0.0306\n",
      "training: 0:00:22<0:00:16 Epoch 13/10 - Batch 91/160, Loss: 0.0344 0.0271\n",
      "training: 0:00:22<0:00:16 Epoch 13/10 - Batch 92/160, Loss: 0.0344 0.0343\n",
      "training: 0:00:22<0:00:16 Epoch 13/10 - Batch 93/160, Loss: 0.0356 0.0405\n",
      "training: 0:00:22<0:00:16 Epoch 13/10 - Batch 94/160, Loss: 0.0351 0.0327\n",
      "training: 0:00:22<0:00:15 Epoch 13/10 - Batch 95/160, Loss: 0.0341 0.0304\n",
      "training: 0:00:23<0:00:15 Epoch 13/10 - Batch 96/160, Loss: 0.0356 0.0414\n",
      "training: 0:00:23<0:00:15 Epoch 13/10 - Batch 97/160, Loss: 0.0366 0.0405\n",
      "training: 0:00:23<0:00:14 Epoch 13/10 - Batch 98/160, Loss: 0.0370 0.0388\n",
      "training: 0:00:23<0:00:14 Epoch 13/10 - Batch 99/160, Loss: 0.0371 0.0377\n",
      "training: 0:00:23<0:00:14 Epoch 13/10 - Batch 100/160, Loss: 0.0373 0.0380\n",
      "training: 0:00:24<0:00:14 Epoch 13/10 - Batch 101/160, Loss: 0.0389 0.0451\n",
      "saving searchnet-model-12-100.pt\n",
      "\n",
      "training: 0:00:25<0:00:14 Epoch 13/10 - Batch 102/160, Loss: 0.0387 0.0381\n",
      "training: 0:00:25<0:00:14 Epoch 13/10 - Batch 103/160, Loss: 0.0373 0.0315\n",
      "training: 0:00:25<0:00:13 Epoch 13/10 - Batch 104/160, Loss: 0.0364 0.0327\n",
      "training: 0:00:25<0:00:13 Epoch 13/10 - Batch 105/160, Loss: 0.0350 0.0298\n",
      "training: 0:00:26<0:00:13 Epoch 13/10 - Batch 106/160, Loss: 0.0348 0.0336\n",
      "training: 0:00:26<0:00:13 Epoch 13/10 - Batch 107/160, Loss: 0.0340 0.0307\n",
      "training: 0:00:26<0:00:12 Epoch 13/10 - Batch 108/160, Loss: 0.0336 0.0319\n",
      "training: 0:00:26<0:00:12 Epoch 13/10 - Batch 109/160, Loss: 0.0326 0.0287\n",
      "training: 0:00:26<0:00:12 Epoch 13/10 - Batch 110/160, Loss: 0.0334 0.0365\n",
      "training: 0:00:27<0:00:11 Epoch 13/10 - Batch 111/160, Loss: 0.0344 0.0388\n",
      "training: 0:00:27<0:00:11 Epoch 13/10 - Batch 112/160, Loss: 0.0349 0.0368\n",
      "training: 0:00:27<0:00:11 Epoch 13/10 - Batch 113/160, Loss: 0.0340 0.0304\n",
      "training: 0:00:27<0:00:11 Epoch 13/10 - Batch 114/160, Loss: 0.0348 0.0379\n",
      "training: 0:00:27<0:00:10 Epoch 13/10 - Batch 115/160, Loss: 0.0343 0.0324\n",
      "training: 0:00:27<0:00:10 Epoch 13/10 - Batch 116/160, Loss: 0.0357 0.0410\n",
      "training: 0:00:28<0:00:10 Epoch 13/10 - Batch 117/160, Loss: 0.0349 0.0318\n",
      "training: 0:00:28<0:00:10 Epoch 13/10 - Batch 118/160, Loss: 0.0356 0.0385\n",
      "training: 0:00:28<0:00:09 Epoch 13/10 - Batch 119/160, Loss: 0.0349 0.0320\n",
      "training: 0:00:28<0:00:09 Epoch 13/10 - Batch 120/160, Loss: 0.0358 0.0394\n",
      "training: 0:00:28<0:00:09 Epoch 13/10 - Batch 121/160, Loss: 0.0354 0.0341\n",
      "training: 0:00:29<0:00:09 Epoch 13/10 - Batch 122/160, Loss: 0.0341 0.0288\n",
      "training: 0:00:29<0:00:08 Epoch 13/10 - Batch 123/160, Loss: 0.0345 0.0362\n",
      "training: 0:00:29<0:00:08 Epoch 13/10 - Batch 124/160, Loss: 0.0338 0.0308\n",
      "training: 0:00:29<0:00:08 Epoch 13/10 - Batch 125/160, Loss: 0.0343 0.0365\n",
      "training: 0:00:29<0:00:08 Epoch 13/10 - Batch 126/160, Loss: 0.0349 0.0374\n",
      "training: 0:00:29<0:00:07 Epoch 13/10 - Batch 127/160, Loss: 0.0349 0.0347\n",
      "training: 0:00:30<0:00:07 Epoch 13/10 - Batch 128/160, Loss: 0.0343 0.0321\n",
      "saving searchnet-model-12-128.pt\n",
      "\n",
      "\n",
      "Validation:\n",
      "\n",
      "validation: 0:00:31<0:00:07 Epoch 13/10 - Batch 129/160, Loss: 0.0098 0.0098\n",
      "validation: 0:00:31<0:00:07 Epoch 13/10 - Batch 130/160, Loss: 0.0102 0.0106\n",
      "validation: 0:00:32<0:00:07 Epoch 13/10 - Batch 131/160, Loss: 0.0094 0.0079\n",
      "validation: 0:00:32<0:00:06 Epoch 13/10 - Batch 132/160, Loss: 0.0089 0.0073\n",
      "validation: 0:00:32<0:00:06 Epoch 13/10 - Batch 133/160, Loss: 0.0085 0.0070\n",
      "validation: 0:00:32<0:00:06 Epoch 13/10 - Batch 134/160, Loss: 0.0083 0.0073\n",
      "validation: 0:00:32<0:00:05 Epoch 13/10 - Batch 135/160, Loss: 0.0081 0.0067\n",
      "validation: 0:00:32<0:00:05 Epoch 13/10 - Batch 136/160, Loss: 0.0079 0.0070\n",
      "validation: 0:00:32<0:00:05 Epoch 13/10 - Batch 137/160, Loss: 0.0078 0.0063\n",
      "validation: 0:00:32<0:00:05 Epoch 13/10 - Batch 138/160, Loss: 0.0077 0.0076\n",
      "validation: 0:00:32<0:00:04 Epoch 13/10 - Batch 139/160, Loss: 0.0076 0.0067\n",
      "validation: 0:00:32<0:00:04 Epoch 13/10 - Batch 140/160, Loss: 0.0080 0.0123\n",
      "validation: 0:00:32<0:00:04 Epoch 13/10 - Batch 141/160, Loss: 0.0080 0.0076\n",
      "validation: 0:00:33<0:00:04 Epoch 13/10 - Batch 142/160, Loss: 0.0080 0.0078\n",
      "validation: 0:00:33<0:00:03 Epoch 13/10 - Batch 143/160, Loss: 0.0079 0.0069\n",
      "validation: 0:00:33<0:00:03 Epoch 13/10 - Batch 144/160, Loss: 0.0082 0.0129\n",
      "validation: 0:00:33<0:00:03 Epoch 13/10 - Batch 145/160, Loss: 0.0088 0.0188\n",
      "validation: 0:00:33<0:00:03 Epoch 13/10 - Batch 146/160, Loss: 0.0087 0.0068\n",
      "validation: 0:00:33<0:00:02 Epoch 13/10 - Batch 147/160, Loss: 0.0087 0.0080\n",
      "validation: 0:00:33<0:00:02 Epoch 13/10 - Batch 148/160, Loss: 0.0087 0.0091\n",
      "validation: 0:00:33<0:00:02 Epoch 13/10 - Batch 149/160, Loss: 0.0086 0.0068\n",
      "validation: 0:00:34<0:00:02 Epoch 13/10 - Batch 150/160, Loss: 0.0087 0.0108\n",
      "validation: 0:00:34<0:00:02 Epoch 13/10 - Batch 151/160, Loss: 0.0087 0.0080\n",
      "validation: 0:00:34<0:00:01 Epoch 13/10 - Batch 152/160, Loss: 0.0087 0.0079\n",
      "validation: 0:00:34<0:00:01 Epoch 13/10 - Batch 153/160, Loss: 0.0088 0.0120\n",
      "validation: 0:00:34<0:00:01 Epoch 13/10 - Batch 154/160, Loss: 0.0088 0.0086\n",
      "validation: 0:00:34<0:00:01 Epoch 13/10 - Batch 155/160, Loss: 0.0087 0.0066\n",
      "validation: 0:00:35<0:00:00 Epoch 13/10 - Batch 156/160, Loss: 0.0087 0.0089\n",
      "validation: 0:00:35<0:00:00 Epoch 13/10 - Batch 157/160, Loss: 0.0086 0.0066\n",
      "validation: 0:00:35<0:00:00 Epoch 13/10 - Batch 158/160, Loss: 0.0086 0.0064\n",
      "validation: 0:00:35<0:00:00 Epoch 13/10 - Batch 159/160, Loss: 0.0085 0.0066\n",
      "validation: 0:00:35<0:00:00 Epoch 13/10 - Batch 160/160, Loss: 0.0086 0.0101\n",
      "recreating train_snippets_shuffled\n",
      "\n",
      "Validation\n",
      "\n",
      "training: 0:00:00<0:01:11 Epoch 14/10 - Batch 1/160, Loss: 0.0362 0.0362\n",
      "saving searchnet-model-13-0.pt\n",
      "\n",
      "training: 0:00:01<0:02:07 Epoch 14/10 - Batch 2/160, Loss: 0.0375 0.0383\n",
      "training: 0:00:06<0:05:48 Epoch 14/10 - Batch 3/160, Loss: 0.0355 0.0327\n",
      "training: 0:00:11<0:07:24 Epoch 14/10 - Batch 4/160, Loss: 0.0329 0.0267\n",
      "training: 0:00:11<0:06:01 Epoch 14/10 - Batch 5/160, Loss: 0.0332 0.0342\n",
      "training: 0:00:22<0:09:30 Epoch 14/10 - Batch 6/160, Loss: 0.0330 0.0323\n",
      "training: 0:00:22<0:08:12 Epoch 14/10 - Batch 7/160, Loss: 0.0341 0.0383\n",
      "training: 0:00:22<0:07:11 Epoch 14/10 - Batch 8/160, Loss: 0.0351 0.0388\n",
      "training: 0:00:22<0:06:24 Epoch 14/10 - Batch 9/160, Loss: 0.0353 0.0362\n",
      "training: 0:00:23<0:05:46 Epoch 14/10 - Batch 10/160, Loss: 0.0348 0.0326\n",
      "training: 0:00:23<0:05:15 Epoch 14/10 - Batch 11/160, Loss: 0.0341 0.0314\n",
      "training: 0:00:23<0:04:49 Epoch 14/10 - Batch 12/160, Loss: 0.0338 0.0327\n",
      "training: 0:00:23<0:04:27 Epoch 14/10 - Batch 13/160, Loss: 0.0337 0.0334\n",
      "training: 0:00:23<0:04:08 Epoch 14/10 - Batch 14/160, Loss: 0.0354 0.0422\n",
      "training: 0:00:24<0:03:52 Epoch 14/10 - Batch 15/160, Loss: 0.0356 0.0362\n",
      "training: 0:00:24<0:03:37 Epoch 14/10 - Batch 16/160, Loss: 0.0357 0.0363\n",
      "training: 0:00:24<0:03:25 Epoch 14/10 - Batch 17/160, Loss: 0.0341 0.0277\n",
      "training: 0:00:24<0:03:13 Epoch 14/10 - Batch 18/160, Loss: 0.0348 0.0376\n",
      "training: 0:00:24<0:03:03 Epoch 14/10 - Batch 19/160, Loss: 0.0344 0.0330\n",
      "training: 0:00:24<0:02:54 Epoch 14/10 - Batch 20/160, Loss: 0.0348 0.0361\n",
      "training: 0:00:25<0:02:46 Epoch 14/10 - Batch 21/160, Loss: 0.0346 0.0338\n",
      "training: 0:00:25<0:02:38 Epoch 14/10 - Batch 22/160, Loss: 0.0351 0.0373\n",
      "training: 0:00:25<0:02:31 Epoch 14/10 - Batch 23/160, Loss: 0.0372 0.0454\n",
      "training: 0:00:25<0:02:25 Epoch 14/10 - Batch 24/160, Loss: 0.0385 0.0437\n",
      "training: 0:00:25<0:02:19 Epoch 14/10 - Batch 25/160, Loss: 0.0379 0.0357\n",
      "training: 0:00:26<0:02:14 Epoch 14/10 - Batch 26/160, Loss: 0.0360 0.0285\n",
      "training: 0:00:26<0:02:09 Epoch 14/10 - Batch 27/160, Loss: 0.0364 0.0378\n",
      "training: 0:00:26<0:02:04 Epoch 14/10 - Batch 28/160, Loss: 0.0355 0.0321\n",
      "training: 0:00:26<0:02:00 Epoch 14/10 - Batch 29/160, Loss: 0.0363 0.0395\n",
      "training: 0:00:26<0:01:56 Epoch 14/10 - Batch 30/160, Loss: 0.0375 0.0422\n",
      "training: 0:00:26<0:01:52 Epoch 14/10 - Batch 31/160, Loss: 0.0355 0.0274\n",
      "training: 0:00:27<0:01:48 Epoch 14/10 - Batch 32/160, Loss: 0.0359 0.0376\n",
      "training: 0:00:27<0:01:45 Epoch 14/10 - Batch 33/160, Loss: 0.0367 0.0400\n",
      "training: 0:00:27<0:01:41 Epoch 14/10 - Batch 34/160, Loss: 0.0364 0.0351\n",
      "training: 0:00:27<0:01:38 Epoch 14/10 - Batch 35/160, Loss: 0.0361 0.0348\n",
      "training: 0:00:27<0:01:36 Epoch 14/10 - Batch 36/160, Loss: 0.0350 0.0306\n",
      "training: 0:00:28<0:01:33 Epoch 14/10 - Batch 37/160, Loss: 0.0344 0.0319\n",
      "training: 0:00:28<0:01:30 Epoch 14/10 - Batch 38/160, Loss: 0.0344 0.0347\n",
      "training: 0:00:28<0:01:28 Epoch 14/10 - Batch 39/160, Loss: 0.0346 0.0355\n",
      "training: 0:00:28<0:01:25 Epoch 14/10 - Batch 40/160, Loss: 0.0342 0.0327\n",
      "training: 0:00:28<0:01:23 Epoch 14/10 - Batch 41/160, Loss: 0.0342 0.0341\n",
      "training: 0:00:28<0:01:21 Epoch 14/10 - Batch 42/160, Loss: 0.0340 0.0331\n",
      "training: 0:00:29<0:01:19 Epoch 14/10 - Batch 43/160, Loss: 0.0331 0.0295\n",
      "training: 0:00:29<0:01:17 Epoch 14/10 - Batch 44/160, Loss: 0.0355 0.0454\n",
      "training: 0:00:29<0:01:15 Epoch 14/10 - Batch 45/160, Loss: 0.0364 0.0400\n",
      "training: 0:00:29<0:01:13 Epoch 14/10 - Batch 46/160, Loss: 0.0363 0.0360\n",
      "training: 0:00:29<0:01:11 Epoch 14/10 - Batch 47/160, Loss: 0.0350 0.0298\n",
      "training: 0:00:30<0:01:10 Epoch 14/10 - Batch 48/160, Loss: 0.0353 0.0365\n",
      "training: 0:00:30<0:01:08 Epoch 14/10 - Batch 49/160, Loss: 0.0359 0.0382\n",
      "training: 0:00:30<0:01:07 Epoch 14/10 - Batch 50/160, Loss: 0.0360 0.0362\n",
      "training: 0:00:30<0:01:05 Epoch 14/10 - Batch 51/160, Loss: 0.0356 0.0340\n",
      "training: 0:00:30<0:01:04 Epoch 14/10 - Batch 52/160, Loss: 0.0366 0.0409\n",
      "training: 0:00:31<0:01:02 Epoch 14/10 - Batch 53/160, Loss: 0.0361 0.0340\n",
      "training: 0:00:31<0:01:01 Epoch 14/10 - Batch 54/160, Loss: 0.0349 0.0299\n",
      "training: 0:00:31<0:00:59 Epoch 14/10 - Batch 55/160, Loss: 0.0356 0.0384\n",
      "training: 0:00:31<0:00:58 Epoch 14/10 - Batch 56/160, Loss: 0.0350 0.0327\n",
      "training: 0:00:31<0:00:57 Epoch 14/10 - Batch 57/160, Loss: 0.0357 0.0383\n",
      "training: 0:00:31<0:00:56 Epoch 14/10 - Batch 58/160, Loss: 0.0353 0.0338\n",
      "training: 0:00:32<0:00:54 Epoch 14/10 - Batch 59/160, Loss: 0.0333 0.0254\n",
      "training: 0:00:32<0:00:53 Epoch 14/10 - Batch 60/160, Loss: 0.0333 0.0333\n",
      "training: 0:00:32<0:00:52 Epoch 14/10 - Batch 61/160, Loss: 0.0322 0.0276\n",
      "training: 0:00:32<0:00:51 Epoch 14/10 - Batch 62/160, Loss: 0.0321 0.0315\n",
      "training: 0:00:32<0:00:50 Epoch 14/10 - Batch 63/160, Loss: 0.0316 0.0298\n",
      "training: 0:00:33<0:00:49 Epoch 14/10 - Batch 64/160, Loss: 0.0306 0.0264\n",
      "training: 0:00:33<0:00:48 Epoch 14/10 - Batch 65/160, Loss: 0.0311 0.0332\n",
      "training: 0:00:33<0:00:47 Epoch 14/10 - Batch 66/160, Loss: 0.0315 0.0332\n",
      "training: 0:00:33<0:00:46 Epoch 14/10 - Batch 67/160, Loss: 0.0325 0.0364\n",
      "training: 0:00:33<0:00:45 Epoch 14/10 - Batch 68/160, Loss: 0.0360 0.0501\n",
      "training: 0:00:33<0:00:44 Epoch 14/10 - Batch 69/160, Loss: 0.0346 0.0290\n",
      "training: 0:00:34<0:00:43 Epoch 14/10 - Batch 70/160, Loss: 0.0348 0.0358\n",
      "training: 0:00:34<0:00:43 Epoch 14/10 - Batch 71/160, Loss: 0.0341 0.0310\n",
      "training: 0:00:34<0:00:42 Epoch 14/10 - Batch 72/160, Loss: 0.0363 0.0452\n",
      "training: 0:00:34<0:00:41 Epoch 14/10 - Batch 73/160, Loss: 0.0364 0.0367\n",
      "training: 0:00:34<0:00:40 Epoch 14/10 - Batch 74/160, Loss: 0.0371 0.0401\n",
      "training: 0:00:35<0:00:39 Epoch 14/10 - Batch 75/160, Loss: 0.0375 0.0392\n",
      "training: 0:00:35<0:00:38 Epoch 14/10 - Batch 76/160, Loss: 0.0373 0.0365\n",
      "training: 0:00:35<0:00:38 Epoch 14/10 - Batch 77/160, Loss: 0.0363 0.0323\n",
      "training: 0:00:35<0:00:37 Epoch 14/10 - Batch 78/160, Loss: 0.0356 0.0329\n",
      "training: 0:00:35<0:00:36 Epoch 14/10 - Batch 79/160, Loss: 0.0350 0.0326\n",
      "training: 0:00:35<0:00:35 Epoch 14/10 - Batch 80/160, Loss: 0.0350 0.0347\n",
      "training: 0:00:36<0:00:35 Epoch 14/10 - Batch 81/160, Loss: 0.0360 0.0401\n",
      "training: 0:00:36<0:00:34 Epoch 14/10 - Batch 82/160, Loss: 0.0354 0.0330\n",
      "training: 0:00:36<0:00:33 Epoch 14/10 - Batch 83/160, Loss: 0.0342 0.0294\n",
      "training: 0:00:36<0:00:33 Epoch 14/10 - Batch 84/160, Loss: 0.0367 0.0466\n",
      "training: 0:00:36<0:00:32 Epoch 14/10 - Batch 85/160, Loss: 0.0357 0.0317\n",
      "training: 0:00:37<0:00:31 Epoch 14/10 - Batch 86/160, Loss: 0.0352 0.0334\n",
      "training: 0:00:37<0:00:31 Epoch 14/10 - Batch 87/160, Loss: 0.0360 0.0390\n",
      "training: 0:00:37<0:00:30 Epoch 14/10 - Batch 88/160, Loss: 0.0366 0.0393\n",
      "training: 0:00:37<0:00:29 Epoch 14/10 - Batch 89/160, Loss: 0.0347 0.0271\n",
      "training: 0:00:37<0:00:29 Epoch 14/10 - Batch 90/160, Loss: 0.0336 0.0291\n",
      "training: 0:00:37<0:00:28 Epoch 14/10 - Batch 91/160, Loss: 0.0322 0.0265\n",
      "training: 0:00:38<0:00:28 Epoch 14/10 - Batch 92/160, Loss: 0.0313 0.0279\n",
      "training: 0:00:38<0:00:27 Epoch 14/10 - Batch 93/160, Loss: 0.0322 0.0358\n",
      "training: 0:00:38<0:00:27 Epoch 14/10 - Batch 94/160, Loss: 0.0324 0.0333\n",
      "training: 0:00:38<0:00:26 Epoch 14/10 - Batch 95/160, Loss: 0.0331 0.0358\n",
      "training: 0:00:38<0:00:25 Epoch 14/10 - Batch 96/160, Loss: 0.0345 0.0400\n",
      "training: 0:00:39<0:00:25 Epoch 14/10 - Batch 97/160, Loss: 0.0349 0.0364\n",
      "training: 0:00:39<0:00:24 Epoch 14/10 - Batch 98/160, Loss: 0.0353 0.0370\n",
      "training: 0:00:39<0:00:24 Epoch 14/10 - Batch 99/160, Loss: 0.0347 0.0322\n",
      "training: 0:00:39<0:00:23 Epoch 14/10 - Batch 100/160, Loss: 0.0345 0.0338\n",
      "training: 0:00:39<0:00:23 Epoch 14/10 - Batch 101/160, Loss: 0.0329 0.0268\n",
      "saving searchnet-model-13-100.pt\n",
      "\n",
      "training: 0:00:41<0:00:23 Epoch 14/10 - Batch 102/160, Loss: 0.0343 0.0400\n",
      "training: 0:00:41<0:00:22 Epoch 14/10 - Batch 103/160, Loss: 0.0357 0.0413\n",
      "training: 0:00:41<0:00:22 Epoch 14/10 - Batch 104/160, Loss: 0.0349 0.0314\n",
      "training: 0:00:41<0:00:21 Epoch 14/10 - Batch 105/160, Loss: 0.0346 0.0333\n",
      "training: 0:00:41<0:00:21 Epoch 14/10 - Batch 106/160, Loss: 0.0351 0.0372\n",
      "training: 0:00:41<0:00:20 Epoch 14/10 - Batch 107/160, Loss: 0.0344 0.0319\n",
      "training: 0:00:42<0:00:20 Epoch 14/10 - Batch 108/160, Loss: 0.0333 0.0289\n",
      "training: 0:00:42<0:00:19 Epoch 14/10 - Batch 109/160, Loss: 0.0355 0.0442\n",
      "training: 0:00:42<0:00:19 Epoch 14/10 - Batch 110/160, Loss: 0.0342 0.0290\n",
      "training: 0:00:42<0:00:18 Epoch 14/10 - Batch 111/160, Loss: 0.0349 0.0376\n",
      "training: 0:00:42<0:00:18 Epoch 14/10 - Batch 112/160, Loss: 0.0364 0.0425\n",
      "training: 0:00:43<0:00:17 Epoch 14/10 - Batch 113/160, Loss: 0.0355 0.0317\n",
      "training: 0:00:43<0:00:17 Epoch 14/10 - Batch 114/160, Loss: 0.0359 0.0374\n",
      "training: 0:00:43<0:00:16 Epoch 14/10 - Batch 115/160, Loss: 0.0362 0.0374\n",
      "training: 0:00:43<0:00:16 Epoch 14/10 - Batch 116/160, Loss: 0.0385 0.0479\n",
      "training: 0:00:43<0:00:16 Epoch 14/10 - Batch 117/160, Loss: 0.0385 0.0383\n",
      "training: 0:00:43<0:00:15 Epoch 14/10 - Batch 118/160, Loss: 0.0379 0.0354\n",
      "training: 0:00:44<0:00:15 Epoch 14/10 - Batch 119/160, Loss: 0.0366 0.0315\n",
      "training: 0:00:44<0:00:14 Epoch 14/10 - Batch 120/160, Loss: 0.0348 0.0275\n",
      "training: 0:00:44<0:00:14 Epoch 14/10 - Batch 121/160, Loss: 0.0356 0.0388\n",
      "training: 0:00:44<0:00:13 Epoch 14/10 - Batch 122/160, Loss: 0.0366 0.0405\n",
      "training: 0:00:44<0:00:13 Epoch 14/10 - Batch 123/160, Loss: 0.0364 0.0355\n",
      "training: 0:00:45<0:00:13 Epoch 14/10 - Batch 124/160, Loss: 0.0347 0.0280\n",
      "training: 0:00:45<0:00:12 Epoch 14/10 - Batch 125/160, Loss: 0.0355 0.0388\n",
      "training: 0:00:45<0:00:12 Epoch 14/10 - Batch 126/160, Loss: 0.0354 0.0350\n",
      "training: 0:00:45<0:00:11 Epoch 14/10 - Batch 127/160, Loss: 0.0351 0.0336\n",
      "training: 0:00:45<0:00:11 Epoch 14/10 - Batch 128/160, Loss: 0.0355 0.0371\n",
      "saving searchnet-model-13-128.pt\n",
      "\n",
      "\n",
      "Validation:\n",
      "\n",
      "validation: 0:01:16<0:00:18 Epoch 14/10 - Batch 129/160, Loss: 0.0093 0.0093\n",
      "validation: 0:01:17<0:00:17 Epoch 14/10 - Batch 130/160, Loss: 0.0096 0.0100\n",
      "validation: 0:01:18<0:00:17 Epoch 14/10 - Batch 131/160, Loss: 0.0106 0.0126\n",
      "validation: 0:01:18<0:00:16 Epoch 14/10 - Batch 132/160, Loss: 0.0106 0.0105\n",
      "validation: 0:01:18<0:00:15 Epoch 14/10 - Batch 133/160, Loss: 0.0101 0.0081\n",
      "validation: 0:01:18<0:00:15 Epoch 14/10 - Batch 134/160, Loss: 0.0098 0.0081\n",
      "validation: 0:01:18<0:00:14 Epoch 14/10 - Batch 135/160, Loss: 0.0094 0.0070\n",
      "validation: 0:01:18<0:00:13 Epoch 14/10 - Batch 136/160, Loss: 0.0101 0.0151\n",
      "validation: 0:01:18<0:00:13 Epoch 14/10 - Batch 137/160, Loss: 0.0126 0.0332\n",
      "validation: 0:01:19<0:00:12 Epoch 14/10 - Batch 138/160, Loss: 0.0121 0.0071\n",
      "validation: 0:01:19<0:00:11 Epoch 14/10 - Batch 139/160, Loss: 0.0116 0.0067\n",
      "validation: 0:01:19<0:00:11 Epoch 14/10 - Batch 140/160, Loss: 0.0118 0.0134\n",
      "validation: 0:01:19<0:00:10 Epoch 14/10 - Batch 141/160, Loss: 0.0114 0.0067\n",
      "validation: 0:01:19<0:00:10 Epoch 14/10 - Batch 142/160, Loss: 0.0111 0.0071\n",
      "validation: 0:01:19<0:00:09 Epoch 14/10 - Batch 143/160, Loss: 0.0108 0.0070\n",
      "validation: 0:01:19<0:00:08 Epoch 14/10 - Batch 144/160, Loss: 0.0107 0.0086\n",
      "validation: 0:01:19<0:00:08 Epoch 14/10 - Batch 145/160, Loss: 0.0104 0.0069\n",
      "validation: 0:01:19<0:00:07 Epoch 14/10 - Batch 146/160, Loss: 0.0104 0.0097\n",
      "validation: 0:01:19<0:00:07 Epoch 14/10 - Batch 147/160, Loss: 0.0104 0.0112\n",
      "validation: 0:01:19<0:00:06 Epoch 14/10 - Batch 148/160, Loss: 0.0102 0.0065\n",
      "validation: 0:01:19<0:00:05 Epoch 14/10 - Batch 149/160, Loss: 0.0101 0.0079\n",
      "validation: 0:01:19<0:00:05 Epoch 14/10 - Batch 150/160, Loss: 0.0100 0.0068\n",
      "validation: 0:01:19<0:00:04 Epoch 14/10 - Batch 151/160, Loss: 0.0100 0.0107\n",
      "validation: 0:01:19<0:00:04 Epoch 14/10 - Batch 152/160, Loss: 0.0102 0.0149\n",
      "validation: 0:01:19<0:00:03 Epoch 14/10 - Batch 153/160, Loss: 0.0101 0.0078\n",
      "validation: 0:01:19<0:00:03 Epoch 14/10 - Batch 154/160, Loss: 0.0100 0.0066\n",
      "validation: 0:01:20<0:00:02 Epoch 14/10 - Batch 155/160, Loss: 0.0099 0.0064\n",
      "validation: 0:01:20<0:00:02 Epoch 14/10 - Batch 156/160, Loss: 0.0098 0.0075\n",
      "validation: 0:01:20<0:00:01 Epoch 14/10 - Batch 157/160, Loss: 0.0098 0.0118\n",
      "validation: 0:01:20<0:00:01 Epoch 14/10 - Batch 158/160, Loss: 0.0098 0.0090\n",
      "validation: 0:01:20<0:00:00 Epoch 14/10 - Batch 159/160, Loss: 0.0098 0.0098\n",
      "validation: 0:01:20<0:00:00 Epoch 14/10 - Batch 160/160, Loss: 0.0099 0.0125\n",
      "recreating train_snippets_shuffled\n",
      "\n",
      "Validation\n",
      "\n",
      "training: 0:00:00<0:00:32 Epoch 15/10 - Batch 1/160, Loss: 0.0389 0.0389\n",
      "saving searchnet-model-14-0.pt\n",
      "\n",
      "training: 0:00:18<0:23:58 Epoch 15/10 - Batch 2/160, Loss: 0.0331 0.0292\n",
      "training: 0:00:22<0:19:42 Epoch 15/10 - Batch 3/160, Loss: 0.0353 0.0386\n",
      "training: 0:00:25<0:16:22 Epoch 15/10 - Batch 4/160, Loss: 0.0362 0.0384\n",
      "training: 0:00:26<0:13:28 Epoch 15/10 - Batch 5/160, Loss: 0.0354 0.0330\n",
      "training: 0:00:27<0:11:40 Epoch 15/10 - Batch 6/160, Loss: 0.0358 0.0369\n",
      "training: 0:00:27<0:10:00 Epoch 15/10 - Batch 7/160, Loss: 0.0360 0.0369\n",
      "training: 0:00:27<0:08:45 Epoch 15/10 - Batch 8/160, Loss: 0.0371 0.0411\n",
      "training: 0:00:27<0:07:47 Epoch 15/10 - Batch 9/160, Loss: 0.0368 0.0356\n",
      "training: 0:00:28<0:07:00 Epoch 15/10 - Batch 10/160, Loss: 0.0352 0.0288\n",
      "training: 0:00:28<0:06:22 Epoch 15/10 - Batch 11/160, Loss: 0.0343 0.0310\n",
      "training: 0:00:28<0:05:50 Epoch 15/10 - Batch 12/160, Loss: 0.0351 0.0383\n",
      "training: 0:00:28<0:05:23 Epoch 15/10 - Batch 13/160, Loss: 0.0343 0.0309\n",
      "training: 0:00:28<0:05:00 Epoch 15/10 - Batch 14/160, Loss: 0.0346 0.0358\n",
      "training: 0:00:28<0:04:39 Epoch 15/10 - Batch 15/160, Loss: 0.0349 0.0361\n",
      "training: 0:00:29<0:04:22 Epoch 15/10 - Batch 16/160, Loss: 0.0341 0.0310\n",
      "training: 0:00:29<0:04:06 Epoch 15/10 - Batch 17/160, Loss: 0.0332 0.0294\n",
      "training: 0:00:29<0:03:52 Epoch 15/10 - Batch 18/160, Loss: 0.0338 0.0361\n",
      "training: 0:00:29<0:03:40 Epoch 15/10 - Batch 19/160, Loss: 0.0333 0.0314\n",
      "training: 0:00:29<0:03:29 Epoch 15/10 - Batch 20/160, Loss: 0.0329 0.0312\n",
      "training: 0:00:30<0:03:18 Epoch 15/10 - Batch 21/160, Loss: 0.0326 0.0316\n",
      "training: 0:00:30<0:03:09 Epoch 15/10 - Batch 22/160, Loss: 0.0348 0.0437\n",
      "training: 0:00:30<0:03:01 Epoch 15/10 - Batch 23/160, Loss: 0.0352 0.0368\n",
      "training: 0:00:30<0:02:53 Epoch 15/10 - Batch 24/160, Loss: 0.0348 0.0332\n",
      "training: 0:00:30<0:02:46 Epoch 15/10 - Batch 25/160, Loss: 0.0362 0.0417\n",
      "training: 0:00:30<0:02:39 Epoch 15/10 - Batch 26/160, Loss: 0.0368 0.0391\n",
      "training: 0:00:31<0:02:33 Epoch 15/10 - Batch 27/160, Loss: 0.0388 0.0469\n",
      "training: 0:00:31<0:02:27 Epoch 15/10 - Batch 28/160, Loss: 0.0378 0.0338\n",
      "training: 0:00:31<0:02:22 Epoch 15/10 - Batch 29/160, Loss: 0.0381 0.0396\n",
      "training: 0:00:31<0:02:17 Epoch 15/10 - Batch 30/160, Loss: 0.0369 0.0321\n",
      "training: 0:00:31<0:02:12 Epoch 15/10 - Batch 31/160, Loss: 0.0362 0.0330\n",
      "training: 0:00:32<0:02:08 Epoch 15/10 - Batch 32/160, Loss: 0.0353 0.0319\n",
      "training: 0:00:32<0:02:04 Epoch 15/10 - Batch 33/160, Loss: 0.0354 0.0358\n",
      "training: 0:00:32<0:02:00 Epoch 15/10 - Batch 34/160, Loss: 0.0342 0.0296\n",
      "training: 0:00:32<0:01:56 Epoch 15/10 - Batch 35/160, Loss: 0.0345 0.0354\n",
      "training: 0:00:32<0:01:52 Epoch 15/10 - Batch 36/160, Loss: 0.0335 0.0296\n",
      "training: 0:00:32<0:01:49 Epoch 15/10 - Batch 37/160, Loss: 0.0337 0.0343\n",
      "training: 0:00:33<0:01:46 Epoch 15/10 - Batch 38/160, Loss: 0.0341 0.0357\n",
      "training: 0:00:33<0:01:43 Epoch 15/10 - Batch 39/160, Loss: 0.0343 0.0351\n",
      "training: 0:00:33<0:01:40 Epoch 15/10 - Batch 40/160, Loss: 0.0327 0.0266\n",
      "training: 0:00:33<0:01:37 Epoch 15/10 - Batch 41/160, Loss: 0.0330 0.0339\n",
      "training: 0:00:33<0:01:35 Epoch 15/10 - Batch 42/160, Loss: 0.0334 0.0349\n",
      "training: 0:00:34<0:01:32 Epoch 15/10 - Batch 43/160, Loss: 0.0331 0.0323\n",
      "training: 0:00:34<0:01:30 Epoch 15/10 - Batch 44/160, Loss: 0.0345 0.0399\n",
      "training: 0:00:34<0:01:28 Epoch 15/10 - Batch 45/160, Loss: 0.0350 0.0369\n",
      "training: 0:00:34<0:01:25 Epoch 15/10 - Batch 46/160, Loss: 0.0347 0.0336\n",
      "training: 0:00:34<0:01:23 Epoch 15/10 - Batch 47/160, Loss: 0.0349 0.0359\n",
      "training: 0:00:34<0:01:21 Epoch 15/10 - Batch 48/160, Loss: 0.0353 0.0368\n",
      "training: 0:00:35<0:01:19 Epoch 15/10 - Batch 49/160, Loss: 0.0362 0.0396\n",
      "training: 0:00:35<0:01:17 Epoch 15/10 - Batch 50/160, Loss: 0.0341 0.0259\n",
      "training: 0:00:35<0:01:15 Epoch 15/10 - Batch 51/160, Loss: 0.0321 0.0238\n",
      "training: 0:00:35<0:01:14 Epoch 15/10 - Batch 52/160, Loss: 0.0332 0.0377\n",
      "training: 0:00:35<0:01:12 Epoch 15/10 - Batch 53/160, Loss: 0.0328 0.0311\n",
      "training: 0:00:36<0:01:10 Epoch 15/10 - Batch 54/160, Loss: 0.0327 0.0323\n",
      "training: 0:00:36<0:01:09 Epoch 15/10 - Batch 55/160, Loss: 0.0342 0.0401\n",
      "training: 0:00:36<0:01:07 Epoch 15/10 - Batch 56/160, Loss: 0.0339 0.0329\n",
      "training: 0:00:36<0:01:06 Epoch 15/10 - Batch 57/160, Loss: 0.0332 0.0306\n",
      "training: 0:00:36<0:01:04 Epoch 15/10 - Batch 58/160, Loss: 0.0347 0.0406\n",
      "training: 0:00:36<0:01:03 Epoch 15/10 - Batch 59/160, Loss: 0.0332 0.0273\n",
      "training: 0:00:37<0:01:01 Epoch 15/10 - Batch 60/160, Loss: 0.0337 0.0354\n",
      "training: 0:00:37<0:01:00 Epoch 15/10 - Batch 61/160, Loss: 0.0328 0.0295\n",
      "training: 0:00:37<0:00:59 Epoch 15/10 - Batch 62/160, Loss: 0.0334 0.0357\n",
      "training: 0:00:37<0:00:58 Epoch 15/10 - Batch 63/160, Loss: 0.0335 0.0339\n",
      "training: 0:00:37<0:00:56 Epoch 15/10 - Batch 64/160, Loss: 0.0354 0.0432\n",
      "training: 0:00:38<0:00:55 Epoch 15/10 - Batch 65/160, Loss: 0.0365 0.0405\n",
      "training: 0:00:38<0:00:54 Epoch 15/10 - Batch 66/160, Loss: 0.0352 0.0302\n",
      "training: 0:00:38<0:00:53 Epoch 15/10 - Batch 67/160, Loss: 0.0362 0.0402\n",
      "training: 0:00:38<0:00:52 Epoch 15/10 - Batch 68/160, Loss: 0.0354 0.0321\n",
      "training: 0:00:38<0:00:51 Epoch 15/10 - Batch 69/160, Loss: 0.0360 0.0382\n",
      "training: 0:00:38<0:00:50 Epoch 15/10 - Batch 70/160, Loss: 0.0357 0.0349\n",
      "training: 0:00:39<0:00:49 Epoch 15/10 - Batch 71/160, Loss: 0.0342 0.0281\n",
      "training: 0:00:39<0:00:48 Epoch 15/10 - Batch 72/160, Loss: 0.0339 0.0328\n",
      "training: 0:00:39<0:00:47 Epoch 15/10 - Batch 73/160, Loss: 0.0341 0.0347\n",
      "training: 0:00:39<0:00:46 Epoch 15/10 - Batch 74/160, Loss: 0.0344 0.0354\n",
      "training: 0:00:39<0:00:45 Epoch 15/10 - Batch 75/160, Loss: 0.0334 0.0296\n",
      "training: 0:00:40<0:00:44 Epoch 15/10 - Batch 76/160, Loss: 0.0344 0.0383\n",
      "training: 0:00:40<0:00:43 Epoch 15/10 - Batch 77/160, Loss: 0.0341 0.0329\n",
      "training: 0:00:40<0:00:42 Epoch 15/10 - Batch 78/160, Loss: 0.0338 0.0329\n",
      "training: 0:00:40<0:00:41 Epoch 15/10 - Batch 79/160, Loss: 0.0329 0.0294\n",
      "training: 0:00:40<0:00:40 Epoch 15/10 - Batch 80/160, Loss: 0.0329 0.0326\n",
      "training: 0:00:40<0:00:39 Epoch 15/10 - Batch 81/160, Loss: 0.0316 0.0266\n",
      "training: 0:00:41<0:00:39 Epoch 15/10 - Batch 82/160, Loss: 0.0326 0.0368\n",
      "training: 0:00:41<0:00:38 Epoch 15/10 - Batch 83/160, Loss: 0.0325 0.0319\n",
      "training: 0:00:41<0:00:37 Epoch 15/10 - Batch 84/160, Loss: 0.0316 0.0281\n",
      "training: 0:00:41<0:00:36 Epoch 15/10 - Batch 85/160, Loss: 0.0319 0.0330\n",
      "training: 0:00:41<0:00:36 Epoch 15/10 - Batch 86/160, Loss: 0.0333 0.0391\n",
      "training: 0:00:42<0:00:35 Epoch 15/10 - Batch 87/160, Loss: 0.0314 0.0239\n",
      "training: 0:00:42<0:00:34 Epoch 15/10 - Batch 88/160, Loss: 0.0327 0.0379\n",
      "training: 0:00:42<0:00:33 Epoch 15/10 - Batch 89/160, Loss: 0.0330 0.0341\n",
      "training: 0:00:42<0:00:33 Epoch 15/10 - Batch 90/160, Loss: 0.0341 0.0385\n",
      "training: 0:00:42<0:00:32 Epoch 15/10 - Batch 91/160, Loss: 0.0334 0.0305\n",
      "training: 0:00:42<0:00:31 Epoch 15/10 - Batch 92/160, Loss: 0.0335 0.0340\n",
      "training: 0:00:43<0:00:31 Epoch 15/10 - Batch 93/160, Loss: 0.0352 0.0420\n",
      "training: 0:00:43<0:00:30 Epoch 15/10 - Batch 94/160, Loss: 0.0344 0.0313\n",
      "training: 0:00:43<0:00:29 Epoch 15/10 - Batch 95/160, Loss: 0.0337 0.0306\n",
      "training: 0:00:43<0:00:29 Epoch 15/10 - Batch 96/160, Loss: 0.0331 0.0309\n",
      "training: 0:00:43<0:00:28 Epoch 15/10 - Batch 97/160, Loss: 0.0312 0.0238\n",
      "training: 0:00:44<0:00:27 Epoch 15/10 - Batch 98/160, Loss: 0.0302 0.0262\n",
      "training: 0:00:44<0:00:27 Epoch 15/10 - Batch 99/160, Loss: 0.0312 0.0350\n",
      "training: 0:00:44<0:00:26 Epoch 15/10 - Batch 100/160, Loss: 0.0323 0.0368\n",
      "training: 0:00:44<0:00:26 Epoch 15/10 - Batch 101/160, Loss: 0.0319 0.0303\n",
      "saving searchnet-model-14-100.pt\n",
      "\n",
      "training: 0:00:47<0:00:27 Epoch 15/10 - Batch 102/160, Loss: 0.0319 0.0319\n",
      "training: 0:00:48<0:00:26 Epoch 15/10 - Batch 103/160, Loss: 0.0342 0.0434\n",
      "training: 0:00:48<0:00:26 Epoch 15/10 - Batch 104/160, Loss: 0.0355 0.0407\n",
      "training: 0:00:48<0:00:25 Epoch 15/10 - Batch 105/160, Loss: 0.0349 0.0325\n",
      "training: 0:00:48<0:00:24 Epoch 15/10 - Batch 106/160, Loss: 0.0352 0.0365\n",
      "training: 0:00:48<0:00:24 Epoch 15/10 - Batch 107/160, Loss: 0.0364 0.0409\n",
      "training: 0:00:49<0:00:23 Epoch 15/10 - Batch 108/160, Loss: 0.0365 0.0370\n",
      "training: 0:00:49<0:00:23 Epoch 15/10 - Batch 109/160, Loss: 0.0368 0.0379\n",
      "training: 0:00:49<0:00:22 Epoch 15/10 - Batch 110/160, Loss: 0.0349 0.0275\n",
      "training: 0:00:49<0:00:21 Epoch 15/10 - Batch 111/160, Loss: 0.0339 0.0300\n",
      "training: 0:00:49<0:00:21 Epoch 15/10 - Batch 112/160, Loss: 0.0346 0.0371\n",
      "training: 0:00:49<0:00:20 Epoch 15/10 - Batch 113/160, Loss: 0.0351 0.0372\n",
      "training: 0:00:50<0:00:20 Epoch 15/10 - Batch 114/160, Loss: 0.0344 0.0315\n",
      "training: 0:00:50<0:00:19 Epoch 15/10 - Batch 115/160, Loss: 0.0360 0.0423\n",
      "training: 0:00:50<0:00:19 Epoch 15/10 - Batch 116/160, Loss: 0.0346 0.0293\n",
      "training: 0:00:50<0:00:18 Epoch 15/10 - Batch 117/160, Loss: 0.0329 0.0258\n",
      "training: 0:00:51<0:00:18 Epoch 15/10 - Batch 118/160, Loss: 0.0335 0.0361\n",
      "training: 0:00:58<0:00:20 Epoch 15/10 - Batch 119/160, Loss: 0.0340 0.0359\n",
      "training: 0:01:05<0:00:21 Epoch 15/10 - Batch 120/160, Loss: 0.0351 0.0394\n",
      "training: 0:01:05<0:00:21 Epoch 15/10 - Batch 121/160, Loss: 0.0341 0.0300\n",
      "training: 0:01:05<0:00:20 Epoch 15/10 - Batch 122/160, Loss: 0.0338 0.0329\n",
      "training: 0:01:05<0:00:19 Epoch 15/10 - Batch 123/160, Loss: 0.0350 0.0397\n",
      "training: 0:01:05<0:00:19 Epoch 15/10 - Batch 124/160, Loss: 0.0340 0.0301\n",
      "training: 0:01:05<0:00:18 Epoch 15/10 - Batch 125/160, Loss: 0.0344 0.0357\n",
      "training: 0:01:06<0:00:17 Epoch 15/10 - Batch 126/160, Loss: 0.0337 0.0310\n",
      "training: 0:01:06<0:00:17 Epoch 15/10 - Batch 127/160, Loss: 0.0343 0.0368\n",
      "training: 0:01:06<0:00:16 Epoch 15/10 - Batch 128/160, Loss: 0.0346 0.0356\n",
      "saving searchnet-model-14-128.pt\n",
      "\n",
      "\n",
      "Validation:\n",
      "\n",
      "validation: 0:01:27<0:00:20 Epoch 15/10 - Batch 129/160, Loss: 0.0109 0.0109\n",
      "validation: 0:01:28<0:00:20 Epoch 15/10 - Batch 130/160, Loss: 0.0083 0.0056\n",
      "validation: 0:01:28<0:00:19 Epoch 15/10 - Batch 131/160, Loss: 0.0077 0.0064\n",
      "validation: 0:01:28<0:00:18 Epoch 15/10 - Batch 132/160, Loss: 0.0075 0.0072\n",
      "validation: 0:01:28<0:00:17 Epoch 15/10 - Batch 133/160, Loss: 0.0077 0.0085\n",
      "validation: 0:01:28<0:00:17 Epoch 15/10 - Batch 134/160, Loss: 0.0075 0.0062\n",
      "validation: 0:01:28<0:00:16 Epoch 15/10 - Batch 135/160, Loss: 0.0077 0.0091\n",
      "validation: 0:01:28<0:00:15 Epoch 15/10 - Batch 136/160, Loss: 0.0079 0.0095\n",
      "validation: 0:01:28<0:00:14 Epoch 15/10 - Batch 137/160, Loss: 0.0079 0.0074\n",
      "validation: 0:01:28<0:00:14 Epoch 15/10 - Batch 138/160, Loss: 0.0078 0.0071\n",
      "validation: 0:01:28<0:00:13 Epoch 15/10 - Batch 139/160, Loss: 0.0079 0.0090\n",
      "validation: 0:01:28<0:00:12 Epoch 15/10 - Batch 140/160, Loss: 0.0078 0.0067\n",
      "validation: 0:01:28<0:00:11 Epoch 15/10 - Batch 141/160, Loss: 0.0079 0.0094\n",
      "validation: 0:01:28<0:00:11 Epoch 15/10 - Batch 142/160, Loss: 0.0081 0.0105\n",
      "validation: 0:01:29<0:00:10 Epoch 15/10 - Batch 143/160, Loss: 0.0082 0.0096\n",
      "validation: 0:01:29<0:00:09 Epoch 15/10 - Batch 144/160, Loss: 0.0082 0.0079\n",
      "validation: 0:01:29<0:00:09 Epoch 15/10 - Batch 145/160, Loss: 0.0081 0.0061\n",
      "validation: 0:01:29<0:00:08 Epoch 15/10 - Batch 146/160, Loss: 0.0080 0.0070\n",
      "validation: 0:01:29<0:00:07 Epoch 15/10 - Batch 147/160, Loss: 0.0082 0.0120\n",
      "validation: 0:01:29<0:00:07 Epoch 15/10 - Batch 148/160, Loss: 0.0082 0.0070\n",
      "validation: 0:01:29<0:00:06 Epoch 15/10 - Batch 149/160, Loss: 0.0081 0.0061\n",
      "validation: 0:01:29<0:00:05 Epoch 15/10 - Batch 150/160, Loss: 0.0080 0.0068\n",
      "validation: 0:01:29<0:00:05 Epoch 15/10 - Batch 151/160, Loss: 0.0080 0.0087\n",
      "validation: 0:01:29<0:00:04 Epoch 15/10 - Batch 152/160, Loss: 0.0080 0.0081\n",
      "validation: 0:01:29<0:00:04 Epoch 15/10 - Batch 153/160, Loss: 0.0080 0.0083\n",
      "validation: 0:01:29<0:00:03 Epoch 15/10 - Batch 154/160, Loss: 0.0080 0.0067\n",
      "validation: 0:01:29<0:00:02 Epoch 15/10 - Batch 155/160, Loss: 0.0080 0.0084\n",
      "validation: 0:01:29<0:00:02 Epoch 15/10 - Batch 156/160, Loss: 0.0080 0.0087\n",
      "validation: 0:01:29<0:00:01 Epoch 15/10 - Batch 157/160, Loss: 0.0080 0.0077\n",
      "validation: 0:01:29<0:00:01 Epoch 15/10 - Batch 158/160, Loss: 0.0081 0.0101\n",
      "validation: 0:01:29<0:00:00 Epoch 15/10 - Batch 159/160, Loss: 0.0080 0.0065\n",
      "validation: 0:01:29<0:00:00 Epoch 15/10 - Batch 160/160, Loss: 0.0081 0.0108\n",
      "recreating train_snippets_shuffled\n",
      "\n",
      "Validation\n",
      "\n",
      "training: 0:00:00<0:00:28 Epoch 16/10 - Batch 1/160, Loss: 0.0279 0.0279\n",
      "saving searchnet-model-15-0.pt\n",
      "\n",
      "training: 0:00:13<0:17:54 Epoch 16/10 - Batch 2/160, Loss: 0.0362 0.0418\n",
      "training: 0:00:15<0:13:17 Epoch 16/10 - Batch 3/160, Loss: 0.0364 0.0368\n",
      "training: 0:00:18<0:12:18 Epoch 16/10 - Batch 4/160, Loss: 0.0350 0.0316\n",
      "training: 0:00:21<0:11:20 Epoch 16/10 - Batch 5/160, Loss: 0.0352 0.0358\n",
      "training: 0:00:23<0:10:12 Epoch 16/10 - Batch 6/160, Loss: 0.0351 0.0347\n",
      "training: 0:00:24<0:08:46 Epoch 16/10 - Batch 7/160, Loss: 0.0337 0.0286\n",
      "training: 0:00:24<0:07:40 Epoch 16/10 - Batch 8/160, Loss: 0.0327 0.0289\n",
      "training: 0:00:24<0:06:49 Epoch 16/10 - Batch 9/160, Loss: 0.0305 0.0220\n",
      "training: 0:00:24<0:06:09 Epoch 16/10 - Batch 10/160, Loss: 0.0326 0.0406\n",
      "training: 0:00:24<0:05:35 Epoch 16/10 - Batch 11/160, Loss: 0.0322 0.0307\n",
      "training: 0:00:24<0:05:08 Epoch 16/10 - Batch 12/160, Loss: 0.0327 0.0348\n",
      "training: 0:00:25<0:04:44 Epoch 16/10 - Batch 13/160, Loss: 0.0340 0.0393\n",
      "training: 0:00:25<0:04:24 Epoch 16/10 - Batch 14/160, Loss: 0.0335 0.0315\n",
      "training: 0:00:25<0:04:06 Epoch 16/10 - Batch 15/160, Loss: 0.0338 0.0350\n",
      "training: 0:00:25<0:03:51 Epoch 16/10 - Batch 16/160, Loss: 0.0345 0.0371\n",
      "training: 0:00:25<0:03:37 Epoch 16/10 - Batch 17/160, Loss: 0.0357 0.0406\n",
      "training: 0:00:26<0:03:25 Epoch 16/10 - Batch 18/160, Loss: 0.0356 0.0354\n",
      "training: 0:00:26<0:03:15 Epoch 16/10 - Batch 19/160, Loss: 0.0363 0.0390\n",
      "training: 0:00:26<0:03:05 Epoch 16/10 - Batch 20/160, Loss: 0.0332 0.0208\n",
      "training: 0:00:26<0:02:56 Epoch 16/10 - Batch 21/160, Loss: 0.0318 0.0261\n",
      "training: 0:00:26<0:02:48 Epoch 16/10 - Batch 22/160, Loss: 0.0333 0.0392\n",
      "training: 0:00:27<0:02:40 Epoch 16/10 - Batch 23/160, Loss: 0.0329 0.0313\n",
      "training: 0:00:27<0:02:34 Epoch 16/10 - Batch 24/160, Loss: 0.0329 0.0329\n",
      "training: 0:00:27<0:02:27 Epoch 16/10 - Batch 25/160, Loss: 0.0334 0.0353\n",
      "training: 0:00:27<0:02:22 Epoch 16/10 - Batch 26/160, Loss: 0.0338 0.0353\n",
      "training: 0:00:27<0:02:16 Epoch 16/10 - Batch 27/160, Loss: 0.0346 0.0377\n",
      "training: 0:00:27<0:02:11 Epoch 16/10 - Batch 28/160, Loss: 0.0347 0.0353\n",
      "training: 0:00:28<0:02:07 Epoch 16/10 - Batch 29/160, Loss: 0.0351 0.0365\n",
      "training: 0:00:28<0:02:02 Epoch 16/10 - Batch 30/160, Loss: 0.0348 0.0335\n",
      "training: 0:00:28<0:01:58 Epoch 16/10 - Batch 31/160, Loss: 0.0346 0.0339\n",
      "training: 0:00:28<0:01:54 Epoch 16/10 - Batch 32/160, Loss: 0.0342 0.0325\n",
      "training: 0:00:28<0:01:51 Epoch 16/10 - Batch 33/160, Loss: 0.0326 0.0261\n",
      "training: 0:00:29<0:01:47 Epoch 16/10 - Batch 34/160, Loss: 0.0329 0.0342\n",
      "training: 0:00:29<0:01:44 Epoch 16/10 - Batch 35/160, Loss: 0.0333 0.0350\n",
      "training: 0:00:29<0:01:41 Epoch 16/10 - Batch 36/160, Loss: 0.0338 0.0355\n",
      "training: 0:00:29<0:01:38 Epoch 16/10 - Batch 37/160, Loss: 0.0343 0.0366\n",
      "training: 0:00:29<0:01:35 Epoch 16/10 - Batch 38/160, Loss: 0.0351 0.0383\n",
      "training: 0:00:29<0:01:32 Epoch 16/10 - Batch 39/160, Loss: 0.0340 0.0296\n",
      "training: 0:00:30<0:01:30 Epoch 16/10 - Batch 40/160, Loss: 0.0348 0.0379\n",
      "training: 0:00:30<0:01:28 Epoch 16/10 - Batch 41/160, Loss: 0.0338 0.0299\n",
      "training: 0:00:30<0:01:25 Epoch 16/10 - Batch 42/160, Loss: 0.0328 0.0285\n",
      "training: 0:00:30<0:01:23 Epoch 16/10 - Batch 43/160, Loss: 0.0343 0.0402\n",
      "training: 0:00:30<0:01:21 Epoch 16/10 - Batch 44/160, Loss: 0.0339 0.0325\n",
      "training: 0:00:31<0:01:19 Epoch 16/10 - Batch 45/160, Loss: 0.0346 0.0372\n",
      "training: 0:00:31<0:01:17 Epoch 16/10 - Batch 46/160, Loss: 0.0359 0.0415\n",
      "training: 0:00:31<0:01:15 Epoch 16/10 - Batch 47/160, Loss: 0.0365 0.0386\n",
      "training: 0:00:31<0:01:13 Epoch 16/10 - Batch 48/160, Loss: 0.0372 0.0400\n",
      "training: 0:00:31<0:01:12 Epoch 16/10 - Batch 49/160, Loss: 0.0359 0.0310\n",
      "training: 0:00:31<0:01:10 Epoch 16/10 - Batch 50/160, Loss: 0.0339 0.0258\n",
      "training: 0:00:32<0:01:08 Epoch 16/10 - Batch 51/160, Loss: 0.0339 0.0340\n",
      "training: 0:00:32<0:01:07 Epoch 16/10 - Batch 52/160, Loss: 0.0321 0.0249\n",
      "training: 0:00:32<0:01:05 Epoch 16/10 - Batch 53/160, Loss: 0.0342 0.0424\n",
      "training: 0:00:32<0:01:04 Epoch 16/10 - Batch 54/160, Loss: 0.0340 0.0334\n",
      "training: 0:00:32<0:01:02 Epoch 16/10 - Batch 55/160, Loss: 0.0339 0.0335\n",
      "training: 0:00:33<0:01:01 Epoch 16/10 - Batch 56/160, Loss: 0.0328 0.0283\n",
      "training: 0:00:33<0:01:00 Epoch 16/10 - Batch 57/160, Loss: 0.0324 0.0307\n",
      "training: 0:00:33<0:00:58 Epoch 16/10 - Batch 58/160, Loss: 0.0321 0.0308\n",
      "training: 0:00:33<0:00:57 Epoch 16/10 - Batch 59/160, Loss: 0.0330 0.0369\n",
      "training: 0:00:33<0:00:56 Epoch 16/10 - Batch 60/160, Loss: 0.0326 0.0307\n",
      "training: 0:00:33<0:00:55 Epoch 16/10 - Batch 61/160, Loss: 0.0341 0.0405\n",
      "training: 0:00:34<0:00:54 Epoch 16/10 - Batch 62/160, Loss: 0.0359 0.0427\n",
      "training: 0:00:34<0:00:52 Epoch 16/10 - Batch 63/160, Loss: 0.0357 0.0352\n",
      "training: 0:00:34<0:00:51 Epoch 16/10 - Batch 64/160, Loss: 0.0367 0.0407\n",
      "training: 0:00:34<0:00:50 Epoch 16/10 - Batch 65/160, Loss: 0.0362 0.0339\n",
      "training: 0:00:34<0:00:49 Epoch 16/10 - Batch 66/160, Loss: 0.0365 0.0378\n",
      "training: 0:00:35<0:00:48 Epoch 16/10 - Batch 67/160, Loss: 0.0351 0.0295\n",
      "training: 0:00:35<0:00:47 Epoch 16/10 - Batch 68/160, Loss: 0.0362 0.0405\n",
      "training: 0:00:35<0:00:46 Epoch 16/10 - Batch 69/160, Loss: 0.0356 0.0333\n",
      "training: 0:00:35<0:00:45 Epoch 16/10 - Batch 70/160, Loss: 0.0338 0.0266\n",
      "training: 0:00:35<0:00:44 Epoch 16/10 - Batch 71/160, Loss: 0.0328 0.0289\n",
      "training: 0:00:35<0:00:43 Epoch 16/10 - Batch 72/160, Loss: 0.0330 0.0338\n",
      "training: 0:00:36<0:00:43 Epoch 16/10 - Batch 73/160, Loss: 0.0335 0.0352\n",
      "training: 0:00:36<0:00:42 Epoch 16/10 - Batch 74/160, Loss: 0.0347 0.0396\n",
      "training: 0:00:36<0:00:41 Epoch 16/10 - Batch 75/160, Loss: 0.0367 0.0449\n",
      "training: 0:00:36<0:00:40 Epoch 16/10 - Batch 76/160, Loss: 0.0364 0.0349\n",
      "training: 0:00:36<0:00:39 Epoch 16/10 - Batch 77/160, Loss: 0.0351 0.0297\n",
      "training: 0:00:37<0:00:38 Epoch 16/10 - Batch 78/160, Loss: 0.0357 0.0385\n",
      "training: 0:00:37<0:00:38 Epoch 16/10 - Batch 79/160, Loss: 0.0342 0.0279\n",
      "training: 0:00:37<0:00:37 Epoch 16/10 - Batch 80/160, Loss: 0.0344 0.0354\n",
      "training: 0:00:37<0:00:36 Epoch 16/10 - Batch 81/160, Loss: 0.0348 0.0365\n",
      "training: 0:00:37<0:00:35 Epoch 16/10 - Batch 82/160, Loss: 0.0347 0.0342\n",
      "training: 0:00:37<0:00:35 Epoch 16/10 - Batch 83/160, Loss: 0.0345 0.0337\n",
      "training: 0:00:38<0:00:34 Epoch 16/10 - Batch 84/160, Loss: 0.0324 0.0239\n",
      "training: 0:00:38<0:00:33 Epoch 16/10 - Batch 85/160, Loss: 0.0311 0.0258\n",
      "training: 0:00:38<0:00:33 Epoch 16/10 - Batch 86/160, Loss: 0.0318 0.0348\n",
      "training: 0:00:38<0:00:32 Epoch 16/10 - Batch 87/160, Loss: 0.0323 0.0345\n",
      "training: 0:00:38<0:00:31 Epoch 16/10 - Batch 88/160, Loss: 0.0330 0.0354\n",
      "training: 0:00:39<0:00:31 Epoch 16/10 - Batch 89/160, Loss: 0.0352 0.0444\n",
      "training: 0:00:39<0:00:30 Epoch 16/10 - Batch 90/160, Loss: 0.0363 0.0404\n",
      "training: 0:00:39<0:00:29 Epoch 16/10 - Batch 91/160, Loss: 0.0365 0.0374\n",
      "training: 0:00:39<0:00:29 Epoch 16/10 - Batch 92/160, Loss: 0.0361 0.0345\n",
      "training: 0:00:39<0:00:28 Epoch 16/10 - Batch 93/160, Loss: 0.0366 0.0386\n",
      "training: 0:00:39<0:00:28 Epoch 16/10 - Batch 94/160, Loss: 0.0367 0.0368\n",
      "training: 0:00:40<0:00:27 Epoch 16/10 - Batch 95/160, Loss: 0.0356 0.0315\n",
      "training: 0:00:40<0:00:26 Epoch 16/10 - Batch 96/160, Loss: 0.0350 0.0323\n",
      "training: 0:00:40<0:00:26 Epoch 16/10 - Batch 97/160, Loss: 0.0360 0.0400\n",
      "training: 0:00:40<0:00:25 Epoch 16/10 - Batch 98/160, Loss: 0.0355 0.0333\n",
      "training: 0:00:40<0:00:25 Epoch 16/10 - Batch 99/160, Loss: 0.0331 0.0239\n",
      "training: 0:00:41<0:00:24 Epoch 16/10 - Batch 100/160, Loss: 0.0332 0.0336\n",
      "training: 0:00:41<0:00:24 Epoch 16/10 - Batch 101/160, Loss: 0.0320 0.0271\n",
      "saving searchnet-model-15-100.pt\n",
      "\n",
      "training: 0:00:42<0:00:24 Epoch 16/10 - Batch 102/160, Loss: 0.0321 0.0326\n",
      "training: 0:00:42<0:00:23 Epoch 16/10 - Batch 103/160, Loss: 0.0329 0.0361\n",
      "training: 0:00:42<0:00:23 Epoch 16/10 - Batch 104/160, Loss: 0.0333 0.0351\n",
      "training: 0:00:43<0:00:22 Epoch 16/10 - Batch 105/160, Loss: 0.0339 0.0359\n",
      "training: 0:00:43<0:00:22 Epoch 16/10 - Batch 106/160, Loss: 0.0336 0.0326\n",
      "training: 0:00:43<0:00:21 Epoch 16/10 - Batch 107/160, Loss: 0.0338 0.0347\n",
      "training: 0:00:43<0:00:20 Epoch 16/10 - Batch 108/160, Loss: 0.0336 0.0325\n",
      "training: 0:00:43<0:00:20 Epoch 16/10 - Batch 109/160, Loss: 0.0337 0.0342\n",
      "training: 0:00:43<0:00:19 Epoch 16/10 - Batch 110/160, Loss: 0.0336 0.0334\n",
      "training: 0:00:44<0:00:19 Epoch 16/10 - Batch 111/160, Loss: 0.0343 0.0368\n",
      "training: 0:00:44<0:00:18 Epoch 16/10 - Batch 112/160, Loss: 0.0326 0.0262\n",
      "training: 0:00:44<0:00:18 Epoch 16/10 - Batch 113/160, Loss: 0.0344 0.0414\n",
      "training: 0:00:44<0:00:18 Epoch 16/10 - Batch 114/160, Loss: 0.0341 0.0330\n",
      "training: 0:00:44<0:00:17 Epoch 16/10 - Batch 115/160, Loss: 0.0352 0.0396\n",
      "training: 0:00:45<0:00:17 Epoch 16/10 - Batch 116/160, Loss: 0.0350 0.0343\n",
      "training: 0:00:45<0:00:16 Epoch 16/10 - Batch 117/160, Loss: 0.0357 0.0386\n",
      "training: 0:00:45<0:00:16 Epoch 16/10 - Batch 118/160, Loss: 0.0348 0.0309\n",
      "training: 0:00:45<0:00:15 Epoch 16/10 - Batch 119/160, Loss: 0.0339 0.0304\n",
      "training: 0:00:45<0:00:15 Epoch 16/10 - Batch 120/160, Loss: 0.0321 0.0251\n",
      "training: 0:00:45<0:00:14 Epoch 16/10 - Batch 121/160, Loss: 0.0323 0.0331\n",
      "training: 0:00:46<0:00:14 Epoch 16/10 - Batch 122/160, Loss: 0.0319 0.0300\n",
      "training: 0:00:46<0:00:13 Epoch 16/10 - Batch 123/160, Loss: 0.0321 0.0331\n",
      "training: 0:00:46<0:00:13 Epoch 16/10 - Batch 124/160, Loss: 0.0327 0.0350\n",
      "training: 0:00:46<0:00:13 Epoch 16/10 - Batch 125/160, Loss: 0.0335 0.0369\n",
      "training: 0:00:46<0:00:12 Epoch 16/10 - Batch 126/160, Loss: 0.0346 0.0389\n",
      "training: 0:00:47<0:00:12 Epoch 16/10 - Batch 127/160, Loss: 0.0358 0.0407\n",
      "training: 0:00:47<0:00:11 Epoch 16/10 - Batch 128/160, Loss: 0.0357 0.0354\n",
      "saving searchnet-model-15-128.pt\n",
      "\n",
      "\n",
      "Validation:\n",
      "\n",
      "validation: 0:01:15<0:00:18 Epoch 16/10 - Batch 129/160, Loss: 0.0101 0.0101\n",
      "validation: 0:01:16<0:00:17 Epoch 16/10 - Batch 130/160, Loss: 0.0132 0.0163\n",
      "validation: 0:01:16<0:00:17 Epoch 16/10 - Batch 131/160, Loss: 0.0128 0.0118\n",
      "validation: 0:01:16<0:00:16 Epoch 16/10 - Batch 132/160, Loss: 0.0135 0.0158\n",
      "validation: 0:01:16<0:00:15 Epoch 16/10 - Batch 133/160, Loss: 0.0120 0.0059\n",
      "validation: 0:01:17<0:00:14 Epoch 16/10 - Batch 134/160, Loss: 0.0111 0.0065\n",
      "validation: 0:01:17<0:00:14 Epoch 16/10 - Batch 135/160, Loss: 0.0104 0.0065\n",
      "validation: 0:01:17<0:00:13 Epoch 16/10 - Batch 136/160, Loss: 0.0102 0.0084\n",
      "validation: 0:01:17<0:00:12 Epoch 16/10 - Batch 137/160, Loss: 0.0111 0.0180\n",
      "validation: 0:01:17<0:00:12 Epoch 16/10 - Batch 138/160, Loss: 0.0106 0.0069\n",
      "validation: 0:01:17<0:00:11 Epoch 16/10 - Batch 139/160, Loss: 0.0102 0.0063\n",
      "validation: 0:01:17<0:00:11 Epoch 16/10 - Batch 140/160, Loss: 0.0100 0.0072\n",
      "validation: 0:01:17<0:00:10 Epoch 16/10 - Batch 141/160, Loss: 0.0097 0.0064\n",
      "validation: 0:01:17<0:00:09 Epoch 16/10 - Batch 142/160, Loss: 0.0095 0.0065\n",
      "validation: 0:01:17<0:00:09 Epoch 16/10 - Batch 143/160, Loss: 0.0095 0.0103\n",
      "validation: 0:01:17<0:00:08 Epoch 16/10 - Batch 144/160, Loss: 0.0094 0.0070\n",
      "validation: 0:01:17<0:00:08 Epoch 16/10 - Batch 145/160, Loss: 0.0095 0.0111\n",
      "validation: 0:01:17<0:00:07 Epoch 16/10 - Batch 146/160, Loss: 0.0093 0.0067\n",
      "validation: 0:01:17<0:00:06 Epoch 16/10 - Batch 147/160, Loss: 0.0093 0.0086\n",
      "validation: 0:01:17<0:00:06 Epoch 16/10 - Batch 148/160, Loss: 0.0092 0.0069\n",
      "validation: 0:01:17<0:00:05 Epoch 16/10 - Batch 149/160, Loss: 0.0093 0.0125\n",
      "validation: 0:01:18<0:00:05 Epoch 16/10 - Batch 150/160, Loss: 0.0093 0.0078\n",
      "validation: 0:01:18<0:00:04 Epoch 16/10 - Batch 151/160, Loss: 0.0095 0.0156\n",
      "validation: 0:01:18<0:00:04 Epoch 16/10 - Batch 152/160, Loss: 0.0095 0.0094\n",
      "validation: 0:01:18<0:00:03 Epoch 16/10 - Batch 153/160, Loss: 0.0094 0.0071\n",
      "validation: 0:01:18<0:00:03 Epoch 16/10 - Batch 154/160, Loss: 0.0093 0.0069\n",
      "validation: 0:01:18<0:00:02 Epoch 16/10 - Batch 155/160, Loss: 0.0092 0.0057\n",
      "validation: 0:01:18<0:00:02 Epoch 16/10 - Batch 156/160, Loss: 0.0093 0.0108\n",
      "validation: 0:01:18<0:00:01 Epoch 16/10 - Batch 157/160, Loss: 0.0093 0.0090\n",
      "validation: 0:01:18<0:00:00 Epoch 16/10 - Batch 158/160, Loss: 0.0094 0.0126\n",
      "validation: 0:01:18<0:00:00 Epoch 16/10 - Batch 159/160, Loss: 0.0093 0.0071\n",
      "validation: 0:01:18<0:00:00 Epoch 16/10 - Batch 160/160, Loss: 0.0093 0.0085\n",
      "recreating train_snippets_shuffled\n",
      "\n",
      "Validation\n",
      "\n",
      "training: 0:00:00<0:00:29 Epoch 17/10 - Batch 1/160, Loss: 0.0284 0.0284\n",
      "saving searchnet-model-16-0.pt\n",
      "\n",
      "training: 0:00:10<0:14:12 Epoch 17/10 - Batch 2/160, Loss: 0.0282 0.0280\n",
      "training: 0:00:12<0:10:50 Epoch 17/10 - Batch 3/160, Loss: 0.0283 0.0286\n",
      "training: 0:00:21<0:13:46 Epoch 17/10 - Batch 4/160, Loss: 0.0307 0.0361\n",
      "training: 0:00:22<0:11:38 Epoch 17/10 - Batch 5/160, Loss: 0.0322 0.0370\n",
      "training: 0:00:22<0:09:43 Epoch 17/10 - Batch 6/160, Loss: 0.0323 0.0326\n",
      "training: 0:00:22<0:08:20 Epoch 17/10 - Batch 7/160, Loss: 0.0318 0.0300\n",
      "training: 0:00:23<0:07:18 Epoch 17/10 - Batch 8/160, Loss: 0.0329 0.0370\n",
      "training: 0:00:23<0:06:30 Epoch 17/10 - Batch 9/160, Loss: 0.0328 0.0322\n",
      "training: 0:00:23<0:05:52 Epoch 17/10 - Batch 10/160, Loss: 0.0321 0.0297\n",
      "training: 0:00:23<0:05:20 Epoch 17/10 - Batch 11/160, Loss: 0.0331 0.0371\n",
      "training: 0:00:23<0:04:54 Epoch 17/10 - Batch 12/160, Loss: 0.0333 0.0339\n",
      "training: 0:00:24<0:04:31 Epoch 17/10 - Batch 13/160, Loss: 0.0342 0.0376\n",
      "training: 0:00:24<0:04:12 Epoch 17/10 - Batch 14/160, Loss: 0.0344 0.0354\n",
      "training: 0:00:24<0:03:55 Epoch 17/10 - Batch 15/160, Loss: 0.0351 0.0377\n",
      "training: 0:00:24<0:03:41 Epoch 17/10 - Batch 16/160, Loss: 0.0367 0.0433\n",
      "training: 0:00:24<0:03:28 Epoch 17/10 - Batch 17/160, Loss: 0.0360 0.0331\n",
      "training: 0:00:24<0:03:16 Epoch 17/10 - Batch 18/160, Loss: 0.0358 0.0350\n",
      "training: 0:00:25<0:03:06 Epoch 17/10 - Batch 19/160, Loss: 0.0342 0.0277\n",
      "training: 0:00:25<0:02:57 Epoch 17/10 - Batch 20/160, Loss: 0.0347 0.0369\n",
      "training: 0:00:25<0:02:48 Epoch 17/10 - Batch 21/160, Loss: 0.0334 0.0279\n",
      "training: 0:00:25<0:02:41 Epoch 17/10 - Batch 22/160, Loss: 0.0339 0.0362\n",
      "training: 0:00:25<0:02:34 Epoch 17/10 - Batch 23/160, Loss: 0.0358 0.0435\n",
      "training: 0:00:26<0:02:27 Epoch 17/10 - Batch 24/160, Loss: 0.0338 0.0258\n",
      "training: 0:00:26<0:02:21 Epoch 17/10 - Batch 25/160, Loss: 0.0323 0.0261\n",
      "training: 0:00:26<0:02:16 Epoch 17/10 - Batch 26/160, Loss: 0.0324 0.0328\n",
      "training: 0:00:26<0:02:11 Epoch 17/10 - Batch 27/160, Loss: 0.0318 0.0293\n",
      "training: 0:00:26<0:02:06 Epoch 17/10 - Batch 28/160, Loss: 0.0327 0.0362\n",
      "training: 0:00:26<0:02:01 Epoch 17/10 - Batch 29/160, Loss: 0.0325 0.0319\n",
      "training: 0:00:27<0:01:57 Epoch 17/10 - Batch 30/160, Loss: 0.0328 0.0339\n",
      "training: 0:00:27<0:01:53 Epoch 17/10 - Batch 31/160, Loss: 0.0333 0.0356\n",
      "training: 0:00:27<0:01:50 Epoch 17/10 - Batch 32/160, Loss: 0.0352 0.0426\n",
      "training: 0:00:27<0:01:46 Epoch 17/10 - Batch 33/160, Loss: 0.0326 0.0221\n",
      "training: 0:00:27<0:01:43 Epoch 17/10 - Batch 34/160, Loss: 0.0345 0.0420\n",
      "training: 0:00:28<0:01:40 Epoch 17/10 - Batch 35/160, Loss: 0.0318 0.0214\n",
      "training: 0:00:28<0:01:37 Epoch 17/10 - Batch 36/160, Loss: 0.0340 0.0428\n",
      "training: 0:00:28<0:01:34 Epoch 17/10 - Batch 37/160, Loss: 0.0353 0.0403\n",
      "training: 0:00:28<0:01:32 Epoch 17/10 - Batch 38/160, Loss: 0.0347 0.0324\n",
      "training: 0:00:28<0:01:29 Epoch 17/10 - Batch 39/160, Loss: 0.0339 0.0304\n",
      "training: 0:00:29<0:01:27 Epoch 17/10 - Batch 40/160, Loss: 0.0331 0.0302\n",
      "training: 0:00:29<0:01:24 Epoch 17/10 - Batch 41/160, Loss: 0.0333 0.0339\n",
      "training: 0:00:29<0:01:22 Epoch 17/10 - Batch 42/160, Loss: 0.0339 0.0362\n",
      "training: 0:00:29<0:01:20 Epoch 17/10 - Batch 43/160, Loss: 0.0341 0.0351\n",
      "training: 0:00:29<0:01:18 Epoch 17/10 - Batch 44/160, Loss: 0.0344 0.0358\n",
      "training: 0:00:29<0:01:16 Epoch 17/10 - Batch 45/160, Loss: 0.0322 0.0233\n",
      "training: 0:00:30<0:01:14 Epoch 17/10 - Batch 46/160, Loss: 0.0328 0.0352\n",
      "training: 0:00:30<0:01:13 Epoch 17/10 - Batch 47/160, Loss: 0.0329 0.0331\n",
      "training: 0:00:30<0:01:11 Epoch 17/10 - Batch 48/160, Loss: 0.0339 0.0379\n",
      "training: 0:00:30<0:01:09 Epoch 17/10 - Batch 49/160, Loss: 0.0348 0.0387\n",
      "training: 0:00:30<0:01:08 Epoch 17/10 - Batch 50/160, Loss: 0.0353 0.0371\n",
      "training: 0:00:31<0:01:06 Epoch 17/10 - Batch 51/160, Loss: 0.0357 0.0373\n",
      "training: 0:00:31<0:01:05 Epoch 17/10 - Batch 52/160, Loss: 0.0356 0.0354\n",
      "training: 0:00:31<0:01:03 Epoch 17/10 - Batch 53/160, Loss: 0.0360 0.0373\n",
      "training: 0:00:31<0:01:02 Epoch 17/10 - Batch 54/160, Loss: 0.0361 0.0367\n",
      "training: 0:00:31<0:01:00 Epoch 17/10 - Batch 55/160, Loss: 0.0363 0.0371\n",
      "training: 0:00:32<0:00:59 Epoch 17/10 - Batch 56/160, Loss: 0.0371 0.0402\n",
      "training: 0:00:32<0:00:58 Epoch 17/10 - Batch 57/160, Loss: 0.0375 0.0390\n",
      "training: 0:00:32<0:00:57 Epoch 17/10 - Batch 58/160, Loss: 0.0367 0.0334\n",
      "training: 0:00:32<0:00:55 Epoch 17/10 - Batch 59/160, Loss: 0.0376 0.0416\n",
      "training: 0:00:32<0:00:54 Epoch 17/10 - Batch 60/160, Loss: 0.0358 0.0284\n",
      "training: 0:00:32<0:00:53 Epoch 17/10 - Batch 61/160, Loss: 0.0375 0.0442\n",
      "training: 0:00:33<0:00:52 Epoch 17/10 - Batch 62/160, Loss: 0.0362 0.0313\n",
      "training: 0:00:33<0:00:51 Epoch 17/10 - Batch 63/160, Loss: 0.0356 0.0332\n",
      "training: 0:00:33<0:00:50 Epoch 17/10 - Batch 64/160, Loss: 0.0362 0.0386\n",
      "training: 0:00:33<0:00:49 Epoch 17/10 - Batch 65/160, Loss: 0.0376 0.0430\n",
      "training: 0:00:33<0:00:48 Epoch 17/10 - Batch 66/160, Loss: 0.0365 0.0320\n",
      "training: 0:00:34<0:00:47 Epoch 17/10 - Batch 67/160, Loss: 0.0357 0.0328\n",
      "training: 0:00:34<0:00:46 Epoch 17/10 - Batch 68/160, Loss: 0.0346 0.0303\n",
      "training: 0:00:34<0:00:45 Epoch 17/10 - Batch 69/160, Loss: 0.0350 0.0362\n",
      "training: 0:00:34<0:00:44 Epoch 17/10 - Batch 70/160, Loss: 0.0348 0.0342\n",
      "training: 0:00:34<0:00:43 Epoch 17/10 - Batch 71/160, Loss: 0.0351 0.0363\n",
      "training: 0:00:34<0:00:42 Epoch 17/10 - Batch 72/160, Loss: 0.0347 0.0330\n",
      "training: 0:00:35<0:00:41 Epoch 17/10 - Batch 73/160, Loss: 0.0349 0.0358\n",
      "training: 0:00:35<0:00:41 Epoch 17/10 - Batch 74/160, Loss: 0.0341 0.0309\n",
      "training: 0:00:35<0:00:40 Epoch 17/10 - Batch 75/160, Loss: 0.0350 0.0388\n",
      "training: 0:00:35<0:00:39 Epoch 17/10 - Batch 76/160, Loss: 0.0330 0.0247\n",
      "training: 0:00:35<0:00:38 Epoch 17/10 - Batch 77/160, Loss: 0.0330 0.0334\n",
      "training: 0:00:36<0:00:37 Epoch 17/10 - Batch 78/160, Loss: 0.0344 0.0396\n",
      "training: 0:00:36<0:00:37 Epoch 17/10 - Batch 79/160, Loss: 0.0342 0.0337\n",
      "training: 0:00:36<0:00:36 Epoch 17/10 - Batch 80/160, Loss: 0.0344 0.0353\n",
      "training: 0:00:36<0:00:35 Epoch 17/10 - Batch 81/160, Loss: 0.0334 0.0291\n",
      "training: 0:00:36<0:00:35 Epoch 17/10 - Batch 82/160, Loss: 0.0333 0.0329\n",
      "training: 0:00:36<0:00:34 Epoch 17/10 - Batch 83/160, Loss: 0.0333 0.0331\n",
      "training: 0:00:37<0:00:33 Epoch 17/10 - Batch 84/160, Loss: 0.0321 0.0275\n",
      "training: 0:00:37<0:00:32 Epoch 17/10 - Batch 85/160, Loss: 0.0333 0.0381\n",
      "training: 0:00:37<0:00:32 Epoch 17/10 - Batch 86/160, Loss: 0.0335 0.0345\n",
      "training: 0:00:37<0:00:31 Epoch 17/10 - Batch 87/160, Loss: 0.0347 0.0393\n",
      "training: 0:00:37<0:00:31 Epoch 17/10 - Batch 88/160, Loss: 0.0343 0.0326\n",
      "training: 0:00:38<0:00:30 Epoch 17/10 - Batch 89/160, Loss: 0.0340 0.0330\n",
      "training: 0:00:38<0:00:29 Epoch 17/10 - Batch 90/160, Loss: 0.0327 0.0271\n",
      "training: 0:00:38<0:00:29 Epoch 17/10 - Batch 91/160, Loss: 0.0351 0.0449\n",
      "training: 0:00:38<0:00:28 Epoch 17/10 - Batch 92/160, Loss: 0.0342 0.0307\n",
      "training: 0:00:38<0:00:27 Epoch 17/10 - Batch 93/160, Loss: 0.0345 0.0356\n",
      "training: 0:00:39<0:00:27 Epoch 17/10 - Batch 94/160, Loss: 0.0342 0.0332\n",
      "training: 0:00:39<0:00:26 Epoch 17/10 - Batch 95/160, Loss: 0.0352 0.0392\n",
      "training: 0:00:39<0:00:26 Epoch 17/10 - Batch 96/160, Loss: 0.0338 0.0282\n",
      "training: 0:00:39<0:00:25 Epoch 17/10 - Batch 97/160, Loss: 0.0353 0.0414\n",
      "training: 0:00:39<0:00:25 Epoch 17/10 - Batch 98/160, Loss: 0.0352 0.0345\n",
      "training: 0:00:39<0:00:24 Epoch 17/10 - Batch 99/160, Loss: 0.0347 0.0331\n",
      "training: 0:00:40<0:00:24 Epoch 17/10 - Batch 100/160, Loss: 0.0349 0.0354\n",
      "training: 0:00:40<0:00:23 Epoch 17/10 - Batch 101/160, Loss: 0.0347 0.0339\n",
      "saving searchnet-model-16-100.pt\n",
      "\n",
      "training: 0:00:41<0:00:23 Epoch 17/10 - Batch 102/160, Loss: 0.0347 0.0345\n",
      "training: 0:00:41<0:00:23 Epoch 17/10 - Batch 103/160, Loss: 0.0341 0.0317\n",
      "training: 0:00:41<0:00:22 Epoch 17/10 - Batch 104/160, Loss: 0.0350 0.0386\n",
      "training: 0:00:42<0:00:22 Epoch 17/10 - Batch 105/160, Loss: 0.0344 0.0321\n",
      "training: 0:00:42<0:00:21 Epoch 17/10 - Batch 106/160, Loss: 0.0362 0.0436\n",
      "training: 0:00:42<0:00:21 Epoch 17/10 - Batch 107/160, Loss: 0.0356 0.0331\n",
      "training: 0:00:42<0:00:20 Epoch 17/10 - Batch 108/160, Loss: 0.0351 0.0333\n",
      "training: 0:00:42<0:00:20 Epoch 17/10 - Batch 109/160, Loss: 0.0349 0.0338\n",
      "training: 0:00:42<0:00:19 Epoch 17/10 - Batch 110/160, Loss: 0.0336 0.0287\n",
      "training: 0:00:43<0:00:19 Epoch 17/10 - Batch 111/160, Loss: 0.0337 0.0338\n",
      "training: 0:00:43<0:00:18 Epoch 17/10 - Batch 112/160, Loss: 0.0332 0.0314\n",
      "training: 0:00:43<0:00:18 Epoch 17/10 - Batch 113/160, Loss: 0.0342 0.0382\n",
      "training: 0:00:43<0:00:17 Epoch 17/10 - Batch 114/160, Loss: 0.0344 0.0352\n",
      "training: 0:00:43<0:00:17 Epoch 17/10 - Batch 115/160, Loss: 0.0338 0.0314\n",
      "training: 0:00:44<0:00:16 Epoch 17/10 - Batch 116/160, Loss: 0.0322 0.0255\n",
      "training: 0:00:44<0:00:16 Epoch 17/10 - Batch 117/160, Loss: 0.0323 0.0328\n",
      "training: 0:00:44<0:00:15 Epoch 17/10 - Batch 118/160, Loss: 0.0339 0.0402\n",
      "training: 0:00:44<0:00:15 Epoch 17/10 - Batch 119/160, Loss: 0.0339 0.0340\n",
      "training: 0:00:44<0:00:14 Epoch 17/10 - Batch 120/160, Loss: 0.0341 0.0347\n",
      "training: 0:00:44<0:00:14 Epoch 17/10 - Batch 121/160, Loss: 0.0330 0.0290\n",
      "training: 0:00:45<0:00:14 Epoch 17/10 - Batch 122/160, Loss: 0.0334 0.0349\n",
      "training: 0:00:45<0:00:13 Epoch 17/10 - Batch 123/160, Loss: 0.0334 0.0332\n",
      "training: 0:00:45<0:00:13 Epoch 17/10 - Batch 124/160, Loss: 0.0337 0.0350\n",
      "training: 0:00:45<0:00:12 Epoch 17/10 - Batch 125/160, Loss: 0.0337 0.0338\n",
      "training: 0:00:45<0:00:12 Epoch 17/10 - Batch 126/160, Loss: 0.0328 0.0290\n",
      "training: 0:00:46<0:00:11 Epoch 17/10 - Batch 127/160, Loss: 0.0324 0.0308\n",
      "training: 0:00:46<0:00:11 Epoch 17/10 - Batch 128/160, Loss: 0.0328 0.0343\n",
      "saving searchnet-model-16-128.pt\n",
      "\n",
      "\n",
      "Validation:\n",
      "\n",
      "validation: 0:01:18<0:00:18 Epoch 17/10 - Batch 129/160, Loss: 0.0078 0.0078\n",
      "validation: 0:01:18<0:00:18 Epoch 17/10 - Batch 130/160, Loss: 0.0092 0.0105\n",
      "validation: 0:01:18<0:00:17 Epoch 17/10 - Batch 131/160, Loss: 0.0105 0.0133\n",
      "validation: 0:01:18<0:00:16 Epoch 17/10 - Batch 132/160, Loss: 0.0104 0.0099\n",
      "validation: 0:01:18<0:00:15 Epoch 17/10 - Batch 133/160, Loss: 0.0100 0.0083\n",
      "validation: 0:01:18<0:00:15 Epoch 17/10 - Batch 134/160, Loss: 0.0096 0.0079\n",
      "validation: 0:01:18<0:00:14 Epoch 17/10 - Batch 135/160, Loss: 0.0092 0.0068\n",
      "validation: 0:01:18<0:00:13 Epoch 17/10 - Batch 136/160, Loss: 0.0093 0.0096\n",
      "validation: 0:01:18<0:00:13 Epoch 17/10 - Batch 137/160, Loss: 0.0090 0.0067\n",
      "validation: 0:01:18<0:00:12 Epoch 17/10 - Batch 138/160, Loss: 0.0106 0.0248\n",
      "validation: 0:01:18<0:00:11 Epoch 17/10 - Batch 139/160, Loss: 0.0102 0.0070\n",
      "validation: 0:01:18<0:00:11 Epoch 17/10 - Batch 140/160, Loss: 0.0099 0.0062\n",
      "validation: 0:01:18<0:00:10 Epoch 17/10 - Batch 141/160, Loss: 0.0098 0.0092\n",
      "validation: 0:01:18<0:00:09 Epoch 17/10 - Batch 142/160, Loss: 0.0098 0.0088\n",
      "validation: 0:01:18<0:00:09 Epoch 17/10 - Batch 143/160, Loss: 0.0097 0.0085\n",
      "validation: 0:01:18<0:00:08 Epoch 17/10 - Batch 144/160, Loss: 0.0095 0.0061\n",
      "validation: 0:01:18<0:00:08 Epoch 17/10 - Batch 145/160, Loss: 0.0100 0.0184\n",
      "validation: 0:01:18<0:00:07 Epoch 17/10 - Batch 146/160, Loss: 0.0098 0.0067\n",
      "validation: 0:01:19<0:00:06 Epoch 17/10 - Batch 147/160, Loss: 0.0096 0.0064\n",
      "validation: 0:01:19<0:00:06 Epoch 17/10 - Batch 148/160, Loss: 0.0095 0.0072\n",
      "validation: 0:01:19<0:00:05 Epoch 17/10 - Batch 149/160, Loss: 0.0094 0.0064\n",
      "validation: 0:01:19<0:00:05 Epoch 17/10 - Batch 150/160, Loss: 0.0093 0.0088\n",
      "validation: 0:01:19<0:00:04 Epoch 17/10 - Batch 151/160, Loss: 0.0092 0.0063\n",
      "validation: 0:01:19<0:00:04 Epoch 17/10 - Batch 152/160, Loss: 0.0091 0.0076\n",
      "validation: 0:01:19<0:00:03 Epoch 17/10 - Batch 153/160, Loss: 0.0092 0.0100\n",
      "validation: 0:01:19<0:00:03 Epoch 17/10 - Batch 154/160, Loss: 0.0091 0.0082\n",
      "validation: 0:01:19<0:00:02 Epoch 17/10 - Batch 155/160, Loss: 0.0091 0.0089\n",
      "validation: 0:01:19<0:00:02 Epoch 17/10 - Batch 156/160, Loss: 0.0090 0.0065\n",
      "validation: 0:01:19<0:00:01 Epoch 17/10 - Batch 157/160, Loss: 0.0090 0.0069\n",
      "validation: 0:01:19<0:00:01 Epoch 17/10 - Batch 158/160, Loss: 0.0089 0.0066\n",
      "validation: 0:01:19<0:00:00 Epoch 17/10 - Batch 159/160, Loss: 0.0089 0.0089\n",
      "validation: 0:01:19<0:00:00 Epoch 17/10 - Batch 160/160, Loss: 0.0088 0.0075\n",
      "recreating train_snippets_shuffled\n",
      "\n",
      "Validation\n",
      "\n",
      "training: 0:00:00<0:00:29 Epoch 18/10 - Batch 1/160, Loss: 0.0299 0.0299\n",
      "saving searchnet-model-17-0.pt\n",
      "\n",
      "training: 0:00:08<0:11:18 Epoch 18/10 - Batch 2/160, Loss: 0.0292 0.0288\n",
      "training: 0:00:14<0:12:32 Epoch 18/10 - Batch 3/160, Loss: 0.0315 0.0348\n",
      "training: 0:00:20<0:13:15 Epoch 18/10 - Batch 4/160, Loss: 0.0352 0.0438\n",
      "training: 0:00:21<0:10:55 Epoch 18/10 - Batch 5/160, Loss: 0.0340 0.0305\n",
      "training: 0:00:23<0:09:59 Epoch 18/10 - Batch 6/160, Loss: 0.0338 0.0330\n",
      "training: 0:00:24<0:08:46 Epoch 18/10 - Batch 7/160, Loss: 0.0325 0.0279\n",
      "training: 0:00:24<0:07:40 Epoch 18/10 - Batch 8/160, Loss: 0.0329 0.0342\n",
      "training: 0:00:24<0:06:50 Epoch 18/10 - Batch 9/160, Loss: 0.0316 0.0265\n",
      "training: 0:00:24<0:06:09 Epoch 18/10 - Batch 10/160, Loss: 0.0349 0.0480\n",
      "training: 0:00:24<0:05:36 Epoch 18/10 - Batch 11/160, Loss: 0.0351 0.0358\n",
      "training: 0:00:25<0:05:08 Epoch 18/10 - Batch 12/160, Loss: 0.0365 0.0423\n",
      "training: 0:00:25<0:04:44 Epoch 18/10 - Batch 13/160, Loss: 0.0346 0.0269\n",
      "training: 0:00:25<0:04:24 Epoch 18/10 - Batch 14/160, Loss: 0.0340 0.0319\n",
      "training: 0:00:25<0:04:07 Epoch 18/10 - Batch 15/160, Loss: 0.0349 0.0385\n",
      "training: 0:00:25<0:03:51 Epoch 18/10 - Batch 16/160, Loss: 0.0344 0.0320\n",
      "training: 0:00:25<0:03:38 Epoch 18/10 - Batch 17/160, Loss: 0.0329 0.0271\n",
      "training: 0:00:26<0:03:25 Epoch 18/10 - Batch 18/160, Loss: 0.0340 0.0385\n",
      "training: 0:00:26<0:03:15 Epoch 18/10 - Batch 19/160, Loss: 0.0348 0.0378\n",
      "training: 0:00:26<0:03:05 Epoch 18/10 - Batch 20/160, Loss: 0.0337 0.0293\n",
      "training: 0:00:26<0:02:56 Epoch 18/10 - Batch 21/160, Loss: 0.0340 0.0350\n",
      "training: 0:00:26<0:02:48 Epoch 18/10 - Batch 22/160, Loss: 0.0343 0.0355\n",
      "training: 0:00:27<0:02:41 Epoch 18/10 - Batch 23/160, Loss: 0.0341 0.0336\n",
      "training: 0:00:27<0:02:34 Epoch 18/10 - Batch 24/160, Loss: 0.0345 0.0359\n",
      "training: 0:00:27<0:02:27 Epoch 18/10 - Batch 25/160, Loss: 0.0348 0.0362\n",
      "training: 0:00:27<0:02:22 Epoch 18/10 - Batch 26/160, Loss: 0.0358 0.0398\n",
      "training: 0:00:27<0:02:16 Epoch 18/10 - Batch 27/160, Loss: 0.0349 0.0311\n",
      "training: 0:00:27<0:02:11 Epoch 18/10 - Batch 28/160, Loss: 0.0357 0.0390\n",
      "training: 0:00:28<0:02:07 Epoch 18/10 - Batch 29/160, Loss: 0.0346 0.0303\n",
      "training: 0:00:28<0:02:02 Epoch 18/10 - Batch 30/160, Loss: 0.0341 0.0321\n",
      "training: 0:00:28<0:01:58 Epoch 18/10 - Batch 31/160, Loss: 0.0335 0.0310\n",
      "training: 0:00:28<0:01:54 Epoch 18/10 - Batch 32/160, Loss: 0.0339 0.0354\n",
      "training: 0:00:28<0:01:51 Epoch 18/10 - Batch 33/160, Loss: 0.0341 0.0349\n",
      "training: 0:00:29<0:01:47 Epoch 18/10 - Batch 34/160, Loss: 0.0339 0.0333\n",
      "training: 0:00:29<0:01:44 Epoch 18/10 - Batch 35/160, Loss: 0.0359 0.0440\n",
      "training: 0:00:29<0:01:41 Epoch 18/10 - Batch 36/160, Loss: 0.0358 0.0352\n",
      "training: 0:00:29<0:01:38 Epoch 18/10 - Batch 37/160, Loss: 0.0363 0.0384\n",
      "training: 0:00:29<0:01:35 Epoch 18/10 - Batch 38/160, Loss: 0.0370 0.0396\n",
      "training: 0:00:30<0:01:33 Epoch 18/10 - Batch 39/160, Loss: 0.0354 0.0293\n",
      "training: 0:00:30<0:01:30 Epoch 18/10 - Batch 40/160, Loss: 0.0350 0.0331\n",
      "training: 0:00:30<0:01:28 Epoch 18/10 - Batch 41/160, Loss: 0.0354 0.0372\n",
      "training: 0:00:30<0:01:25 Epoch 18/10 - Batch 42/160, Loss: 0.0361 0.0386\n",
      "training: 0:00:30<0:01:23 Epoch 18/10 - Batch 43/160, Loss: 0.0361 0.0364\n",
      "training: 0:00:30<0:01:21 Epoch 18/10 - Batch 44/160, Loss: 0.0347 0.0291\n",
      "training: 0:00:31<0:01:19 Epoch 18/10 - Batch 45/160, Loss: 0.0346 0.0342\n",
      "training: 0:00:31<0:01:17 Epoch 18/10 - Batch 46/160, Loss: 0.0353 0.0379\n",
      "training: 0:00:31<0:01:15 Epoch 18/10 - Batch 47/160, Loss: 0.0353 0.0355\n",
      "training: 0:00:31<0:01:13 Epoch 18/10 - Batch 48/160, Loss: 0.0369 0.0434\n",
      "training: 0:00:31<0:01:12 Epoch 18/10 - Batch 49/160, Loss: 0.0370 0.0373\n",
      "training: 0:00:32<0:01:10 Epoch 18/10 - Batch 50/160, Loss: 0.0352 0.0280\n",
      "training: 0:00:32<0:01:08 Epoch 18/10 - Batch 51/160, Loss: 0.0360 0.0390\n",
      "training: 0:00:32<0:01:07 Epoch 18/10 - Batch 52/160, Loss: 0.0337 0.0245\n",
      "training: 0:00:32<0:01:05 Epoch 18/10 - Batch 53/160, Loss: 0.0345 0.0379\n",
      "training: 0:00:32<0:01:04 Epoch 18/10 - Batch 54/160, Loss: 0.0339 0.0314\n",
      "training: 0:00:32<0:01:02 Epoch 18/10 - Batch 55/160, Loss: 0.0347 0.0377\n",
      "training: 0:00:33<0:01:01 Epoch 18/10 - Batch 56/160, Loss: 0.0327 0.0246\n",
      "training: 0:00:33<0:01:00 Epoch 18/10 - Batch 57/160, Loss: 0.0325 0.0317\n",
      "training: 0:00:33<0:00:58 Epoch 18/10 - Batch 58/160, Loss: 0.0323 0.0315\n",
      "training: 0:00:33<0:00:57 Epoch 18/10 - Batch 59/160, Loss: 0.0310 0.0260\n",
      "training: 0:00:33<0:00:56 Epoch 18/10 - Batch 60/160, Loss: 0.0321 0.0366\n",
      "training: 0:00:34<0:00:55 Epoch 18/10 - Batch 61/160, Loss: 0.0338 0.0404\n",
      "training: 0:00:34<0:00:54 Epoch 18/10 - Batch 62/160, Loss: 0.0345 0.0375\n",
      "training: 0:00:34<0:00:53 Epoch 18/10 - Batch 63/160, Loss: 0.0349 0.0364\n",
      "training: 0:00:34<0:00:51 Epoch 18/10 - Batch 64/160, Loss: 0.0344 0.0321\n",
      "training: 0:00:34<0:00:50 Epoch 18/10 - Batch 65/160, Loss: 0.0340 0.0327\n",
      "training: 0:00:34<0:00:49 Epoch 18/10 - Batch 66/160, Loss: 0.0341 0.0344\n",
      "training: 0:00:35<0:00:48 Epoch 18/10 - Batch 67/160, Loss: 0.0345 0.0364\n",
      "training: 0:00:35<0:00:47 Epoch 18/10 - Batch 68/160, Loss: 0.0349 0.0361\n",
      "training: 0:00:35<0:00:46 Epoch 18/10 - Batch 69/160, Loss: 0.0344 0.0328\n",
      "training: 0:00:35<0:00:45 Epoch 18/10 - Batch 70/160, Loss: 0.0350 0.0370\n",
      "training: 0:00:35<0:00:44 Epoch 18/10 - Batch 71/160, Loss: 0.0342 0.0310\n",
      "training: 0:00:36<0:00:44 Epoch 18/10 - Batch 72/160, Loss: 0.0341 0.0336\n",
      "training: 0:00:36<0:00:43 Epoch 18/10 - Batch 73/160, Loss: 0.0350 0.0390\n",
      "training: 0:00:36<0:00:42 Epoch 18/10 - Batch 74/160, Loss: 0.0343 0.0315\n",
      "training: 0:00:36<0:00:41 Epoch 18/10 - Batch 75/160, Loss: 0.0350 0.0376\n",
      "training: 0:00:36<0:00:40 Epoch 18/10 - Batch 76/160, Loss: 0.0340 0.0302\n",
      "training: 0:00:36<0:00:39 Epoch 18/10 - Batch 77/160, Loss: 0.0337 0.0324\n",
      "training: 0:00:37<0:00:39 Epoch 18/10 - Batch 78/160, Loss: 0.0335 0.0329\n",
      "training: 0:00:37<0:00:38 Epoch 18/10 - Batch 79/160, Loss: 0.0335 0.0332\n",
      "training: 0:00:37<0:00:37 Epoch 18/10 - Batch 80/160, Loss: 0.0348 0.0403\n",
      "training: 0:00:37<0:00:36 Epoch 18/10 - Batch 81/160, Loss: 0.0335 0.0281\n",
      "training: 0:00:37<0:00:36 Epoch 18/10 - Batch 82/160, Loss: 0.0332 0.0319\n",
      "training: 0:00:38<0:00:35 Epoch 18/10 - Batch 83/160, Loss: 0.0338 0.0366\n",
      "training: 0:00:38<0:00:34 Epoch 18/10 - Batch 84/160, Loss: 0.0345 0.0369\n",
      "training: 0:00:38<0:00:33 Epoch 18/10 - Batch 85/160, Loss: 0.0336 0.0303\n",
      "training: 0:00:38<0:00:33 Epoch 18/10 - Batch 86/160, Loss: 0.0341 0.0359\n",
      "training: 0:00:38<0:00:32 Epoch 18/10 - Batch 87/160, Loss: 0.0344 0.0359\n",
      "training: 0:00:38<0:00:31 Epoch 18/10 - Batch 88/160, Loss: 0.0335 0.0297\n",
      "training: 0:00:39<0:00:31 Epoch 18/10 - Batch 89/160, Loss: 0.0332 0.0321\n",
      "training: 0:00:39<0:00:30 Epoch 18/10 - Batch 90/160, Loss: 0.0337 0.0354\n",
      "training: 0:00:39<0:00:29 Epoch 18/10 - Batch 91/160, Loss: 0.0320 0.0255\n",
      "training: 0:00:39<0:00:29 Epoch 18/10 - Batch 92/160, Loss: 0.0331 0.0372\n",
      "training: 0:00:39<0:00:28 Epoch 18/10 - Batch 93/160, Loss: 0.0316 0.0257\n",
      "training: 0:00:40<0:00:28 Epoch 18/10 - Batch 94/160, Loss: 0.0322 0.0348\n",
      "training: 0:00:40<0:00:27 Epoch 18/10 - Batch 95/160, Loss: 0.0334 0.0379\n",
      "training: 0:00:40<0:00:26 Epoch 18/10 - Batch 96/160, Loss: 0.0329 0.0309\n",
      "training: 0:00:40<0:00:26 Epoch 18/10 - Batch 97/160, Loss: 0.0324 0.0305\n",
      "training: 0:00:40<0:00:25 Epoch 18/10 - Batch 98/160, Loss: 0.0334 0.0373\n",
      "training: 0:00:40<0:00:25 Epoch 18/10 - Batch 99/160, Loss: 0.0333 0.0329\n",
      "training: 0:00:41<0:00:24 Epoch 18/10 - Batch 100/160, Loss: 0.0343 0.0381\n",
      "training: 0:00:41<0:00:24 Epoch 18/10 - Batch 101/160, Loss: 0.0343 0.0347\n",
      "saving searchnet-model-17-100.pt\n",
      "\n",
      "training: 0:00:42<0:00:24 Epoch 18/10 - Batch 102/160, Loss: 0.0345 0.0350\n",
      "training: 0:00:42<0:00:23 Epoch 18/10 - Batch 103/160, Loss: 0.0356 0.0401\n",
      "training: 0:00:42<0:00:23 Epoch 18/10 - Batch 104/160, Loss: 0.0353 0.0343\n",
      "training: 0:00:43<0:00:22 Epoch 18/10 - Batch 105/160, Loss: 0.0353 0.0353\n",
      "training: 0:00:43<0:00:22 Epoch 18/10 - Batch 106/160, Loss: 0.0353 0.0350\n",
      "training: 0:00:43<0:00:21 Epoch 18/10 - Batch 107/160, Loss: 0.0335 0.0262\n",
      "training: 0:00:43<0:00:21 Epoch 18/10 - Batch 108/160, Loss: 0.0339 0.0359\n",
      "training: 0:00:43<0:00:20 Epoch 18/10 - Batch 109/160, Loss: 0.0342 0.0354\n",
      "training: 0:00:44<0:00:20 Epoch 18/10 - Batch 110/160, Loss: 0.0363 0.0446\n",
      "training: 0:00:44<0:00:19 Epoch 18/10 - Batch 111/160, Loss: 0.0360 0.0349\n",
      "training: 0:00:44<0:00:19 Epoch 18/10 - Batch 112/160, Loss: 0.0363 0.0373\n",
      "training: 0:00:44<0:00:18 Epoch 18/10 - Batch 113/160, Loss: 0.0352 0.0311\n",
      "training: 0:00:44<0:00:18 Epoch 18/10 - Batch 114/160, Loss: 0.0340 0.0290\n",
      "training: 0:00:44<0:00:17 Epoch 18/10 - Batch 115/160, Loss: 0.0334 0.0312\n",
      "training: 0:00:45<0:00:17 Epoch 18/10 - Batch 116/160, Loss: 0.0330 0.0311\n",
      "training: 0:00:45<0:00:16 Epoch 18/10 - Batch 117/160, Loss: 0.0331 0.0337\n",
      "training: 0:00:45<0:00:16 Epoch 18/10 - Batch 118/160, Loss: 0.0334 0.0346\n",
      "training: 0:00:45<0:00:15 Epoch 18/10 - Batch 119/160, Loss: 0.0334 0.0332\n",
      "training: 0:00:45<0:00:15 Epoch 18/10 - Batch 120/160, Loss: 0.0343 0.0378\n",
      "training: 0:00:46<0:00:14 Epoch 18/10 - Batch 121/160, Loss: 0.0358 0.0420\n",
      "training: 0:00:46<0:00:14 Epoch 18/10 - Batch 122/160, Loss: 0.0351 0.0322\n",
      "training: 0:00:46<0:00:13 Epoch 18/10 - Batch 123/160, Loss: 0.0344 0.0318\n",
      "training: 0:00:46<0:00:13 Epoch 18/10 - Batch 124/160, Loss: 0.0335 0.0299\n",
      "training: 0:00:46<0:00:13 Epoch 18/10 - Batch 125/160, Loss: 0.0332 0.0318\n",
      "training: 0:00:46<0:00:12 Epoch 18/10 - Batch 126/160, Loss: 0.0341 0.0378\n",
      "training: 0:00:47<0:00:12 Epoch 18/10 - Batch 127/160, Loss: 0.0349 0.0382\n",
      "training: 0:00:47<0:00:11 Epoch 18/10 - Batch 128/160, Loss: 0.0340 0.0303\n",
      "saving searchnet-model-17-128.pt\n",
      "\n",
      "\n",
      "Validation:\n",
      "\n",
      "validation: 0:01:08<0:00:16 Epoch 18/10 - Batch 129/160, Loss: 0.0073 0.0073\n",
      "validation: 0:01:11<0:00:16 Epoch 18/10 - Batch 130/160, Loss: 0.0068 0.0062\n",
      "validation: 0:01:11<0:00:15 Epoch 18/10 - Batch 131/160, Loss: 0.0089 0.0133\n",
      "validation: 0:01:18<0:00:16 Epoch 18/10 - Batch 132/160, Loss: 0.0083 0.0065\n",
      "validation: 0:01:19<0:00:16 Epoch 18/10 - Batch 133/160, Loss: 0.0081 0.0071\n",
      "validation: 0:01:19<0:00:15 Epoch 18/10 - Batch 134/160, Loss: 0.0085 0.0104\n",
      "validation: 0:01:19<0:00:14 Epoch 18/10 - Batch 135/160, Loss: 0.0090 0.0123\n",
      "validation: 0:01:19<0:00:13 Epoch 18/10 - Batch 136/160, Loss: 0.0089 0.0082\n",
      "validation: 0:01:19<0:00:13 Epoch 18/10 - Batch 137/160, Loss: 0.0087 0.0072\n",
      "validation: 0:01:19<0:00:12 Epoch 18/10 - Batch 138/160, Loss: 0.0090 0.0112\n",
      "validation: 0:01:19<0:00:11 Epoch 18/10 - Batch 139/160, Loss: 0.0090 0.0095\n",
      "validation: 0:01:19<0:00:11 Epoch 18/10 - Batch 140/160, Loss: 0.0088 0.0065\n",
      "validation: 0:01:19<0:00:10 Epoch 18/10 - Batch 141/160, Loss: 0.0087 0.0071\n",
      "validation: 0:01:19<0:00:10 Epoch 18/10 - Batch 142/160, Loss: 0.0086 0.0070\n",
      "validation: 0:01:19<0:00:09 Epoch 18/10 - Batch 143/160, Loss: 0.0084 0.0069\n",
      "validation: 0:01:19<0:00:08 Epoch 18/10 - Batch 144/160, Loss: 0.0083 0.0064\n",
      "validation: 0:01:19<0:00:08 Epoch 18/10 - Batch 145/160, Loss: 0.0082 0.0065\n",
      "validation: 0:01:19<0:00:07 Epoch 18/10 - Batch 146/160, Loss: 0.0081 0.0064\n",
      "validation: 0:01:19<0:00:07 Epoch 18/10 - Batch 147/160, Loss: 0.0081 0.0081\n",
      "validation: 0:01:19<0:00:06 Epoch 18/10 - Batch 148/160, Loss: 0.0080 0.0061\n",
      "validation: 0:01:19<0:00:05 Epoch 18/10 - Batch 149/160, Loss: 0.0080 0.0075\n",
      "validation: 0:01:20<0:00:05 Epoch 18/10 - Batch 150/160, Loss: 0.0084 0.0165\n",
      "validation: 0:01:20<0:00:04 Epoch 18/10 - Batch 151/160, Loss: 0.0086 0.0147\n",
      "validation: 0:01:20<0:00:04 Epoch 18/10 - Batch 152/160, Loss: 0.0089 0.0158\n",
      "validation: 0:01:20<0:00:03 Epoch 18/10 - Batch 153/160, Loss: 0.0091 0.0122\n",
      "validation: 0:01:20<0:00:03 Epoch 18/10 - Batch 154/160, Loss: 0.0090 0.0069\n",
      "validation: 0:01:20<0:00:02 Epoch 18/10 - Batch 155/160, Loss: 0.0090 0.0086\n",
      "validation: 0:01:20<0:00:02 Epoch 18/10 - Batch 156/160, Loss: 0.0089 0.0062\n",
      "validation: 0:01:20<0:00:01 Epoch 18/10 - Batch 157/160, Loss: 0.0088 0.0057\n",
      "validation: 0:01:20<0:00:01 Epoch 18/10 - Batch 158/160, Loss: 0.0088 0.0093\n",
      "validation: 0:01:20<0:00:00 Epoch 18/10 - Batch 159/160, Loss: 0.0087 0.0063\n",
      "validation: 0:01:20<0:00:00 Epoch 18/10 - Batch 160/160, Loss: 0.0086 0.0062\n",
      "recreating train_snippets_shuffled\n",
      "\n",
      "Validation\n",
      "\n",
      "training: 0:00:00<0:00:30 Epoch 19/10 - Batch 1/160, Loss: 0.0303 0.0303\n",
      "saving searchnet-model-18-0.pt\n",
      "\n",
      "training: 0:00:07<0:10:01 Epoch 19/10 - Batch 2/160, Loss: 0.0399 0.0464\n",
      "training: 0:00:20<0:18:09 Epoch 19/10 - Batch 3/160, Loss: 0.0380 0.0351\n",
      "training: 0:00:21<0:13:39 Epoch 19/10 - Batch 4/160, Loss: 0.0337 0.0238\n",
      "training: 0:00:21<0:10:56 Epoch 19/10 - Batch 5/160, Loss: 0.0334 0.0324\n",
      "training: 0:00:21<0:09:08 Epoch 19/10 - Batch 6/160, Loss: 0.0330 0.0317\n",
      "training: 0:00:21<0:07:51 Epoch 19/10 - Batch 7/160, Loss: 0.0347 0.0408\n",
      "training: 0:00:21<0:06:53 Epoch 19/10 - Batch 8/160, Loss: 0.0365 0.0436\n",
      "training: 0:00:21<0:06:07 Epoch 19/10 - Batch 9/160, Loss: 0.0361 0.0347\n",
      "training: 0:00:22<0:05:31 Epoch 19/10 - Batch 10/160, Loss: 0.0356 0.0333\n",
      "training: 0:00:22<0:05:01 Epoch 19/10 - Batch 11/160, Loss: 0.0355 0.0351\n",
      "training: 0:00:22<0:04:37 Epoch 19/10 - Batch 12/160, Loss: 0.0341 0.0284\n",
      "training: 0:00:22<0:04:16 Epoch 19/10 - Batch 13/160, Loss: 0.0325 0.0260\n",
      "training: 0:00:22<0:03:58 Epoch 19/10 - Batch 14/160, Loss: 0.0326 0.0332\n",
      "training: 0:00:23<0:03:42 Epoch 19/10 - Batch 15/160, Loss: 0.0347 0.0433\n",
      "training: 0:00:23<0:03:28 Epoch 19/10 - Batch 16/160, Loss: 0.0332 0.0271\n",
      "training: 0:00:23<0:03:16 Epoch 19/10 - Batch 17/160, Loss: 0.0350 0.0420\n",
      "training: 0:00:23<0:03:06 Epoch 19/10 - Batch 18/160, Loss: 0.0341 0.0304\n",
      "training: 0:00:23<0:02:56 Epoch 19/10 - Batch 19/160, Loss: 0.0329 0.0285\n",
      "training: 0:00:23<0:02:47 Epoch 19/10 - Batch 20/160, Loss: 0.0323 0.0295\n",
      "training: 0:00:24<0:02:39 Epoch 19/10 - Batch 21/160, Loss: 0.0317 0.0293\n",
      "training: 0:00:24<0:02:32 Epoch 19/10 - Batch 22/160, Loss: 0.0324 0.0352\n",
      "training: 0:00:24<0:02:26 Epoch 19/10 - Batch 23/160, Loss: 0.0344 0.0428\n",
      "training: 0:00:24<0:02:20 Epoch 19/10 - Batch 24/160, Loss: 0.0359 0.0419\n",
      "training: 0:00:24<0:02:14 Epoch 19/10 - Batch 25/160, Loss: 0.0366 0.0390\n",
      "training: 0:00:25<0:02:09 Epoch 19/10 - Batch 26/160, Loss: 0.0350 0.0289\n",
      "training: 0:00:25<0:02:04 Epoch 19/10 - Batch 27/160, Loss: 0.0349 0.0346\n",
      "training: 0:00:25<0:01:59 Epoch 19/10 - Batch 28/160, Loss: 0.0351 0.0359\n",
      "training: 0:00:25<0:01:55 Epoch 19/10 - Batch 29/160, Loss: 0.0344 0.0312\n",
      "training: 0:00:25<0:01:51 Epoch 19/10 - Batch 30/160, Loss: 0.0356 0.0405\n",
      "training: 0:00:25<0:01:48 Epoch 19/10 - Batch 31/160, Loss: 0.0334 0.0245\n",
      "training: 0:00:26<0:01:44 Epoch 19/10 - Batch 32/160, Loss: 0.0339 0.0359\n",
      "training: 0:00:26<0:01:41 Epoch 19/10 - Batch 33/160, Loss: 0.0326 0.0275\n",
      "training: 0:00:26<0:01:38 Epoch 19/10 - Batch 34/160, Loss: 0.0324 0.0317\n",
      "training: 0:00:26<0:01:35 Epoch 19/10 - Batch 35/160, Loss: 0.0330 0.0352\n",
      "training: 0:00:26<0:01:32 Epoch 19/10 - Batch 36/160, Loss: 0.0335 0.0357\n",
      "training: 0:00:27<0:01:30 Epoch 19/10 - Batch 37/160, Loss: 0.0342 0.0371\n",
      "training: 0:00:27<0:01:27 Epoch 19/10 - Batch 38/160, Loss: 0.0345 0.0354\n",
      "training: 0:00:27<0:01:25 Epoch 19/10 - Batch 39/160, Loss: 0.0335 0.0298\n",
      "training: 0:00:27<0:01:22 Epoch 19/10 - Batch 40/160, Loss: 0.0331 0.0314\n",
      "training: 0:00:27<0:01:20 Epoch 19/10 - Batch 41/160, Loss: 0.0331 0.0329\n",
      "training: 0:00:28<0:01:18 Epoch 19/10 - Batch 42/160, Loss: 0.0338 0.0368\n",
      "training: 0:00:28<0:01:16 Epoch 19/10 - Batch 43/160, Loss: 0.0340 0.0346\n",
      "training: 0:00:28<0:01:14 Epoch 19/10 - Batch 44/160, Loss: 0.0345 0.0366\n",
      "training: 0:00:28<0:01:13 Epoch 19/10 - Batch 45/160, Loss: 0.0346 0.0351\n",
      "training: 0:00:28<0:01:11 Epoch 19/10 - Batch 46/160, Loss: 0.0335 0.0291\n",
      "training: 0:00:28<0:01:09 Epoch 19/10 - Batch 47/160, Loss: 0.0341 0.0364\n",
      "training: 0:00:29<0:01:07 Epoch 19/10 - Batch 48/160, Loss: 0.0338 0.0328\n",
      "training: 0:00:29<0:01:06 Epoch 19/10 - Batch 49/160, Loss: 0.0342 0.0359\n",
      "training: 0:00:29<0:01:04 Epoch 19/10 - Batch 50/160, Loss: 0.0354 0.0401\n",
      "training: 0:00:29<0:01:03 Epoch 19/10 - Batch 51/160, Loss: 0.0349 0.0329\n",
      "training: 0:00:29<0:01:02 Epoch 19/10 - Batch 52/160, Loss: 0.0345 0.0330\n",
      "training: 0:00:30<0:01:00 Epoch 19/10 - Batch 53/160, Loss: 0.0331 0.0274\n",
      "training: 0:00:30<0:00:59 Epoch 19/10 - Batch 54/160, Loss: 0.0325 0.0300\n",
      "training: 0:00:30<0:00:58 Epoch 19/10 - Batch 55/160, Loss: 0.0316 0.0281\n",
      "training: 0:00:30<0:00:56 Epoch 19/10 - Batch 56/160, Loss: 0.0321 0.0341\n",
      "training: 0:00:30<0:00:55 Epoch 19/10 - Batch 57/160, Loss: 0.0320 0.0318\n",
      "training: 0:00:30<0:00:54 Epoch 19/10 - Batch 58/160, Loss: 0.0312 0.0281\n",
      "training: 0:00:31<0:00:53 Epoch 19/10 - Batch 59/160, Loss: 0.0307 0.0284\n",
      "training: 0:00:31<0:00:52 Epoch 19/10 - Batch 60/160, Loss: 0.0317 0.0359\n",
      "training: 0:00:31<0:00:51 Epoch 19/10 - Batch 61/160, Loss: 0.0320 0.0331\n",
      "training: 0:00:31<0:00:50 Epoch 19/10 - Batch 62/160, Loss: 0.0317 0.0303\n",
      "training: 0:00:31<0:00:49 Epoch 19/10 - Batch 63/160, Loss: 0.0325 0.0357\n",
      "training: 0:00:32<0:00:48 Epoch 19/10 - Batch 64/160, Loss: 0.0339 0.0398\n",
      "training: 0:00:32<0:00:47 Epoch 19/10 - Batch 65/160, Loss: 0.0331 0.0298\n",
      "training: 0:00:32<0:00:46 Epoch 19/10 - Batch 66/160, Loss: 0.0320 0.0273\n",
      "training: 0:00:32<0:00:45 Epoch 19/10 - Batch 67/160, Loss: 0.0313 0.0286\n",
      "training: 0:00:32<0:00:44 Epoch 19/10 - Batch 68/160, Loss: 0.0328 0.0390\n",
      "training: 0:00:32<0:00:43 Epoch 19/10 - Batch 69/160, Loss: 0.0328 0.0325\n",
      "training: 0:00:33<0:00:42 Epoch 19/10 - Batch 70/160, Loss: 0.0336 0.0370\n",
      "training: 0:00:33<0:00:41 Epoch 19/10 - Batch 71/160, Loss: 0.0328 0.0296\n",
      "training: 0:00:33<0:00:40 Epoch 19/10 - Batch 72/160, Loss: 0.0329 0.0332\n",
      "training: 0:00:33<0:00:40 Epoch 19/10 - Batch 73/160, Loss: 0.0337 0.0368\n",
      "training: 0:00:33<0:00:39 Epoch 19/10 - Batch 74/160, Loss: 0.0338 0.0343\n",
      "training: 0:00:34<0:00:38 Epoch 19/10 - Batch 75/160, Loss: 0.0345 0.0375\n",
      "training: 0:00:34<0:00:37 Epoch 19/10 - Batch 76/160, Loss: 0.0352 0.0379\n",
      "training: 0:00:34<0:00:37 Epoch 19/10 - Batch 77/160, Loss: 0.0332 0.0251\n",
      "training: 0:00:34<0:00:36 Epoch 19/10 - Batch 78/160, Loss: 0.0327 0.0306\n",
      "training: 0:00:34<0:00:35 Epoch 19/10 - Batch 79/160, Loss: 0.0332 0.0355\n",
      "training: 0:00:34<0:00:34 Epoch 19/10 - Batch 80/160, Loss: 0.0336 0.0348\n",
      "training: 0:00:35<0:00:34 Epoch 19/10 - Batch 81/160, Loss: 0.0338 0.0346\n",
      "training: 0:00:35<0:00:33 Epoch 19/10 - Batch 82/160, Loss: 0.0331 0.0306\n",
      "training: 0:00:35<0:00:32 Epoch 19/10 - Batch 83/160, Loss: 0.0334 0.0346\n",
      "training: 0:00:35<0:00:32 Epoch 19/10 - Batch 84/160, Loss: 0.0340 0.0365\n",
      "training: 0:00:35<0:00:31 Epoch 19/10 - Batch 85/160, Loss: 0.0340 0.0338\n",
      "training: 0:00:36<0:00:31 Epoch 19/10 - Batch 86/160, Loss: 0.0342 0.0349\n",
      "training: 0:00:36<0:00:30 Epoch 19/10 - Batch 87/160, Loss: 0.0325 0.0258\n",
      "training: 0:00:36<0:00:29 Epoch 19/10 - Batch 88/160, Loss: 0.0332 0.0359\n",
      "training: 0:00:36<0:00:29 Epoch 19/10 - Batch 89/160, Loss: 0.0324 0.0292\n",
      "training: 0:00:36<0:00:28 Epoch 19/10 - Batch 90/160, Loss: 0.0324 0.0324\n",
      "training: 0:00:36<0:00:28 Epoch 19/10 - Batch 91/160, Loss: 0.0318 0.0293\n",
      "training: 0:00:37<0:00:27 Epoch 19/10 - Batch 92/160, Loss: 0.0316 0.0307\n",
      "training: 0:00:37<0:00:26 Epoch 19/10 - Batch 93/160, Loss: 0.0317 0.0323\n",
      "training: 0:00:37<0:00:26 Epoch 19/10 - Batch 94/160, Loss: 0.0324 0.0351\n",
      "training: 0:00:37<0:00:25 Epoch 19/10 - Batch 95/160, Loss: 0.0324 0.0326\n",
      "training: 0:00:37<0:00:25 Epoch 19/10 - Batch 96/160, Loss: 0.0333 0.0369\n",
      "training: 0:00:38<0:00:24 Epoch 19/10 - Batch 97/160, Loss: 0.0327 0.0302\n",
      "training: 0:00:38<0:00:24 Epoch 19/10 - Batch 98/160, Loss: 0.0318 0.0281\n",
      "training: 0:00:38<0:00:23 Epoch 19/10 - Batch 99/160, Loss: 0.0336 0.0408\n",
      "training: 0:00:38<0:00:23 Epoch 19/10 - Batch 100/160, Loss: 0.0320 0.0258\n",
      "training: 0:00:38<0:00:22 Epoch 19/10 - Batch 101/160, Loss: 0.0327 0.0353\n",
      "saving searchnet-model-18-100.pt\n",
      "\n",
      "training: 0:00:41<0:00:23 Epoch 19/10 - Batch 102/160, Loss: 0.0330 0.0342\n",
      "training: 0:00:41<0:00:23 Epoch 19/10 - Batch 103/160, Loss: 0.0328 0.0323\n",
      "training: 0:00:41<0:00:22 Epoch 19/10 - Batch 104/160, Loss: 0.0338 0.0377\n",
      "training: 0:00:42<0:00:22 Epoch 19/10 - Batch 105/160, Loss: 0.0315 0.0224\n",
      "training: 0:00:42<0:00:21 Epoch 19/10 - Batch 106/160, Loss: 0.0315 0.0314\n",
      "training: 0:00:42<0:00:21 Epoch 19/10 - Batch 107/160, Loss: 0.0327 0.0375\n",
      "training: 0:00:42<0:00:20 Epoch 19/10 - Batch 108/160, Loss: 0.0332 0.0352\n",
      "training: 0:00:42<0:00:20 Epoch 19/10 - Batch 109/160, Loss: 0.0335 0.0345\n",
      "training: 0:00:42<0:00:19 Epoch 19/10 - Batch 110/160, Loss: 0.0312 0.0222\n",
      "training: 0:00:43<0:00:19 Epoch 19/10 - Batch 111/160, Loss: 0.0310 0.0303\n",
      "training: 0:00:43<0:00:18 Epoch 19/10 - Batch 112/160, Loss: 0.0309 0.0303\n",
      "training: 0:00:43<0:00:18 Epoch 19/10 - Batch 113/160, Loss: 0.0304 0.0284\n",
      "training: 0:00:43<0:00:17 Epoch 19/10 - Batch 114/160, Loss: 0.0309 0.0330\n",
      "training: 0:00:43<0:00:17 Epoch 19/10 - Batch 115/160, Loss: 0.0307 0.0298\n",
      "training: 0:00:44<0:00:16 Epoch 19/10 - Batch 116/160, Loss: 0.0318 0.0364\n",
      "training: 0:00:44<0:00:16 Epoch 19/10 - Batch 117/160, Loss: 0.0317 0.0314\n",
      "training: 0:00:44<0:00:15 Epoch 19/10 - Batch 118/160, Loss: 0.0321 0.0335\n",
      "training: 0:00:44<0:00:15 Epoch 19/10 - Batch 119/160, Loss: 0.0312 0.0278\n",
      "training: 0:00:44<0:00:14 Epoch 19/10 - Batch 120/160, Loss: 0.0338 0.0438\n",
      "training: 0:00:46<0:00:14 Epoch 19/10 - Batch 121/160, Loss: 0.0341 0.0354\n",
      "training: 0:00:47<0:00:14 Epoch 19/10 - Batch 122/160, Loss: 0.0334 0.0305\n",
      "training: 0:00:55<0:00:16 Epoch 19/10 - Batch 123/160, Loss: 0.0344 0.0388\n",
      "training: 0:00:57<0:00:16 Epoch 19/10 - Batch 124/160, Loss: 0.0337 0.0306\n",
      "training: 0:00:58<0:00:16 Epoch 19/10 - Batch 125/160, Loss: 0.0339 0.0347\n",
      "training: 0:00:58<0:00:15 Epoch 19/10 - Batch 126/160, Loss: 0.0333 0.0312\n",
      "training: 0:00:58<0:00:15 Epoch 19/10 - Batch 127/160, Loss: 0.0338 0.0355\n",
      "training: 0:00:58<0:00:14 Epoch 19/10 - Batch 128/160, Loss: 0.0325 0.0273\n",
      "saving searchnet-model-18-128.pt\n",
      "\n",
      "\n",
      "Validation:\n",
      "\n",
      "validation: 0:01:10<0:00:16 Epoch 19/10 - Batch 129/160, Loss: 0.0061 0.0061\n",
      "validation: 0:01:11<0:00:16 Epoch 19/10 - Batch 130/160, Loss: 0.0063 0.0065\n",
      "validation: 0:01:13<0:00:16 Epoch 19/10 - Batch 131/160, Loss: 0.0063 0.0065\n",
      "validation: 0:01:15<0:00:16 Epoch 19/10 - Batch 132/160, Loss: 0.0072 0.0099\n",
      "validation: 0:01:17<0:00:15 Epoch 19/10 - Batch 133/160, Loss: 0.0071 0.0064\n",
      "validation: 0:01:18<0:00:15 Epoch 19/10 - Batch 134/160, Loss: 0.0072 0.0078\n",
      "validation: 0:01:18<0:00:14 Epoch 19/10 - Batch 135/160, Loss: 0.0071 0.0064\n",
      "validation: 0:01:18<0:00:13 Epoch 19/10 - Batch 136/160, Loss: 0.0072 0.0085\n",
      "validation: 0:01:18<0:00:13 Epoch 19/10 - Batch 137/160, Loss: 0.0073 0.0079\n",
      "validation: 0:01:18<0:00:12 Epoch 19/10 - Batch 138/160, Loss: 0.0072 0.0065\n",
      "validation: 0:01:18<0:00:11 Epoch 19/10 - Batch 139/160, Loss: 0.0071 0.0061\n",
      "validation: 0:01:18<0:00:11 Epoch 19/10 - Batch 140/160, Loss: 0.0074 0.0100\n",
      "validation: 0:01:18<0:00:10 Epoch 19/10 - Batch 141/160, Loss: 0.0077 0.0116\n",
      "validation: 0:01:18<0:00:09 Epoch 19/10 - Batch 142/160, Loss: 0.0077 0.0075\n",
      "validation: 0:01:18<0:00:09 Epoch 19/10 - Batch 143/160, Loss: 0.0081 0.0134\n",
      "validation: 0:01:19<0:00:08 Epoch 19/10 - Batch 144/160, Loss: 0.0081 0.0083\n",
      "validation: 0:01:19<0:00:08 Epoch 19/10 - Batch 145/160, Loss: 0.0080 0.0075\n",
      "validation: 0:01:19<0:00:07 Epoch 19/10 - Batch 146/160, Loss: 0.0082 0.0107\n",
      "validation: 0:01:19<0:00:07 Epoch 19/10 - Batch 147/160, Loss: 0.0082 0.0087\n",
      "validation: 0:01:19<0:00:06 Epoch 19/10 - Batch 148/160, Loss: 0.0083 0.0102\n",
      "validation: 0:01:19<0:00:05 Epoch 19/10 - Batch 149/160, Loss: 0.0091 0.0254\n",
      "validation: 0:01:19<0:00:05 Epoch 19/10 - Batch 150/160, Loss: 0.0091 0.0081\n",
      "validation: 0:01:19<0:00:04 Epoch 19/10 - Batch 151/160, Loss: 0.0090 0.0060\n",
      "validation: 0:01:19<0:00:04 Epoch 19/10 - Batch 152/160, Loss: 0.0088 0.0062\n",
      "validation: 0:01:19<0:00:03 Epoch 19/10 - Batch 153/160, Loss: 0.0088 0.0078\n",
      "validation: 0:01:19<0:00:03 Epoch 19/10 - Batch 154/160, Loss: 0.0087 0.0062\n",
      "validation: 0:01:19<0:00:02 Epoch 19/10 - Batch 155/160, Loss: 0.0087 0.0085\n",
      "validation: 0:01:19<0:00:02 Epoch 19/10 - Batch 156/160, Loss: 0.0086 0.0059\n",
      "validation: 0:01:19<0:00:01 Epoch 19/10 - Batch 157/160, Loss: 0.0085 0.0063\n",
      "validation: 0:01:19<0:00:01 Epoch 19/10 - Batch 158/160, Loss: 0.0085 0.0082\n",
      "validation: 0:01:19<0:00:00 Epoch 19/10 - Batch 159/160, Loss: 0.0085 0.0099\n",
      "validation: 0:01:19<0:00:00 Epoch 19/10 - Batch 160/160, Loss: 0.0087 0.0137\n",
      "recreating train_snippets_shuffled\n",
      "\n",
      "Validation\n",
      "\n",
      "training: 0:00:00<0:00:31 Epoch 20/10 - Batch 1/160, Loss: 0.0253 0.0253\n",
      "saving searchnet-model-19-0.pt\n",
      "\n",
      "training: 0:00:02<0:03:51 Epoch 20/10 - Batch 2/160, Loss: 0.0328 0.0378\n",
      "training: 0:00:06<0:05:42 Epoch 20/10 - Batch 3/160, Loss: 0.0334 0.0342\n",
      "training: 0:00:12<0:07:54 Epoch 20/10 - Batch 4/160, Loss: 0.0329 0.0317\n",
      "training: 0:00:17<0:08:52 Epoch 20/10 - Batch 5/160, Loss: 0.0325 0.0314\n",
      "training: 0:00:22<0:09:39 Epoch 20/10 - Batch 6/160, Loss: 0.0313 0.0270\n",
      "training: 0:00:22<0:08:17 Epoch 20/10 - Batch 7/160, Loss: 0.0323 0.0362\n",
      "training: 0:00:22<0:07:15 Epoch 20/10 - Batch 8/160, Loss: 0.0322 0.0318\n",
      "training: 0:00:23<0:06:27 Epoch 20/10 - Batch 9/160, Loss: 0.0312 0.0271\n",
      "training: 0:00:23<0:05:49 Epoch 20/10 - Batch 10/160, Loss: 0.0324 0.0373\n",
      "training: 0:00:23<0:05:17 Epoch 20/10 - Batch 11/160, Loss: 0.0312 0.0266\n",
      "training: 0:00:23<0:04:51 Epoch 20/10 - Batch 12/160, Loss: 0.0324 0.0368\n",
      "training: 0:00:23<0:04:29 Epoch 20/10 - Batch 13/160, Loss: 0.0318 0.0295\n",
      "training: 0:00:24<0:04:10 Epoch 20/10 - Batch 14/160, Loss: 0.0329 0.0374\n",
      "training: 0:00:24<0:03:53 Epoch 20/10 - Batch 15/160, Loss: 0.0319 0.0277\n",
      "training: 0:00:24<0:03:39 Epoch 20/10 - Batch 16/160, Loss: 0.0328 0.0367\n",
      "training: 0:00:24<0:03:26 Epoch 20/10 - Batch 17/160, Loss: 0.0324 0.0305\n",
      "training: 0:00:24<0:03:15 Epoch 20/10 - Batch 18/160, Loss: 0.0333 0.0368\n",
      "training: 0:00:24<0:03:04 Epoch 20/10 - Batch 19/160, Loss: 0.0323 0.0285\n",
      "training: 0:00:25<0:02:55 Epoch 20/10 - Batch 20/160, Loss: 0.0338 0.0399\n",
      "training: 0:00:25<0:02:47 Epoch 20/10 - Batch 21/160, Loss: 0.0333 0.0310\n",
      "training: 0:00:25<0:02:39 Epoch 20/10 - Batch 22/160, Loss: 0.0336 0.0351\n",
      "training: 0:00:25<0:02:32 Epoch 20/10 - Batch 23/160, Loss: 0.0329 0.0298\n",
      "training: 0:00:25<0:02:26 Epoch 20/10 - Batch 24/160, Loss: 0.0328 0.0327\n",
      "training: 0:00:26<0:02:20 Epoch 20/10 - Batch 25/160, Loss: 0.0330 0.0335\n",
      "training: 0:00:26<0:02:14 Epoch 20/10 - Batch 26/160, Loss: 0.0321 0.0284\n",
      "training: 0:00:26<0:02:09 Epoch 20/10 - Batch 27/160, Loss: 0.0314 0.0285\n",
      "training: 0:00:26<0:02:05 Epoch 20/10 - Batch 28/160, Loss: 0.0339 0.0439\n",
      "training: 0:00:26<0:02:00 Epoch 20/10 - Batch 29/160, Loss: 0.0330 0.0296\n",
      "training: 0:00:26<0:01:56 Epoch 20/10 - Batch 30/160, Loss: 0.0328 0.0321\n",
      "training: 0:00:27<0:01:52 Epoch 20/10 - Batch 31/160, Loss: 0.0337 0.0370\n",
      "training: 0:00:27<0:01:49 Epoch 20/10 - Batch 32/160, Loss: 0.0330 0.0301\n",
      "training: 0:00:27<0:01:45 Epoch 20/10 - Batch 33/160, Loss: 0.0328 0.0323\n",
      "training: 0:00:27<0:01:42 Epoch 20/10 - Batch 34/160, Loss: 0.0332 0.0347\n",
      "training: 0:00:27<0:01:39 Epoch 20/10 - Batch 35/160, Loss: 0.0337 0.0359\n",
      "training: 0:00:28<0:01:36 Epoch 20/10 - Batch 36/160, Loss: 0.0325 0.0278\n",
      "training: 0:00:28<0:01:33 Epoch 20/10 - Batch 37/160, Loss: 0.0328 0.0340\n",
      "training: 0:00:28<0:01:31 Epoch 20/10 - Batch 38/160, Loss: 0.0332 0.0348\n",
      "training: 0:00:28<0:01:28 Epoch 20/10 - Batch 39/160, Loss: 0.0336 0.0349\n",
      "training: 0:00:28<0:01:26 Epoch 20/10 - Batch 40/160, Loss: 0.0330 0.0308\n",
      "training: 0:00:28<0:01:24 Epoch 20/10 - Batch 41/160, Loss: 0.0320 0.0278\n",
      "training: 0:00:29<0:01:21 Epoch 20/10 - Batch 42/160, Loss: 0.0329 0.0366\n",
      "training: 0:00:29<0:01:19 Epoch 20/10 - Batch 43/160, Loss: 0.0329 0.0328\n",
      "training: 0:00:29<0:01:17 Epoch 20/10 - Batch 44/160, Loss: 0.0331 0.0339\n",
      "training: 0:00:29<0:01:15 Epoch 20/10 - Batch 45/160, Loss: 0.0322 0.0286\n",
      "training: 0:00:29<0:01:14 Epoch 20/10 - Batch 46/160, Loss: 0.0321 0.0319\n",
      "training: 0:00:30<0:01:12 Epoch 20/10 - Batch 47/160, Loss: 0.0321 0.0321\n",
      "training: 0:00:30<0:01:10 Epoch 20/10 - Batch 48/160, Loss: 0.0323 0.0328\n",
      "training: 0:00:30<0:01:08 Epoch 20/10 - Batch 49/160, Loss: 0.0327 0.0342\n",
      "training: 0:00:30<0:01:07 Epoch 20/10 - Batch 50/160, Loss: 0.0328 0.0331\n",
      "training: 0:00:30<0:01:05 Epoch 20/10 - Batch 51/160, Loss: 0.0321 0.0294\n",
      "training: 0:00:30<0:01:04 Epoch 20/10 - Batch 52/160, Loss: 0.0327 0.0350\n",
      "training: 0:00:31<0:01:02 Epoch 20/10 - Batch 53/160, Loss: 0.0331 0.0350\n",
      "training: 0:00:31<0:01:01 Epoch 20/10 - Batch 54/160, Loss: 0.0320 0.0275\n",
      "training: 0:00:31<0:01:00 Epoch 20/10 - Batch 55/160, Loss: 0.0312 0.0280\n",
      "training: 0:00:31<0:00:58 Epoch 20/10 - Batch 56/160, Loss: 0.0326 0.0382\n",
      "training: 0:00:31<0:00:57 Epoch 20/10 - Batch 57/160, Loss: 0.0314 0.0267\n",
      "training: 0:00:32<0:00:56 Epoch 20/10 - Batch 58/160, Loss: 0.0328 0.0383\n",
      "training: 0:00:32<0:00:55 Epoch 20/10 - Batch 59/160, Loss: 0.0328 0.0329\n",
      "training: 0:00:32<0:00:54 Epoch 20/10 - Batch 60/160, Loss: 0.0325 0.0310\n",
      "training: 0:00:32<0:00:52 Epoch 20/10 - Batch 61/160, Loss: 0.0333 0.0364\n",
      "training: 0:00:32<0:00:51 Epoch 20/10 - Batch 62/160, Loss: 0.0331 0.0325\n",
      "training: 0:00:32<0:00:50 Epoch 20/10 - Batch 63/160, Loss: 0.0343 0.0389\n",
      "training: 0:00:33<0:00:49 Epoch 20/10 - Batch 64/160, Loss: 0.0341 0.0336\n",
      "training: 0:00:33<0:00:48 Epoch 20/10 - Batch 65/160, Loss: 0.0340 0.0332\n",
      "training: 0:00:33<0:00:47 Epoch 20/10 - Batch 66/160, Loss: 0.0323 0.0259\n",
      "training: 0:00:33<0:00:46 Epoch 20/10 - Batch 67/160, Loss: 0.0318 0.0298\n",
      "training: 0:00:33<0:00:45 Epoch 20/10 - Batch 68/160, Loss: 0.0320 0.0324\n",
      "training: 0:00:34<0:00:44 Epoch 20/10 - Batch 69/160, Loss: 0.0326 0.0351\n",
      "training: 0:00:34<0:00:44 Epoch 20/10 - Batch 70/160, Loss: 0.0331 0.0353\n",
      "training: 0:00:34<0:00:43 Epoch 20/10 - Batch 71/160, Loss: 0.0329 0.0318\n",
      "training: 0:00:34<0:00:42 Epoch 20/10 - Batch 72/160, Loss: 0.0319 0.0278\n",
      "training: 0:00:34<0:00:41 Epoch 20/10 - Batch 73/160, Loss: 0.0325 0.0352\n",
      "training: 0:00:35<0:00:40 Epoch 20/10 - Batch 74/160, Loss: 0.0325 0.0325\n",
      "training: 0:00:35<0:00:39 Epoch 20/10 - Batch 75/160, Loss: 0.0331 0.0356\n",
      "training: 0:00:35<0:00:39 Epoch 20/10 - Batch 76/160, Loss: 0.0332 0.0334\n",
      "training: 0:00:35<0:00:38 Epoch 20/10 - Batch 77/160, Loss: 0.0326 0.0304\n",
      "training: 0:00:35<0:00:37 Epoch 20/10 - Batch 78/160, Loss: 0.0340 0.0393\n",
      "training: 0:00:35<0:00:36 Epoch 20/10 - Batch 79/160, Loss: 0.0351 0.0396\n",
      "training: 0:00:36<0:00:36 Epoch 20/10 - Batch 80/160, Loss: 0.0339 0.0292\n",
      "training: 0:00:36<0:00:35 Epoch 20/10 - Batch 81/160, Loss: 0.0350 0.0392\n",
      "training: 0:00:36<0:00:34 Epoch 20/10 - Batch 82/160, Loss: 0.0340 0.0299\n",
      "training: 0:00:36<0:00:34 Epoch 20/10 - Batch 83/160, Loss: 0.0328 0.0283\n",
      "training: 0:00:36<0:00:33 Epoch 20/10 - Batch 84/160, Loss: 0.0343 0.0400\n",
      "training: 0:00:37<0:00:32 Epoch 20/10 - Batch 85/160, Loss: 0.0332 0.0292\n",
      "training: 0:00:37<0:00:32 Epoch 20/10 - Batch 86/160, Loss: 0.0343 0.0386\n",
      "training: 0:00:37<0:00:31 Epoch 20/10 - Batch 87/160, Loss: 0.0335 0.0304\n",
      "training: 0:00:37<0:00:30 Epoch 20/10 - Batch 88/160, Loss: 0.0316 0.0238\n",
      "training: 0:00:37<0:00:30 Epoch 20/10 - Batch 89/160, Loss: 0.0314 0.0307\n",
      "training: 0:00:37<0:00:29 Epoch 20/10 - Batch 90/160, Loss: 0.0308 0.0287\n",
      "training: 0:00:38<0:00:28 Epoch 20/10 - Batch 91/160, Loss: 0.0317 0.0350\n",
      "training: 0:00:38<0:00:28 Epoch 20/10 - Batch 92/160, Loss: 0.0317 0.0321\n",
      "training: 0:00:38<0:00:27 Epoch 20/10 - Batch 93/160, Loss: 0.0321 0.0336\n",
      "training: 0:00:38<0:00:27 Epoch 20/10 - Batch 94/160, Loss: 0.0320 0.0315\n",
      "training: 0:00:38<0:00:26 Epoch 20/10 - Batch 95/160, Loss: 0.0334 0.0389\n",
      "training: 0:00:39<0:00:26 Epoch 20/10 - Batch 96/160, Loss: 0.0336 0.0343\n",
      "training: 0:00:39<0:00:25 Epoch 20/10 - Batch 97/160, Loss: 0.0345 0.0383\n",
      "training: 0:00:39<0:00:24 Epoch 20/10 - Batch 98/160, Loss: 0.0339 0.0317\n",
      "training: 0:00:39<0:00:24 Epoch 20/10 - Batch 99/160, Loss: 0.0320 0.0242\n",
      "training: 0:00:39<0:00:23 Epoch 20/10 - Batch 100/160, Loss: 0.0301 0.0227\n",
      "training: 0:00:39<0:00:23 Epoch 20/10 - Batch 101/160, Loss: 0.0301 0.0298\n",
      "saving searchnet-model-19-100.pt\n",
      "\n",
      "training: 0:00:45<0:00:25 Epoch 20/10 - Batch 102/160, Loss: 0.0309 0.0340\n",
      "training: 0:00:45<0:00:25 Epoch 20/10 - Batch 103/160, Loss: 0.0311 0.0322\n",
      "training: 0:00:45<0:00:24 Epoch 20/10 - Batch 104/160, Loss: 0.0320 0.0357\n",
      "training: 0:00:45<0:00:23 Epoch 20/10 - Batch 105/160, Loss: 0.0302 0.0230\n",
      "training: 0:00:45<0:00:23 Epoch 20/10 - Batch 106/160, Loss: 0.0313 0.0355\n",
      "training: 0:00:46<0:00:22 Epoch 20/10 - Batch 107/160, Loss: 0.0305 0.0274\n",
      "training: 0:00:46<0:00:22 Epoch 20/10 - Batch 108/160, Loss: 0.0310 0.0330\n",
      "training: 0:00:47<0:00:22 Epoch 20/10 - Batch 109/160, Loss: 0.0313 0.0324\n",
      "training: 0:00:47<0:00:21 Epoch 20/10 - Batch 110/160, Loss: 0.0319 0.0345\n",
      "training: 0:00:50<0:00:22 Epoch 20/10 - Batch 111/160, Loss: 0.0317 0.0307\n",
      "training: 0:00:52<0:00:22 Epoch 20/10 - Batch 112/160, Loss: 0.0314 0.0301\n",
      "training: 0:00:55<0:00:22 Epoch 20/10 - Batch 113/160, Loss: 0.0317 0.0330\n",
      "training: 0:00:56<0:00:22 Epoch 20/10 - Batch 114/160, Loss: 0.0313 0.0296\n",
      "training: 0:01:01<0:00:24 Epoch 20/10 - Batch 115/160, Loss: 0.0308 0.0286\n",
      "training: 0:01:01<0:00:23 Epoch 20/10 - Batch 116/160, Loss: 0.0310 0.0318\n",
      "training: 0:01:02<0:00:22 Epoch 20/10 - Batch 117/160, Loss: 0.0319 0.0354\n",
      "training: 0:01:02<0:00:22 Epoch 20/10 - Batch 118/160, Loss: 0.0317 0.0311\n",
      "training: 0:01:02<0:00:21 Epoch 20/10 - Batch 119/160, Loss: 0.0313 0.0296\n",
      "training: 0:01:02<0:00:20 Epoch 20/10 - Batch 120/160, Loss: 0.0327 0.0382\n",
      "training: 0:01:02<0:00:20 Epoch 20/10 - Batch 121/160, Loss: 0.0332 0.0353\n",
      "training: 0:01:03<0:00:19 Epoch 20/10 - Batch 122/160, Loss: 0.0338 0.0362\n",
      "training: 0:01:03<0:00:19 Epoch 20/10 - Batch 123/160, Loss: 0.0339 0.0342\n",
      "training: 0:01:03<0:00:18 Epoch 20/10 - Batch 124/160, Loss: 0.0330 0.0297\n",
      "training: 0:01:03<0:00:17 Epoch 20/10 - Batch 125/160, Loss: 0.0348 0.0418\n",
      "training: 0:01:03<0:00:17 Epoch 20/10 - Batch 126/160, Loss: 0.0341 0.0315\n",
      "training: 0:01:03<0:00:16 Epoch 20/10 - Batch 127/160, Loss: 0.0339 0.0327\n",
      "training: 0:01:04<0:00:16 Epoch 20/10 - Batch 128/160, Loss: 0.0357 0.0431\n",
      "saving searchnet-model-19-128.pt\n",
      "\n",
      "\n",
      "Validation:\n",
      "\n",
      "validation: 0:01:18<0:00:18 Epoch 20/10 - Batch 129/160, Loss: 0.0079 0.0079\n",
      "validation: 0:01:26<0:00:19 Epoch 20/10 - Batch 130/160, Loss: 0.0102 0.0126\n",
      "validation: 0:01:26<0:00:19 Epoch 20/10 - Batch 131/160, Loss: 0.0088 0.0061\n",
      "validation: 0:01:26<0:00:18 Epoch 20/10 - Batch 132/160, Loss: 0.0084 0.0071\n",
      "validation: 0:01:26<0:00:17 Epoch 20/10 - Batch 133/160, Loss: 0.0091 0.0118\n",
      "validation: 0:01:26<0:00:16 Epoch 20/10 - Batch 134/160, Loss: 0.0094 0.0110\n",
      "validation: 0:01:26<0:00:15 Epoch 20/10 - Batch 135/160, Loss: 0.0091 0.0070\n",
      "validation: 0:01:26<0:00:15 Epoch 20/10 - Batch 136/160, Loss: 0.0091 0.0095\n",
      "validation: 0:01:26<0:00:14 Epoch 20/10 - Batch 137/160, Loss: 0.0088 0.0064\n",
      "validation: 0:01:26<0:00:13 Epoch 20/10 - Batch 138/160, Loss: 0.0086 0.0070\n",
      "validation: 0:01:26<0:00:13 Epoch 20/10 - Batch 139/160, Loss: 0.0089 0.0121\n",
      "validation: 0:01:26<0:00:12 Epoch 20/10 - Batch 140/160, Loss: 0.0089 0.0082\n",
      "validation: 0:01:26<0:00:11 Epoch 20/10 - Batch 141/160, Loss: 0.0089 0.0097\n",
      "validation: 0:01:26<0:00:10 Epoch 20/10 - Batch 142/160, Loss: 0.0087 0.0062\n",
      "validation: 0:01:26<0:00:10 Epoch 20/10 - Batch 143/160, Loss: 0.0086 0.0072\n",
      "validation: 0:01:26<0:00:09 Epoch 20/10 - Batch 144/160, Loss: 0.0085 0.0061\n",
      "validation: 0:01:26<0:00:08 Epoch 20/10 - Batch 145/160, Loss: 0.0084 0.0067\n",
      "validation: 0:01:26<0:00:08 Epoch 20/10 - Batch 146/160, Loss: 0.0086 0.0128\n",
      "validation: 0:01:26<0:00:07 Epoch 20/10 - Batch 147/160, Loss: 0.0085 0.0064\n",
      "validation: 0:01:27<0:00:07 Epoch 20/10 - Batch 148/160, Loss: 0.0085 0.0078\n",
      "validation: 0:01:27<0:00:06 Epoch 20/10 - Batch 149/160, Loss: 0.0084 0.0072\n",
      "validation: 0:01:27<0:00:05 Epoch 20/10 - Batch 150/160, Loss: 0.0083 0.0065\n",
      "validation: 0:01:27<0:00:05 Epoch 20/10 - Batch 151/160, Loss: 0.0083 0.0083\n",
      "validation: 0:01:27<0:00:04 Epoch 20/10 - Batch 152/160, Loss: 0.0083 0.0077\n",
      "validation: 0:01:27<0:00:03 Epoch 20/10 - Batch 153/160, Loss: 0.0083 0.0082\n",
      "validation: 0:01:27<0:00:03 Epoch 20/10 - Batch 154/160, Loss: 0.0083 0.0071\n",
      "validation: 0:01:27<0:00:02 Epoch 20/10 - Batch 155/160, Loss: 0.0083 0.0104\n",
      "validation: 0:01:27<0:00:02 Epoch 20/10 - Batch 156/160, Loss: 0.0083 0.0068\n",
      "validation: 0:01:27<0:00:01 Epoch 20/10 - Batch 157/160, Loss: 0.0082 0.0061\n",
      "validation: 0:01:27<0:00:01 Epoch 20/10 - Batch 158/160, Loss: 0.0083 0.0101\n",
      "validation: 0:01:27<0:00:00 Epoch 20/10 - Batch 159/160, Loss: 0.0083 0.0087\n",
      "validation: 0:01:27<0:00:00 Epoch 20/10 - Batch 160/160, Loss: 0.0082 0.0061\n",
      "recreating train_snippets_shuffled\n",
      "\n",
      "Validation\n",
      "\n",
      "training: 0:00:00<0:00:29 Epoch 21/10 - Batch 1/160, Loss: 0.0322 0.0322\n",
      "saving searchnet-model-20-0.pt\n",
      "\n",
      "training: 0:00:01<0:01:51 Epoch 21/10 - Batch 2/160, Loss: 0.0340 0.0353\n",
      "training: 0:00:01<0:01:25 Epoch 21/10 - Batch 3/160, Loss: 0.0362 0.0395\n",
      "training: 0:00:01<0:01:14 Epoch 21/10 - Batch 4/160, Loss: 0.0402 0.0496\n",
      "training: 0:00:02<0:01:05 Epoch 21/10 - Batch 5/160, Loss: 0.0375 0.0294\n",
      "training: 0:00:02<0:00:59 Epoch 21/10 - Batch 6/160, Loss: 0.0362 0.0317\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m40\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mhandle_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 147\u001b[0m, in \u001b[0;36mhandle_epoch\u001b[0;34m(batch_size, num_epochs, epoch)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (i, batch) \u001b[38;5;129;01min\u001b[39;00m get_epoch_part(i0, train_batchs, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    146\u001b[0m     performance \u001b[38;5;241m=\u001b[39m (min_alpha, alpha, running_loss, start_time)\n\u001b[0;32m--> 147\u001b[0m     running_loss, alpha \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_train_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperformance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m i0 \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    151\u001b[0m save(epoch, i0)\n",
      "Cell \u001b[0;32mIn[23], line 105\u001b[0m, in \u001b[0;36mhandle_train_batch\u001b[0;34m(i, epoch, batch, performance, sizes)\u001b[0m\n\u001b[1;32m    102\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    104\u001b[0m current_alpha \u001b[38;5;241m=\u001b[39m min_alpha \u001b[38;5;241m+\u001b[39m alpha\n\u001b[0;32m--> 105\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m current_alpha) \u001b[38;5;241m*\u001b[39m running_loss \u001b[38;5;241m+\u001b[39m current_alpha \u001b[38;5;241m*\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m alpha \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.5\u001b[39m\n\u001b[1;32m    108\u001b[0m log(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m, start_time, sizes, epoch, i, running_loss, loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10, 40):\n",
    "    handle_epoch(batch_size, num_epochs, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78b5da4b-c5b6-4ced-9c0a-700d7ecc7a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation: 0:00:00<0:00:01 Epoch 14/20 - Batch 23/160, Loss: 0.0024 0.0312\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "cum_loss = 0\n",
    "start_time = time.time()\n",
    "(i, batch) = next(get_epoch_part(10, 160, 1))\n",
    "cum_loss = handle_val_batch(10, 22, 13, batch, start_time, cum_loss, (128, 20, 160))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
