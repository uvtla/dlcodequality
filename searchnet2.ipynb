{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6db99f-03f8-4d9c-b270-11efa25b0cd7",
   "metadata": {},
   "source": [
    "# After getting files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8225e83-c777-4e68-8d88-1c2a82ca195f",
   "metadata": {},
   "source": [
    "## Insert into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00457c64-4536-4a2a-be83-04e1f1e66e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting dataset location\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "try:\n",
    "    dataset\n",
    "except:\n",
    "    print('getting dataset location')\n",
    "    dataset = Path('dataset').absolute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28b7e35-3703-4a56-9707-fadde8c3f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(dataset)\n",
    "\n",
    "js_dir = dataset / 'javascript'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40094612-df9b-4969-bdd3-94b0f201afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9c940ca-f75f-4f96-9960-3c301cdde7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3, json\n",
    "from tqdm import tqdm\n",
    "from contextlib import contextmanager\n",
    "\n",
    "train = js_dir / 'final' / 'jsonl' / 'train'\n",
    "os.chdir(train)\n",
    "\n",
    "@contextmanager\n",
    "def get_cursor(database_name='rsn_train'):\n",
    "    with sqlite3.connect(database_name) as conn:\n",
    "        yield conn.cursor()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b02a9475-3743-4131-bc5b-864a34c129c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "def simple_parse_xml(content, nested):\n",
    "    pattern = r'\\s*<([^\\s]*?)>\\s*'\n",
    "    open = re.search(pattern, content)\n",
    "    if not open:\n",
    "        return [content]\n",
    "    [open_start, open_end] = open.span()\n",
    "    before = content[:open_start]\n",
    "    tag = open.group(1)\n",
    "    inner_and_after = content[open_end:]\n",
    "    close = re.search(f'</{re.escape(tag)}>', inner_and_after)\n",
    "    if close:\n",
    "        [close_start, close_end] = close.span()\n",
    "    else:\n",
    "        close = re.search(pattern, inner_and_after)\n",
    "        if close:\n",
    "            [close_start, _] = close.span()\n",
    "        else:\n",
    "            close_start = len(inner_and_after)\n",
    "        close_end = close_start\n",
    "    inner = inner_and_after[:close_start]\n",
    "    after = inner_and_after[close_end:]\n",
    "    inner = simple_parse_xml(inner, nested) if nested else inner\n",
    "    return [[before, tag, inner], *simple_parse_xml(after, nested)]\n",
    "\n",
    "def atom_to_re(s):\n",
    "    tokens = [token.strip() for token in re.split(r'\\s+|(?=\\W)|(?<=\\W)', s.strip()) if token]\n",
    "    escaped = [regex.escape(token, special_only=True) for token in tokens]\n",
    "    return r'\\s*' + r'\\s*'.join(escaped) + r'\\s*'\n",
    "\n",
    "def str_to_re(s):\n",
    "    codes = re.split(r'\\s*(?://[^\\n]*(?:\\n|$)|/\\*.*?\\*/|\\.{3,})\\s*', s,  flags=re.DOTALL)\n",
    "    return '(?:.*?)'.join(atom_to_re(code) for code in codes)\n",
    "\n",
    "def node_to_re(node, c):\n",
    "    if type(node) == str:\n",
    "        return str_to_re(node), []\n",
    "    before, tag, content = node\n",
    "    before_re = str_to_re(before)\n",
    "    c[0] += 1\n",
    "    open_gr = c[0]\n",
    "    content_re, content_tags = make_regex(content, c)\n",
    "    c[0] += 1\n",
    "    close_gr = c[0]\n",
    "    open_re = '\\s*(|<'+re.escape(tag)+'>)\\s*'\n",
    "    close_re = '\\s*(|</'+re.escape(tag)+'>)\\s*'\n",
    "    return before_re+open_re+content_re+close_re, [(tag, open_gr, close_gr, content_tags)]\n",
    "\n",
    "\n",
    "def make_regex(tree, c):\n",
    "    regs, tags = zip(*(node_to_re(node, c) for node in tree))\n",
    "    return re.sub(r'(\\\\s\\*)+', r'\\\\s*', ''.join(regs)), [t for tag in tags for t in tag] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904bebf-5087-4d02-ad73-5e4f5aac1cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re\n",
    "omitted = []\n",
    "output = []\n",
    "for text_file in js_dir.glob('thread2-*.txt'):\n",
    "    outfile = str.replace(str(text_file), '.txt', '.json')\n",
    "    if os.path.exists(outfile):\n",
    "        continue\n",
    "    with open(text_file, 'r') as file:\n",
    "        file_contents = file.read()\n",
    "    parsed = simple_parse_xml(file_contents, nested=False)\n",
    "    output.append({\n",
    "        'file': str(text_file),\n",
    "        'split': [p[0] for p in parsed[:-1]] + [parsed[-1]],\n",
    "        'len': len(parsed),\n",
    "        'ok': len(parsed) == 11\n",
    "    })\n",
    "    if len(parsed) != 11:\n",
    "        omitted.append(text_file)\n",
    "        continue\n",
    "    with open(outfile, 'w') as file:\n",
    "        file.write(json.dumps([p[1:] for p in parsed]))\n",
    "\n",
    "with open('parse3.log', 'w') as file:\n",
    "    file.write(json.dumps(output))\n",
    "for out in output:\n",
    "    if not out['ok']:\n",
    "        continue\n",
    "    print(out['file'])\n",
    "    print(out['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709bdefe-5748-4a18-8be6-ae73ddaeead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    # cursor.execute('drop table if exists snippets')\n",
    "    # cursor.execute('drop table if exists region')\n",
    "    cursor.execute('create table if not exists snippets (ID INTEGER PRIMARY KEY, idx INTEGER, code TEXT, locations JSON, regions JSON, SRP boolean)')\n",
    "    cursor.execute('create table if not exists region (ID INTEGER PRIMARY KEY, code TEXT, vector JSON)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8996c197-897d-4d14-bb18-1034b3a38b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_region(code):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute('insert into region (code) values (?)', (code, ))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "def insert_snippet(id, index, code, locations, regions, srp):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute(\n",
    "            'insert into snippets (ID, idx, code, locations, regions, SRP) values (?, ?, ?, ?, ?, ?)',\n",
    "            (id, index, code, json.dumps(locations), json.dumps(regions), srp))\n",
    "\n",
    "def flat_wrong_tags(tags, code, m):\n",
    "    clean_tags = []\n",
    "    for tag in tags:\n",
    "        clean_tags += flat_wrong_tag(tag, clean_tags, code, m)\n",
    "    return clean_tags\n",
    "\n",
    "def flat_wrong_tag(tag, clean_tags, code, m):\n",
    "    name, open, close, sub_tags = tag\n",
    "    clean_sub_tags = flat_wrong_tags(sub_tags, code, m)\n",
    "    if m.group(open):\n",
    "        if len(clean_tags):\n",
    "            pname, popen, pclose, psub_tags = clean_tags[-1]\n",
    "            if not m.group(pclose):\n",
    "                clean_tags[-1] = (pname, popen, close, psub_tags)\n",
    "        return clean_sub_tags\n",
    "    return [(name, open, close, clean_sub_tags)]\n",
    "\n",
    "def tag_to_json(tag, code, m, handle_region):\n",
    "    name, open, close, sub_tags = tag\n",
    "    start = m.span(open)[0]\n",
    "    regions, body, end = tags_to_json(start, sub_tags, code, m, handle_region)\n",
    "    body += code[end:m.span(close)[0]] \n",
    "    region_id = handle_region(f'function {name} () {{\\n{body}\\n}}')\n",
    "    regions = [(start, region_id)] + regions\n",
    "    return regions\n",
    "\n",
    "\n",
    "def tags_to_json(outer_index, tags, code, m, handle_region):\n",
    "    regions = []\n",
    "    outer_body = ''\n",
    "    for tag in tags:\n",
    "        name, open, close, _ = tag\n",
    "        outer_body += code[outer_index:m.span(open)[0]] + '\\n' + name + '();\\n'\n",
    "        outer_index = m.span(close)[0]\n",
    "        regions += tag_to_json(tag, code, m, handle_region)\n",
    "    return regions, outer_body, outer_index\n",
    "\n",
    "\n",
    "def to_json(tags, code, m, handle_region):\n",
    "    regions, body, end = tags_to_json(0, tags, code, m, handle_region)\n",
    "    body += code[end:] \n",
    "    region_id = handle_region(body)\n",
    "    regions = [(0, region_id)] + regions\n",
    "    if len(regions) > 1:\n",
    "        regions.append((end, region_id))\n",
    "    return regions\n",
    "\n",
    "def strip_js_comments(js_code):\n",
    "    js_code = re.sub(r'\\n?//.*?\\n', '\\n', js_code)\n",
    "    js_code = re.sub(r'/\\*.*?\\*/', '', js_code, flags=re.DOTALL)\n",
    "    return js_code    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aca0325d-5b5d-43ad-9649-e4c60ccd872a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8780487804878049"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(js_dir.glob('thread2-*.json')))/len(list(js_dir.glob('thread2-*.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194654e4-f22c-452e-9dfb-5a953fb6a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ok, n_all = 0, 0\n",
    "for text_file in tqdm(list(js_dir.glob('thread2-*.json'))):\n",
    "    index = int(re.match('.*thread2-(.*)\\.json', str(text_file)).group(1))\n",
    "    limit = 10\n",
    "    with get_cursor() as cursor:\n",
    "        codes = list(cursor.execute('select id, code from shuffled limit ? offset ?', (limit, limit * index + 1)))\n",
    "    with open(text_file, 'r') as file:\n",
    "        file_contents = file.read()\n",
    "    for (id, code), obj in zip(codes, json.loads(file_contents)):\n",
    "        original = strip_js_comments(code)\n",
    "        xml = obj[1]\n",
    "        reg_str, tags = make_regex(simple_parse_xml(xml, True), [0])\n",
    "        reg = regex.compile(reg_str, flags = regex.DOTALL)\n",
    "        m = reg.match(original)\n",
    "        n_all += 1\n",
    "        if m:\n",
    "            n_ok += 1\n",
    "            tags = flat_wrong_tags(tags, original, m)\n",
    "            if len(tags) == 1 and not len(tags[0][3]):\n",
    "                tags = []\n",
    "            regions = to_json(tags, original, m, insert_region)\n",
    "            insert_snippet(id, index, original, *zip(*regions), len(tags) == 0)\n",
    "            \n",
    "            \n",
    "print(f'{n_ok}/{n_all}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a65ab-ab7c-4efe-a4d3-2f0f5145c12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    print(list(cursor.execute('select idx < 650, count(*), SRP from snippets group by idx < 650, SRP')))\n",
    "    print(list(cursor.execute('select count(*) from region')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fee2198e-ec9b-46ee-a700-4a102de567d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8663b41d-56f6-4ebd-ba46-8efbfe5be2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(ids):\n",
    "    with get_cursor() as cursor:\n",
    "        regions = cursor.execute(f'select id, code from region where id in ({ids})')\n",
    "        for region in list(regions):\n",
    "            id, code = region\n",
    "            tokenized_inputs = tokenizer([code], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            tokenized_inputs.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**tokenized_inputs)\n",
    "                last_hidden_states = outputs.last_hidden_state\n",
    "            average_hidden_states = last_hidden_states.mean(dim=1)\n",
    "            yield id, list(average_hidden_states.cpu().numpy()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e81adab-31e7-478f-8f0c-9bc251b82e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9133040935672515\n"
     ]
    }
   ],
   "source": [
    "with get_cursor() as cursor:\n",
    "    print(next(cursor.execute('select count(*) from snippets'))[0] *.1 / len(list(js_dir.glob('thread2-*.json'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211bdf4d-2c35-4cd9-81b6-8f3775c59e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('drop table if exists tokenized')\n",
    "    cursor.execute('create table if not exists tokenized (ID INTEGER PRIMARY KEY, input_ids JSON, region_ids JSON)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655692b4-a8bc-4aeb-95f4-9a23fedb1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    snippets = list(cursor.execute('select id, code, locations, regions from snippets where idx < 650'))\n",
    "for (id, code, locations, regions) in tqdm(snippets):\n",
    "    tokens = tokenizer.encode_plus(code, truncation=True, return_offsets_mapping=True)\n",
    "    offset_mapping = tokens['offset_mapping']\n",
    "    regions, locations = json.loads(regions),json.loads(locations)\n",
    "    if not len(regions):\n",
    "        continue\n",
    "    locations.append(len(code))\n",
    "    i = 0\n",
    "    region_ids = []\n",
    "    for (start, end) in offset_mapping[1:-1]:\n",
    "        while start > locations[i+1]:\n",
    "            i += 1\n",
    "        region_ids.append(regions[i])\n",
    "    region_ids = [0] + region_ids + [0]\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute(\n",
    "            'insert into tokenized (ID, input_ids, region_ids) values (?, ?, ?)',\n",
    "            (id, json.dumps(tokens['input_ids']), json.dumps(region_ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bef707b-169d-4458-9f8c-18e57fd47407",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('drop table if exists train_snippets')\n",
    "    cursor.execute('create table train_snippets as select * from snippets where idx < 650')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5569bc6b-d6e2-453d-8dab-c16d2cca3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('create table if not exists current_batch(idx INTEGER)')\n",
    "    if not len(list(cursor.execute('select * from current_batch'))):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78bc3235-55f3-4130-bc94-40624dd4a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('create table if not exists last_batch(idx INTEGER, epoch_idx INTEGER)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fda53217-c701-4ded-9c2b-c8f63e77a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def set_last_batch(last_batch, epoch_idx):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute('update last_batch set idx = ?, epoch_idx = ?', (last_batch, epoch_idx))\n",
    "\n",
    "def get_unsafe_last_batch(batches):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute('select * from last_batch')\n",
    "        item = cursor.fetchone()\n",
    "        if not item:\n",
    "            cursor.execute('insert into last_batch values (0, 0)')\n",
    "            return 0\n",
    "        else:\n",
    "            return item\n",
    "\n",
    "def get_last_batch(batches):\n",
    "    last_batch, last_epoch = get_unsafe_last_batch(batches)\n",
    "    if last_batch >= batches:\n",
    "        last_batch = 0\n",
    "        last_epoch += 1\n",
    "        set_last_batch(0, last_epoch)\n",
    "    return last_batch, last_epoch\n",
    "        \n",
    "def get_batch_tokens(batch_size, i):\n",
    "    with get_cursor() as cursor:\n",
    "        ids = ','.join(\n",
    "            str(x[0]) for x in cursor.execute('select id from train_snippets_shuffled limit ? offset ?', (batch_size, i * batch_size))\n",
    "        )\n",
    "        tokens = cursor.execute(f'select input_ids, region_ids from tokenized where id in ({ids})')\n",
    "        return [(json.loads(input_ids), json.loads(region_ids)) for (input_ids, region_ids) in tokens]\n",
    "        \n",
    "\n",
    "def get_batch_regions(tokens):\n",
    "    with get_cursor() as cursor:\n",
    "        region_ids_str = ','.join(set(str(region_id) for (_, region_ids) in tokens for region_id in region_ids))\n",
    "        return dict(get_labels(region_ids_str))\n",
    "\n",
    "def prepare_iteration(input_ids, region_ids, regions):\n",
    "    label_size = 768\n",
    "    size = len(input_ids)\n",
    "    if size > 512:\n",
    "        print(size)\n",
    "    empties = 512 - size\n",
    "    null_vector = [0] * label_size\n",
    "    for r_id in region_ids:\n",
    "        if r_id and not (r_id in regions):\n",
    "            print(r_id)\n",
    "    label = [regions[r_id] if r_id else null_vector for r_id in region_ids]\n",
    "    label += [null_vector] * empties\n",
    "    input_ids += [0] * empties\n",
    "    attention = [1] * size + [0] * empties\n",
    "    try:\n",
    "        torch.FloatTensor(label)\n",
    "    except:\n",
    "        print(regions[region_ids[1]])\n",
    "\n",
    "    return input_ids, attention, label\n",
    "\n",
    "def get_batch(batch_size, i):\n",
    "    tokens = get_batch_tokens(batch_size, i)\n",
    "    regions = get_batch_regions(tokens)\n",
    "    batch = [prepare_iteration(input_ids, region_ids, regions) for (input_ids, region_ids) in tokens]\n",
    "    input_ids, attention, label = zip(*batch)\n",
    "    return (torch.IntTensor(input_ids), torch.FloatTensor(attention), torch.FloatTensor(label)), tokens\n",
    "\n",
    "\n",
    "def get_epoch_part(i0, batchs, batch_size):\n",
    "    return ((i, get_batch(batch_size, i)[0]) for i in range(i0, batchs, 1))\n",
    "\n",
    "def tee(text):\n",
    "    print(f\"\\r{text}\\r\")\n",
    "    with open('train-resnet.log', 'a') as file:\n",
    "        file.write(f\"{text}\\n\")\n",
    "\n",
    "def log(title, start_time, sizes, epoch, i, mean_loss, loss):\n",
    "    _, num_epochs, batches = sizes\n",
    "    i += 1\n",
    "    dt = time.time() - start_time\n",
    "    elapsed = timedelta(seconds=int(dt))\n",
    "    remaining = timedelta(seconds=int(dt*(batches-i)/i))\n",
    "    text = f\"{title}: {elapsed}<{remaining} Epoch {epoch+1}/{num_epochs} - Batch {i}/{batches}, Loss: {mean_loss:.4f} {loss.item():.4f}\"\n",
    "    tee(text)\n",
    "\n",
    "def save(epoch, i):\n",
    "    \n",
    "    torch.save(model.state_dict(), f'searchnet-bcemodel-{epoch}-{i}.pt')\n",
    "    torch.save(optimizer.state_dict(), f'searchnet-bceoptimizer-{epoch}-{i}.pt')\n",
    "    set_last_batch(i, epoch)\n",
    "    tee(f'saving searchnet-bcemodel-{epoch}-{i}.pt\\n')\n",
    "\n",
    "def handle_train_batch(i, epoch, batch, performance, sizes):\n",
    "    input_ids, attention, labels = (c.to(device) for c in batch)\n",
    "    min_alpha, alpha, running_loss, start_time = performance\n",
    "    train_batchs, num_epochs, batches = sizes\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids, attention_mask=attention)\n",
    "    loss = loss_fn(outputs.last_hidden_state, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    current_alpha = min_alpha + alpha\n",
    "    running_loss = (1 - current_alpha) * running_loss + current_alpha * loss.item()\n",
    "    alpha *= .5\n",
    "    \n",
    "    log('training', start_time, sizes, epoch, i, running_loss, loss)\n",
    "    if i % 100 == 0:\n",
    "        save(epoch, i)\n",
    "    return running_loss, alpha\n",
    "    \n",
    "def handle_val_batch(i0, i, epoch, batch, start_time, cum_loss, sizes):\n",
    "    input_ids, attention, labels = (c.to(device) for c in batch)\n",
    "    _, num_epochs, batches = sizes\n",
    "\n",
    "    with torch.no_grad():            \n",
    "        outputs = model(input_ids, attention_mask=attention)\n",
    "        loss = loss_fn(outputs.last_hidden_state, labels)\n",
    "    \n",
    "    cum_loss += loss.item()\n",
    "    mean_loss = cum_loss / (i - i0 + 1)\n",
    "    \n",
    "    log('validation', start_time, sizes, epoch, i, mean_loss, loss)\n",
    "    return cum_loss\n",
    "    \n",
    "\n",
    "def handle_epoch(batch_size, num_epochs):\n",
    "    with get_cursor() as cursor:\n",
    "        epoch_size = next(cursor.execute('select count(*) from train_snippets'))[0]\n",
    "    batches = (epoch_size - 1) // batch_size + 1\n",
    "    train_batchs = 4 * batches // 5\n",
    "    i0, epoch0 = get_last_batch(train_batchs)\n",
    "    num_epochs += epoch0\n",
    "    i = i0 - 1\n",
    "    min_alpha = .2\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    alpha = 1 - min_alpha\n",
    "\n",
    "    model.train()\n",
    "    sizes = (train_batchs, num_epochs, batches)\n",
    "    tee('\\nTraining\\n')\n",
    "    for (i, batch) in get_epoch_part(i0, train_batchs, batch_size):\n",
    "        performance = (min_alpha, alpha, running_loss, start_time)\n",
    "        running_loss, alpha = handle_train_batch(i, epoch, batch, performance, sizes)\n",
    "    \n",
    "    \n",
    "    i0 = i + 1\n",
    "    save(epoch, i0)\n",
    "    \n",
    "    model.eval()\n",
    "    cum_loss = 0\n",
    "    tee('\\nValidation:\\n')\n",
    "    for (i, batch) in get_epoch_part(i0, batches, batch_size):\n",
    "        cum_loss = handle_val_batch(i0, i, epoch, batch, start_time, cum_loss, sizes)\n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59af576d-6e6b-4f4d-910d-ff23dab8cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_last_batch(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df189e27-706e-4ee0-81e2-ef57d1522b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "#set_last_batch(0, 0)\n",
    "for epoch in range(0):\n",
    "    handle_epoch(batch_size, num_epochs)\n",
    "\n",
    "model.load_state_dict(torch.load('searchnet-bcemodel-0-200.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78b5da4b-c5b6-4ced-9c0a-700d7ecc7a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_last_batch(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "932bf29a-b41d-48c4-be7f-a4e61abfc46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import torch\n",
    "device = torch.device('cuda')\n",
    "model = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e20882d-06a2-40de-96e1-645b661e1192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# Function to find the best matching between predicted and true labels\n",
    "def best_label_matching(y_true, y_pred):\n",
    "    D = np.max([y_pred.max(), y_true.max()]) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
    "    return list(zip(row_ind, col_ind))\n",
    "\n",
    "\n",
    "\n",
    "def compute_accuracy(data, true_labels):\n",
    "\n",
    "\n",
    "    unique_labels = np.unique(true_labels)\n",
    "    label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    true_labels = np.array([label_mapping[label] for label in true_labels])\n",
    "\n",
    "    \n",
    "    # Perform Hierarchical Agglomerative Clustering\n",
    "    Z = linkage(pdist(data), method='ward')\n",
    "    predicted_labels = fcluster(Z, t=len(set(true_labels)), criterion='maxclust')\n",
    "    \n",
    "    \n",
    "    # Find the best matching\n",
    "    ind = best_label_matching(true_labels, predicted_labels)\n",
    "    ind_dict = {i[0]: i[1] for i in ind}\n",
    "    \n",
    "    # Remap predicted labels to best matching true labels\n",
    "    remapped_predicted_labels = np.array([ind_dict.get(label, 0) for label in predicted_labels])\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    return accuracy_score(true_labels, remapped_predicted_labels)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a962ea4-5b19-4f37-972a-5985e53131e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.rand(512, 786)  # Replace with your actual data\n",
    "true_labels = np.random.randint(5, size=512)  # Replace with your actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0d7d52f-4906-4bdf-b6f9-d93446126463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.26171875\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {compute_accuracy(data, true_labels+10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a20d780a-15f0-4ab0-9f82-3a2c4e1c8d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "d = []\n",
    "def get_batcgh(batch_size, i):\n",
    "    batch, tokens = get_batch(batch_size, i)\n",
    "    input_ids, attention, label = (c.to(device) for c in batch)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention)[0]\n",
    "        data = outputs.cpu()[0].numpy()\n",
    "        labels = np.array(tokens[0][0])\n",
    "        d.append((data, labels))\n",
    "        print(labels.shape)\n",
    "        #print(compute_accuracy(outputs.cpu()[0], np.array(tokens[0][0])))\n",
    "get_batcgh(1, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fab2ca97-c058-4356-ac5a-57d932952fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 786)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52c2711c-a137-45d0-9251-06df13a6e74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3b92e1c-6721-44a5-9f17-524f733f3710",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data, true_labels), = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4fe67bd-5f25-45bb-9e8b-a59334e78a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc09c7f-0ddc-47af-be38-f4c17b996445",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "494dde43-fa47-4cc6-8eac-0080e9ba575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = np.unique(true_labels)\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "true_labels = np.array([label_mapping[label] for label in true_labels])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da9bc09c-d573-41d6-8319-8a004a235836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 31, 27, 11, 33,  6, 29,  3, 18,  8, 25, 37, 15, 10, 35, 36,  2,\n",
       "       12, 36, 16, 29, 30, 25, 37, 15, 15, 15, 17,  9, 20, 28, 34, 22, 23,\n",
       "       14,  4,  5,  7, 21, 26, 37, 15, 32, 38, 15, 13, 27, 11, 19, 33, 16,\n",
       "       29,  3, 18,  8, 37, 24,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4b94a73-e426-4504-8753-975cae4d5901",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(pdist(data), method='ward')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b786ff3-31a5-4706-a61a-eb79d9c64a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 18, 38, 39, 36, 24, 33, 20, 35, 21, 22,  2, 11, 14, 24, 34, 25,\n",
       "       16, 32, 25, 33, 21, 22,  2, 10, 10, 11, 14, 14, 12, 17, 26, 27, 28,\n",
       "       13, 29, 31, 30, 32, 21,  6, 11, 23,  6, 11, 15, 38, 39, 19, 37, 25,\n",
       "       33, 20, 35, 21,  6, 23,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "        9,  9,  5,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "        9,  9,  9,  9,  9,  5,  5,  5,  2,  5,  5,  4,  4,  9,  4,  9,  5,\n",
       "        5,  9,  5,  9,  9,  9,  9,  9,  4,  9,  9,  5,  5,  5,  9,  4,  9,\n",
       "        4,  4,  4,  5,  1,  1,  5,  2,  5,  5,  4,  4,  4,  4,  4,  4,  4,\n",
       "        3,  5,  5,  5,  9,  9,  4,  4,  4,  4,  9,  5,  5,  5,  5,  5,  5,\n",
       "        4,  4,  4,  4,  5,  6,  6,  1,  2,  7,  7,  7,  7,  7,  9,  4,  9,\n",
       "        4,  3,  3,  5,  5,  7,  7,  7,  7,  7,  7,  5,  1,  1,  1,  2,  2,\n",
       "        7,  7,  7,  7,  3,  5,  4,  3,  5,  3,  5,  2,  5,  7,  7,  7,  4,\n",
       "        4,  4,  4,  5,  5,  3,  5,  5,  5,  4,  4,  4,  3,  4,  5,  1,  1,\n",
       "        1,  2,  2,  7,  7,  7,  7,  4,  4,  4,  4,  3,  3,  5,  5,  9,  7,\n",
       "        9,  7,  4,  4,  4,  1,  1,  1,  2,  2,  2,  5,  5,  3,  3,  5,  5,\n",
       "        2,  5,  1,  2,  2,  2,  2,  2,  7,  7,  4,  4,  4,  5,  5,  5,  5,\n",
       "        5,  4,  4,  4,  4,  4,  5,  6,  6,  1,  2,  2,  7,  7,  7,  7,  4,\n",
       "        4,  4,  5,  3,  4,  5,  5,  7,  7,  7,  7,  7,  4,  4,  1,  3,  5,\n",
       "        5,  7,  7,  7,  7,  7,  1,  1,  1,  1,  2,  2,  7,  2,  7,  3,  5,\n",
       "        3,  3,  5,  3,  2,  7,  7,  7,  7,  7,  4,  1,  1,  1,  2,  2,  2,\n",
       "        2,  3,  3,  3,  5,  2,  1,  1,  2,  2,  2,  2,  7,  7,  4,  4,  3,\n",
       "        3,  5,  5,  5,  7,  7,  4,  1,  1,  1,  1,  2,  2,  2,  2,  3,  3,\n",
       "        4,  4,  3,  3,  2,  2,  2,  7,  7,  4,  4,  1,  1,  2,  2,  2,  2,\n",
       "        3,  3,  5,  2,  1,  1,  2,  2,  2,  2,  7,  4,  4,  4,  5,  5,  5,\n",
       "        5,  4,  9,  4,  4,  9,  6,  1,  2,  7,  7,  4,  4,  5,  5,  6,  5,\n",
       "        2,  7,  7,  7,  9,  9,  9,  9,  9,  5,  9,  7,  9,  4,  4,  9,  5,\n",
       "        1,  5,  2,  2,  4,  3,  4,  4,  4,  5,  5,  5,  7,  7,  4,  4,  9,\n",
       "        9,  9,  5,  5,  4,  4,  9,  4,  9,  5,  1,  2,  2,  7,  7,  4,  9,\n",
       "        4,  9,  4,  9,  5,  9,  9,  9,  4,  4,  9,  5,  5,  5,  5,  5,  4,\n",
       "        4,  4,  5,  1,  1,  2,  2,  5,  7,  4,  9,  9,  4,  3,  5,  5,  5,\n",
       "        9,  8], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = fcluster(Z, t=len(set(true_labels)), criterion='maxclust')\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2094d89b-330e-412a-a26a-cf1cd0717f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ind = best_label_matching(true_labels, predicted_labels)\n",
    "ind_dict = {i[0]: i[1] for i in ind}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d493a7ca-9108-454e-a6fe-fa47c1d828ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "remapped_predicted_labels = np.array([ind_dict.get(label, 0) for label in predicted_labels])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "208166f6-98fb-4590-a915-7584eea06867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 31, 27, 11, 33, 35, 29,  3, 18,  8, 25, 37, 15,  9, 35, 36, 16,\n",
       "       12, 21, 16, 29,  8, 25, 37, 39, 39, 15,  9,  9, 20, 28, 34, 22, 23,\n",
       "       14,  4,  5,  7, 21,  8, 38, 15, 24, 38, 15, 13, 27, 11, 19, 30, 16,\n",
       "       29,  3, 18,  8, 38, 24,  1, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
       "       32, 32, 17, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
       "       32, 32, 32, 32, 32, 17, 17, 17, 37, 17, 17,  0,  0, 32,  0, 32, 17,\n",
       "       17, 32, 17, 32, 32, 32, 32, 32,  0, 32, 32, 17, 17, 17, 32,  0, 32,\n",
       "        0,  0,  0, 17,  2,  2, 17, 37, 17, 17,  0,  0,  0,  0,  0,  0,  0,\n",
       "       10, 17, 17, 17, 32, 32,  0,  0,  0,  0, 32, 17, 17, 17, 17, 17, 17,\n",
       "        0,  0,  0,  0, 17, 38, 38,  2, 37, 26, 26, 26, 26, 26, 32,  0, 32,\n",
       "        0, 10, 10, 17, 17, 26, 26, 26, 26, 26, 26, 17,  2,  2,  2, 37, 37,\n",
       "       26, 26, 26, 26, 10, 17,  0, 10, 17, 10, 17, 37, 17, 26, 26, 26,  0,\n",
       "        0,  0,  0, 17, 17, 10, 17, 17, 17,  0,  0,  0, 10,  0, 17,  2,  2,\n",
       "        2, 37, 37, 26, 26, 26, 26,  0,  0,  0,  0, 10, 10, 17, 17, 32, 26,\n",
       "       32, 26,  0,  0,  0,  2,  2,  2, 37, 37, 37, 17, 17, 10, 10, 17, 17,\n",
       "       37, 17,  2, 37, 37, 37, 37, 37, 26, 26,  0,  0,  0, 17, 17, 17, 17,\n",
       "       17,  0,  0,  0,  0,  0, 17, 38, 38,  2, 37, 37, 26, 26, 26, 26,  0,\n",
       "        0,  0, 17, 10,  0, 17, 17, 26, 26, 26, 26, 26,  0,  0,  2, 10, 17,\n",
       "       17, 26, 26, 26, 26, 26,  2,  2,  2,  2, 37, 37, 26, 37, 26, 10, 17,\n",
       "       10, 10, 17, 10, 37, 26, 26, 26, 26, 26,  0,  2,  2,  2, 37, 37, 37,\n",
       "       37, 10, 10, 10, 17, 37,  2,  2, 37, 37, 37, 37, 26, 26,  0,  0, 10,\n",
       "       10, 17, 17, 17, 26, 26,  0,  2,  2,  2,  2, 37, 37, 37, 37, 10, 10,\n",
       "        0,  0, 10, 10, 37, 37, 37, 26, 26,  0,  0,  2,  2, 37, 37, 37, 37,\n",
       "       10, 10, 17, 37,  2,  2, 37, 37, 37, 37, 26,  0,  0,  0, 17, 17, 17,\n",
       "       17,  0, 32,  0,  0, 32, 38,  2, 37, 26, 26,  0,  0, 17, 17, 38, 17,\n",
       "       37, 26, 26, 26, 32, 32, 32, 32, 32, 17, 32, 26, 32,  0,  0, 32, 17,\n",
       "        2, 17, 37, 37,  0, 10,  0,  0,  0, 17, 17, 17, 26, 26,  0,  0, 32,\n",
       "       32, 32, 17, 17,  0,  0, 32,  0, 32, 17,  2, 37, 37, 26, 26,  0, 32,\n",
       "        0, 32,  0, 32, 17, 32, 32, 32,  0,  0, 32, 17, 17, 17, 17, 17,  0,\n",
       "        0,  0, 17,  2,  2, 37, 37, 17, 26,  0, 32, 32,  0, 10, 17, 17, 17,\n",
       "       32,  1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remapped_predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c731101-d3e4-42ff-ba7f-a581abfd95a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.265625"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(true_labels, remapped_predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff47af24-e147-4608-8445-3634f0482eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
