{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6db99f-03f8-4d9c-b270-11efa25b0cd7",
   "metadata": {},
   "source": [
    "# After getting files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8225e83-c777-4e68-8d88-1c2a82ca195f",
   "metadata": {},
   "source": [
    "## Insert into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00457c64-4536-4a2a-be83-04e1f1e66e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting dataset location\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "try:\n",
    "    dataset\n",
    "except:\n",
    "    print('getting dataset location')\n",
    "    dataset = Path('dataset').absolute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28b7e35-3703-4a56-9707-fadde8c3f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(dataset)\n",
    "\n",
    "js_dir = dataset / 'javascript'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40094612-df9b-4969-bdd3-94b0f201afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c940ca-f75f-4f96-9960-3c301cdde7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3, json\n",
    "from tqdm import tqdm\n",
    "from contextlib import contextmanager\n",
    "\n",
    "train = js_dir / 'final' / 'jsonl' / 'train'\n",
    "os.chdir(train)\n",
    "\n",
    "@contextmanager\n",
    "def get_cursor(database_name='rsn_train'):\n",
    "    with sqlite3.connect(database_name) as conn:\n",
    "        yield conn.cursor()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b02a9475-3743-4131-bc5b-864a34c129c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "def simple_parse_xml(content, nested):\n",
    "    pattern = r'\\s*<([^\\s]*?)>\\s*'\n",
    "    open = re.search(pattern, content)\n",
    "    if not open:\n",
    "        return [content]\n",
    "    [open_start, open_end] = open.span()\n",
    "    before = content[:open_start]\n",
    "    tag = open.group(1)\n",
    "    inner_and_after = content[open_end:]\n",
    "    close = re.search(f'</{re.escape(tag)}>', inner_and_after)\n",
    "    if close:\n",
    "        [close_start, close_end] = close.span()\n",
    "    else:\n",
    "        close = re.search(pattern, inner_and_after)\n",
    "        if close:\n",
    "            [close_start, _] = close.span()\n",
    "        else:\n",
    "            close_start = len(inner_and_after)\n",
    "        close_end = close_start\n",
    "    inner = inner_and_after[:close_start]\n",
    "    after = inner_and_after[close_end:]\n",
    "    inner = simple_parse_xml(inner, nested) if nested else inner\n",
    "    return [[before, tag, inner], *simple_parse_xml(after, nested)]\n",
    "\n",
    "def atom_to_re(s):\n",
    "    tokens = [token.strip() for token in re.split(r'\\s+|(?=\\W)|(?<=\\W)', s.strip()) if token]\n",
    "    escaped = [regex.escape(token, special_only=True) for token in tokens]\n",
    "    return r'\\s*' + r'\\s*'.join(escaped) + r'\\s*'\n",
    "\n",
    "def str_to_re(s):\n",
    "    codes = re.split(r'\\s*(?://[^\\n]*(?:\\n|$)|/\\*.*?\\*/|\\.{3,})\\s*', s,  flags=re.DOTALL)\n",
    "    return '(?:.*?)'.join(atom_to_re(code) for code in codes)\n",
    "\n",
    "def node_to_re(node, c):\n",
    "    if type(node) == str:\n",
    "        return str_to_re(node), []\n",
    "    before, tag, content = node\n",
    "    before_re = str_to_re(before)\n",
    "    c[0] += 1\n",
    "    open_gr = c[0]\n",
    "    content_re, content_tags = make_regex(content, c)\n",
    "    c[0] += 1\n",
    "    close_gr = c[0]\n",
    "    open_re = '\\s*(|<'+re.escape(tag)+'>)\\s*'\n",
    "    close_re = '\\s*(|</'+re.escape(tag)+'>)\\s*'\n",
    "    return before_re+open_re+content_re+close_re, [(tag, open_gr, close_gr, content_tags)]\n",
    "\n",
    "\n",
    "def make_regex(tree, c):\n",
    "    regs, tags = zip(*(node_to_re(node, c) for node in tree))\n",
    "    return re.sub(r'(\\\\s\\*)+', r'\\\\s*', ''.join(regs)), [t for tag in tags for t in tag] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904bebf-5087-4d02-ad73-5e4f5aac1cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re\n",
    "omitted = []\n",
    "output = []\n",
    "for text_file in js_dir.glob('thread2-*.txt'):\n",
    "    outfile = str.replace(str(text_file), '.txt', '.json')\n",
    "    if os.path.exists(outfile):\n",
    "        continue\n",
    "    with open(text_file, 'r') as file:\n",
    "        file_contents = file.read()\n",
    "    parsed = simple_parse_xml(file_contents, nested=False)\n",
    "    output.append({\n",
    "        'file': str(text_file),\n",
    "        'split': [p[0] for p in parsed[:-1]] + [parsed[-1]],\n",
    "        'len': len(parsed),\n",
    "        'ok': len(parsed) == 11\n",
    "    })\n",
    "    if len(parsed) != 11:\n",
    "        omitted.append(text_file)\n",
    "        continue\n",
    "    with open(outfile, 'w') as file:\n",
    "        file.write(json.dumps([p[1:] for p in parsed]))\n",
    "\n",
    "with open('parse3.log', 'w') as file:\n",
    "    file.write(json.dumps(output))\n",
    "for out in output:\n",
    "    if not out['ok']:\n",
    "        continue\n",
    "    print(out['file'])\n",
    "    print(out['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709bdefe-5748-4a18-8be6-ae73ddaeead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    # cursor.execute('drop table if exists snippets')\n",
    "    # cursor.execute('drop table if exists region')\n",
    "    cursor.execute('create table if not exists snippets (ID INTEGER PRIMARY KEY, idx INTEGER, code TEXT, locations JSON, regions JSON, SRP boolean)')\n",
    "    cursor.execute('create table if not exists region (ID INTEGER PRIMARY KEY, code TEXT, vector JSON)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8996c197-897d-4d14-bb18-1034b3a38b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_region(code):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute('insert into region (code) values (?)', (code, ))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "def insert_snippet(id, index, code, locations, regions, srp):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute(\n",
    "            'insert into snippets (ID, idx, code, locations, regions, SRP) values (?, ?, ?, ?, ?, ?)',\n",
    "            (id, index, code, json.dumps(locations), json.dumps(regions), srp))\n",
    "\n",
    "def flat_wrong_tags(tags, code, m):\n",
    "    clean_tags = []\n",
    "    for tag in tags:\n",
    "        clean_tags += flat_wrong_tag(tag, clean_tags, code, m)\n",
    "    return clean_tags\n",
    "\n",
    "def flat_wrong_tag(tag, clean_tags, code, m):\n",
    "    name, open, close, sub_tags = tag\n",
    "    clean_sub_tags = flat_wrong_tags(sub_tags, code, m)\n",
    "    if m.group(open):\n",
    "        if len(clean_tags):\n",
    "            pname, popen, pclose, psub_tags = clean_tags[-1]\n",
    "            if not m.group(pclose):\n",
    "                clean_tags[-1] = (pname, popen, close, psub_tags)\n",
    "        return clean_sub_tags\n",
    "    return [(name, open, close, clean_sub_tags)]\n",
    "\n",
    "def tag_to_json(tag, code, m, handle_region):\n",
    "    name, open, close, sub_tags = tag\n",
    "    start = m.span(open)[0]\n",
    "    regions, body, end = tags_to_json(start, sub_tags, code, m, handle_region)\n",
    "    body += code[end:m.span(close)[0]] \n",
    "    region_id = handle_region(f'function {name} () {{\\n{body}\\n}}')\n",
    "    regions = [(start, region_id)] + regions\n",
    "    return regions\n",
    "\n",
    "\n",
    "def tags_to_json(outer_index, tags, code, m, handle_region):\n",
    "    regions = []\n",
    "    outer_body = ''\n",
    "    for tag in tags:\n",
    "        name, open, close, _ = tag\n",
    "        outer_body += code[outer_index:m.span(open)[0]] + '\\n' + name + '();\\n'\n",
    "        outer_index = m.span(close)[0]\n",
    "        regions += tag_to_json(tag, code, m, handle_region)\n",
    "    return regions, outer_body, outer_index\n",
    "\n",
    "\n",
    "def to_json(tags, code, m, handle_region):\n",
    "    regions, body, end = tags_to_json(0, tags, code, m, handle_region)\n",
    "    body += code[end:] \n",
    "    region_id = handle_region(body)\n",
    "    regions = [(0, region_id)] + regions\n",
    "    if len(regions) > 1:\n",
    "        regions.append((end, region_id))\n",
    "    return regions\n",
    "\n",
    "def strip_js_comments(js_code):\n",
    "    js_code = re.sub(r'\\n?//.*?\\n', '\\n', js_code)\n",
    "    js_code = re.sub(r'/\\*.*?\\*/', '', js_code, flags=re.DOTALL)\n",
    "    return js_code    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aca0325d-5b5d-43ad-9649-e4c60ccd872a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8780487804878049"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(js_dir.glob('thread2-*.json')))/len(list(js_dir.glob('thread2-*.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194654e4-f22c-452e-9dfb-5a953fb6a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ok, n_all = 0, 0\n",
    "for text_file in tqdm(list(js_dir.glob('thread2-*.json'))):\n",
    "    index = int(re.match('.*thread2-(.*)\\.json', str(text_file)).group(1))\n",
    "    limit = 10\n",
    "    with get_cursor() as cursor:\n",
    "        codes = list(cursor.execute('select id, code from shuffled limit ? offset ?', (limit, limit * index + 1)))\n",
    "    with open(text_file, 'r') as file:\n",
    "        file_contents = file.read()\n",
    "    for (id, code), obj in zip(codes, json.loads(file_contents)):\n",
    "        original = strip_js_comments(code)\n",
    "        xml = obj[1]\n",
    "        reg_str, tags = make_regex(simple_parse_xml(xml, True), [0])\n",
    "        reg = regex.compile(reg_str, flags = regex.DOTALL)\n",
    "        m = reg.match(original)\n",
    "        n_all += 1\n",
    "        if m:\n",
    "            n_ok += 1\n",
    "            tags = flat_wrong_tags(tags, original, m)\n",
    "            if len(tags) == 1 and not len(tags[0][3]):\n",
    "                tags = []\n",
    "            regions = to_json(tags, original, m, insert_region)\n",
    "            insert_snippet(id, index, original, *zip(*regions), len(tags) == 0)\n",
    "            \n",
    "            \n",
    "print(f'{n_ok}/{n_all}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a65ab-ab7c-4efe-a4d3-2f0f5145c12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    print(list(cursor.execute('select idx < 650, count(*), SRP from snippets group by idx < 650, SRP')))\n",
    "    print(list(cursor.execute('select count(*) from region')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fee2198e-ec9b-46ee-a700-4a102de567d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8663b41d-56f6-4ebd-ba46-8efbfe5be2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt(v):\n",
    "     return v / 2 + .5\n",
    "def get_labels(ids):\n",
    "    with get_cursor() as cursor:\n",
    "        regions = cursor.execute(f'select id, code from region where id in ({ids})')\n",
    "        for region in list(regions):\n",
    "            id, code = region\n",
    "            tokenized_inputs = tokenizer([code], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            tokenized_inputs.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**tokenized_inputs)\n",
    "                last_hidden_states = outputs.last_hidden_state\n",
    "            average_hidden_states = last_hidden_states.mean(dim=1)\n",
    "            yield id, list(average_hidden_states.cpu().numpy()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e81adab-31e7-478f-8f0c-9bc251b82e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9133040935672515\n"
     ]
    }
   ],
   "source": [
    "with get_cursor() as cursor:\n",
    "    print(next(cursor.execute('select count(*) from snippets'))[0] *.1 / len(list(js_dir.glob('thread2-*.json'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211bdf4d-2c35-4cd9-81b6-8f3775c59e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('drop table if exists tokenized')\n",
    "    cursor.execute('create table if not exists tokenized (ID INTEGER PRIMARY KEY, input_ids JSON, region_ids JSON)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655692b4-a8bc-4aeb-95f4-9a23fedb1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    snippets = list(cursor.execute('select id, code, locations, regions from snippets where idx < 650'))\n",
    "for (id, code, locations, regions) in tqdm(snippets):\n",
    "    tokens = tokenizer.encode_plus(code, truncation=True, return_offsets_mapping=True)\n",
    "    offset_mapping = tokens['offset_mapping']\n",
    "    regions, locations = json.loads(regions),json.loads(locations)\n",
    "    if not len(regions):\n",
    "        continue\n",
    "    locations.append(len(code))\n",
    "    i = 0\n",
    "    region_ids = []\n",
    "    for (start, end) in offset_mapping[1:-1]:\n",
    "        while start > locations[i+1]:\n",
    "            i += 1\n",
    "        region_ids.append(regions[i])\n",
    "    region_ids = [0] + region_ids + [0]\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute(\n",
    "            'insert into tokenized (ID, input_ids, region_ids) values (?, ?, ?)',\n",
    "            (id, json.dumps(tokens['input_ids']), json.dumps(region_ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bef707b-169d-4458-9f8c-18e57fd47407",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('drop table if exists train_snippets')\n",
    "    cursor.execute('create table train_snippets as select * from snippets where idx < 650')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5569bc6b-d6e2-453d-8dab-c16d2cca3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('create table if not exists current_batch(idx INTEGER)')\n",
    "    if not len(list(cursor.execute('select * from current_batch'))):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78bc3235-55f3-4130-bc94-40624dd4a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('create table if not exists last_batch(idx INTEGER, epoch_idx INTEGER)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fda53217-c701-4ded-9c2b-c8f63e77a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def set_last_batch(last_batch, epoch_idx):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute('update last_batch set idx = ?, epoch_idx = ?', (last_batch, epoch_idx))\n",
    "\n",
    "def get_unsafe_last_batch(batches):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute('select * from last_batch')\n",
    "        item = cursor.fetchone()\n",
    "        if not item:\n",
    "            cursor.execute('insert into last_batch values (0, 0)')\n",
    "            return 0\n",
    "        else:\n",
    "            return item\n",
    "\n",
    "def get_last_batch(batches):\n",
    "    last_batch, last_epoch = get_unsafe_last_batch(batches)\n",
    "    if last_batch >= batches:\n",
    "        last_batch = 0\n",
    "        last_epoch += 1\n",
    "        set_last_batch(0, last_epoch)\n",
    "    return last_batch, last_epoch\n",
    "        \n",
    "def get_batch_tokens(batch_size, i):\n",
    "    with get_cursor() as cursor:\n",
    "        ids = ','.join(\n",
    "            str(x[0]) for x in cursor.execute('select id from train_snippets_shuffled limit ? offset ?', (batch_size, i))\n",
    "        )\n",
    "        tokens = cursor.execute(f'select input_ids, region_ids from tokenized where id in ({ids})')\n",
    "        zz = [(json.loads(input_ids), json.loads(region_ids)) for (input_ids, region_ids) in tokens]\n",
    "        print(batch_size, len(zz))\n",
    "        return zz\n",
    "        \n",
    "\n",
    "def get_batch_regions(tokens):\n",
    "    with get_cursor() as cursor:\n",
    "        region_ids_str = ','.join(set(str(region_id) for (_, region_ids) in tokens for region_id in region_ids))\n",
    "        return dict(get_labels(region_ids_str))\n",
    "\n",
    "def prepare_iteration(input_ids, region_ids, regions):\n",
    "    label_size = 768\n",
    "    size = len(input_ids)\n",
    "    if size > 512:\n",
    "        print(size)\n",
    "    empties = 512 - size\n",
    "    null_vector = [0] * label_size\n",
    "    for r_id in region_ids:\n",
    "        if r_id and not (r_id in regions):\n",
    "            print(r_id)\n",
    "    label = [regions[r_id] if r_id else null_vector for r_id in region_ids]\n",
    "    label += [null_vector] * empties\n",
    "    input_ids += [0] * empties\n",
    "    attention = [1] * size + [0] * empties\n",
    "    try:\n",
    "        torch.FloatTensor(label)\n",
    "    except:\n",
    "        print(regions[region_ids[1]])\n",
    "\n",
    "    return input_ids, attention, label\n",
    "\n",
    "def get_batch(batch_size, i):\n",
    "    tokens = get_batch_tokens(batch_size, i)\n",
    "    regions = get_batch_regions(tokens)\n",
    "    batch = [prepare_iteration(input_ids, region_ids, regions) for (input_ids, region_ids) in tokens]\n",
    "    input_ids, attention, label = zip(*batch)\n",
    "    return torch.IntTensor(input_ids), torch.FloatTensor(attention), torch.FloatTensor(label)\n",
    "\n",
    "\n",
    "def get_epoch_part(i0, batchs, batch_size):\n",
    "    return ((i, get_batch(batch_size, i)) for i in range(i0, batchs, 1))\n",
    "\n",
    "def tee(text):\n",
    "    print(f\"\\r{text}\\r\")\n",
    "    with open('train-resnet.log', 'a') as file:\n",
    "        file.write(f\"{text}\\n\")\n",
    "\n",
    "def log(title, start_time, sizes, epoch, i, mean_loss, loss):\n",
    "    _, num_epochs, batches = sizes\n",
    "    i += 1\n",
    "    dt = time.time() - start_time\n",
    "    elapsed = timedelta(seconds=int(dt))\n",
    "    remaining = timedelta(seconds=int(dt*(batches-i)/i))\n",
    "    text = f\"{title}: {elapsed}<{remaining} Epoch {epoch+1}/{num_epochs} - Batch {i}/{batches}, Loss: {mean_loss:.4f} {loss.item():.4f}\"\n",
    "    tee(text)\n",
    "\n",
    "def save(epoch, i):\n",
    "    \n",
    "    torch.save(model.state_dict(), f'searchnet-bcemodel-{epoch}-{i}.pt')\n",
    "    torch.save(optimizer.state_dict(), f'searchnet-bceoptimizer-{epoch}-{i}.pt')\n",
    "    set_last_batch(i, epoch)\n",
    "    tee(f'saving searchnet-bcemodel-{epoch}-{i}.pt\\n')\n",
    "\n",
    "def handle_train_batch(i, epoch, batch, performance, sizes):\n",
    "    input_ids, attention, labels = (c.to(device) for c in batch)\n",
    "    min_alpha, alpha, running_loss, start_time = performance\n",
    "    train_batchs, num_epochs, batches = sizes\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids, attention_mask=attention)\n",
    "    loss = loss_fn(outputs.last_hidden_state, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    current_alpha = min_alpha + alpha\n",
    "    running_loss = (1 - current_alpha) * running_loss + current_alpha * loss.item()\n",
    "    alpha *= .5\n",
    "    \n",
    "    log('training', start_time, sizes, epoch, i, running_loss, loss)\n",
    "    if i % 100 == 0:\n",
    "        save(epoch, i)\n",
    "    return running_loss, alpha\n",
    "    \n",
    "def handle_val_batch(i0, i, epoch, batch, start_time, cum_loss, sizes):\n",
    "    input_ids, attention, labels = (c.to(device) for c in batch)\n",
    "    _, num_epochs, batches = sizes\n",
    "\n",
    "    with torch.no_grad():            \n",
    "        outputs = model(input_ids, attention_mask=attention)\n",
    "        loss = loss_fn(outputs.last_hidden_state, labels)\n",
    "    \n",
    "    cum_loss += loss.item()\n",
    "    mean_loss = cum_loss / (i - i0 + 1)\n",
    "    \n",
    "    log('validation', start_time, sizes, epoch, i, mean_loss, loss)\n",
    "    return cum_loss\n",
    "    \n",
    "\n",
    "def handle_epoch(batch_size, num_epochs):\n",
    "    with get_cursor() as cursor:\n",
    "        epoch_size = next(cursor.execute('select count(*) from train_snippets'))[0]\n",
    "    batches = (epoch_size - 1) // batch_size + 1\n",
    "    train_batchs = 4 * batches // 5\n",
    "    i0, epoch0 = get_last_batch(train_batchs)\n",
    "    num_epochs += epoch0\n",
    "    i = i0 - 1\n",
    "    min_alpha = .2\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    alpha = 1 - min_alpha\n",
    "\n",
    "    model.train()\n",
    "    sizes = (train_batchs, num_epochs, batches)\n",
    "    tee('\\nTraining\\n')\n",
    "    for (i, batch) in get_epoch_part(i0, train_batchs, batch_size):\n",
    "        performance = (min_alpha, alpha, running_loss, start_time)\n",
    "        running_loss, alpha = handle_train_batch(i, epoch, batch, performance, sizes)\n",
    "    \n",
    "    \n",
    "    i0 = i + 1\n",
    "    save(epoch, i0)\n",
    "    \n",
    "    model.eval()\n",
    "    cum_loss = 0\n",
    "    tee('\\nValidation:\\n')\n",
    "    for (i, batch) in get_epoch_part(i0, batches, batch_size):\n",
    "        cum_loss = handle_val_batch(i0, i, epoch, batch, start_time, cum_loss, sizes)\n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59af576d-6e6b-4f4d-910d-ff23dab8cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_last_batch(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df189e27-706e-4ee0-81e2-ef57d1522b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training\n",
      "\n",
      "16 16\n",
      "training: 0:00:02<0:11:00 Epoch 1/10 - Batch 1/320, Loss: 0.2561 0.2561\n",
      "saving searchnet-bcemodel-0-0.pt\n",
      "\n",
      "16 16\n",
      "training: 0:00:04<0:13:14 Epoch 1/10 - Batch 2/320, Loss: 0.2455 0.2384\n",
      "16 16\n",
      "training: 0:00:06<0:12:18 Epoch 1/10 - Batch 3/320, Loss: 0.2385 0.2281\n",
      "16 16\n",
      "training: 0:00:09<0:12:08 Epoch 1/10 - Batch 4/320, Loss: 0.2298 0.2095\n",
      "16 16\n",
      "training: 0:00:11<0:11:49 Epoch 1/10 - Batch 5/320, Loss: 0.2227 0.2013\n",
      "16 16\n",
      "training: 0:00:13<0:11:39 Epoch 1/10 - Batch 6/320, Loss: 0.2140 0.1839\n",
      "16 16\n",
      "training: 0:00:15<0:11:29 Epoch 1/10 - Batch 7/320, Loss: 0.2057 0.1750\n",
      "16 16\n",
      "training: 0:00:17<0:11:20 Epoch 1/10 - Batch 8/320, Loss: 0.1978 0.1675\n",
      "16 16\n",
      "training: 0:00:19<0:11:10 Epoch 1/10 - Batch 9/320, Loss: 0.1905 0.1617\n",
      "16 16\n",
      "training: 0:00:21<0:11:04 Epoch 1/10 - Batch 10/320, Loss: 0.1818 0.1474\n",
      "16 16\n",
      "training: 0:00:23<0:10:57 Epoch 1/10 - Batch 11/320, Loss: 0.1742 0.1439\n",
      "16 16\n",
      "training: 0:00:25<0:10:51 Epoch 1/10 - Batch 12/320, Loss: 0.1661 0.1337\n",
      "16 16\n",
      "training: 0:00:27<0:10:46 Epoch 1/10 - Batch 13/320, Loss: 0.1583 0.1271\n",
      "16 16\n",
      "training: 0:00:29<0:10:41 Epoch 1/10 - Batch 14/320, Loss: 0.1505 0.1193\n",
      "16 16\n",
      "training: 0:00:31<0:10:36 Epoch 1/10 - Batch 15/320, Loss: 0.1431 0.1139\n",
      "16 16\n",
      "training: 0:00:33<0:10:32 Epoch 1/10 - Batch 16/320, Loss: 0.1359 0.1068\n",
      "16 16\n",
      "training: 0:00:35<0:10:29 Epoch 1/10 - Batch 17/320, Loss: 0.1284 0.0984\n",
      "16 16\n",
      "training: 0:00:37<0:10:27 Epoch 1/10 - Batch 18/320, Loss: 0.1213 0.0928\n",
      "16 16\n",
      "training: 0:00:39<0:10:26 Epoch 1/10 - Batch 19/320, Loss: 0.1134 0.0822\n",
      "16 16\n",
      "training: 0:00:41<0:10:23 Epoch 1/10 - Batch 20/320, Loss: 0.1069 0.0808\n",
      "16 16\n",
      "training: 0:00:43<0:10:21 Epoch 1/10 - Batch 21/320, Loss: 0.1005 0.0746\n",
      "16 16\n",
      "training: 0:00:45<0:10:18 Epoch 1/10 - Batch 22/320, Loss: 0.0951 0.0735\n",
      "16 16\n",
      "training: 0:00:47<0:10:16 Epoch 1/10 - Batch 23/320, Loss: 0.0899 0.0690\n",
      "16 16\n",
      "training: 0:00:49<0:10:14 Epoch 1/10 - Batch 24/320, Loss: 0.0852 0.0668\n",
      "16 16\n",
      "training: 0:00:51<0:10:13 Epoch 1/10 - Batch 25/320, Loss: 0.0809 0.0635\n",
      "16 16\n",
      "training: 0:00:54<0:10:11 Epoch 1/10 - Batch 26/320, Loss: 0.0771 0.0618\n",
      "16 16\n",
      "training: 0:00:56<0:10:10 Epoch 1/10 - Batch 27/320, Loss: 0.0737 0.0601\n",
      "16 16\n",
      "training: 0:00:58<0:10:09 Epoch 1/10 - Batch 28/320, Loss: 0.0713 0.0615\n",
      "16 16\n",
      "training: 0:01:00<0:10:07 Epoch 1/10 - Batch 29/320, Loss: 0.0699 0.0646\n",
      "16 16\n",
      "training: 0:01:02<0:10:06 Epoch 1/10 - Batch 30/320, Loss: 0.0673 0.0570\n",
      "16 16\n",
      "training: 0:01:04<0:10:04 Epoch 1/10 - Batch 31/320, Loss: 0.0657 0.0590\n",
      "16 16\n",
      "training: 0:01:07<0:10:04 Epoch 1/10 - Batch 32/320, Loss: 0.0634 0.0543\n",
      "16 16\n",
      "training: 0:01:09<0:10:03 Epoch 1/10 - Batch 33/320, Loss: 0.0622 0.0573\n",
      "16 16\n",
      "training: 0:01:11<0:10:02 Epoch 1/10 - Batch 34/320, Loss: 0.0614 0.0585\n",
      "16 16\n",
      "training: 0:01:13<0:10:01 Epoch 1/10 - Batch 35/320, Loss: 0.0615 0.0617\n",
      "16 16\n",
      "training: 0:01:16<0:09:59 Epoch 1/10 - Batch 36/320, Loss: 0.0609 0.0586\n",
      "16 16\n",
      "training: 0:01:18<0:09:56 Epoch 1/10 - Batch 37/320, Loss: 0.0596 0.0546\n",
      "16 16\n",
      "training: 0:01:20<0:09:54 Epoch 1/10 - Batch 38/320, Loss: 0.0592 0.0572\n",
      "16 16\n",
      "training: 0:01:22<0:09:52 Epoch 1/10 - Batch 39/320, Loss: 0.0587 0.0571\n",
      "16 16\n",
      "training: 0:01:24<0:09:49 Epoch 1/10 - Batch 40/320, Loss: 0.0579 0.0546\n",
      "16 16\n",
      "training: 0:01:26<0:09:47 Epoch 1/10 - Batch 41/320, Loss: 0.0574 0.0555\n",
      "16 16\n",
      "training: 0:01:28<0:09:44 Epoch 1/10 - Batch 42/320, Loss: 0.0573 0.0568\n",
      "16 16\n",
      "training: 0:01:30<0:09:41 Epoch 1/10 - Batch 43/320, Loss: 0.0567 0.0543\n",
      "16 16\n",
      "training: 0:01:32<0:09:38 Epoch 1/10 - Batch 44/320, Loss: 0.0556 0.0513\n",
      "16 16\n",
      "training: 0:01:34<0:09:36 Epoch 1/10 - Batch 45/320, Loss: 0.0547 0.0510\n",
      "16 16\n",
      "training: 0:01:36<0:09:34 Epoch 1/10 - Batch 46/320, Loss: 0.0547 0.0548\n",
      "16 16\n",
      "training: 0:01:38<0:09:31 Epoch 1/10 - Batch 47/320, Loss: 0.0544 0.0533\n",
      "16 16\n",
      "training: 0:01:40<0:09:28 Epoch 1/10 - Batch 48/320, Loss: 0.0546 0.0553\n",
      "16 16\n",
      "training: 0:01:42<0:09:24 Epoch 1/10 - Batch 49/320, Loss: 0.0553 0.0579\n",
      "16 16\n",
      "training: 0:01:44<0:09:21 Epoch 1/10 - Batch 50/320, Loss: 0.0552 0.0547\n",
      "16 16\n",
      "training: 0:01:45<0:09:18 Epoch 1/10 - Batch 51/320, Loss: 0.0548 0.0536\n",
      "16 16\n",
      "training: 0:01:47<0:09:16 Epoch 1/10 - Batch 52/320, Loss: 0.0544 0.0526\n",
      "16 16\n",
      "training: 0:01:49<0:09:13 Epoch 1/10 - Batch 53/320, Loss: 0.0546 0.0556\n",
      "16 16\n",
      "training: 0:01:51<0:09:11 Epoch 1/10 - Batch 54/320, Loss: 0.0545 0.0542\n",
      "16 16\n",
      "training: 0:01:53<0:09:08 Epoch 1/10 - Batch 55/320, Loss: 0.0544 0.0539\n",
      "16 16\n",
      "training: 0:01:55<0:09:06 Epoch 1/10 - Batch 56/320, Loss: 0.0541 0.0530\n",
      "16 16\n",
      "training: 0:01:57<0:09:03 Epoch 1/10 - Batch 57/320, Loss: 0.0539 0.0529\n",
      "16 16\n",
      "training: 0:01:59<0:09:01 Epoch 1/10 - Batch 58/320, Loss: 0.0532 0.0505\n",
      "16 16\n",
      "training: 0:02:01<0:08:59 Epoch 1/10 - Batch 59/320, Loss: 0.0535 0.0544\n",
      "16 16\n",
      "training: 0:02:04<0:08:57 Epoch 1/10 - Batch 60/320, Loss: 0.0524 0.0482\n",
      "16 16\n",
      "training: 0:02:06<0:08:55 Epoch 1/10 - Batch 61/320, Loss: 0.0519 0.0497\n",
      "16 16\n",
      "training: 0:02:08<0:08:53 Epoch 1/10 - Batch 62/320, Loss: 0.0513 0.0488\n",
      "16 16\n",
      "training: 0:02:10<0:08:51 Epoch 1/10 - Batch 63/320, Loss: 0.0505 0.0473\n",
      "16 16\n",
      "training: 0:02:12<0:08:49 Epoch 1/10 - Batch 64/320, Loss: 0.0497 0.0466\n",
      "16 16\n",
      "training: 0:02:14<0:08:47 Epoch 1/10 - Batch 65/320, Loss: 0.0491 0.0468\n",
      "16 16\n",
      "training: 0:02:16<0:08:46 Epoch 1/10 - Batch 66/320, Loss: 0.0488 0.0477\n",
      "16 16\n",
      "training: 0:02:18<0:08:44 Epoch 1/10 - Batch 67/320, Loss: 0.0486 0.0474\n",
      "16 16\n",
      "training: 0:02:20<0:08:42 Epoch 1/10 - Batch 68/320, Loss: 0.0479 0.0453\n",
      "16 16\n",
      "training: 0:02:23<0:08:40 Epoch 1/10 - Batch 69/320, Loss: 0.0475 0.0461\n",
      "16 16\n",
      "training: 0:02:25<0:08:38 Epoch 1/10 - Batch 70/320, Loss: 0.0469 0.0443\n",
      "16 16\n",
      "training: 0:02:27<0:08:36 Epoch 1/10 - Batch 71/320, Loss: 0.0466 0.0456\n",
      "16 16\n",
      "training: 0:02:29<0:08:34 Epoch 1/10 - Batch 72/320, Loss: 0.0462 0.0445\n",
      "16 16\n",
      "training: 0:02:31<0:08:32 Epoch 1/10 - Batch 73/320, Loss: 0.0453 0.0416\n",
      "16 16\n",
      "training: 0:02:33<0:08:29 Epoch 1/10 - Batch 74/320, Loss: 0.0448 0.0430\n",
      "16 16\n",
      "training: 0:02:35<0:08:27 Epoch 1/10 - Batch 75/320, Loss: 0.0446 0.0437\n",
      "16 16\n",
      "training: 0:02:37<0:08:25 Epoch 1/10 - Batch 76/320, Loss: 0.0443 0.0432\n",
      "16 16\n",
      "training: 0:02:39<0:08:23 Epoch 1/10 - Batch 77/320, Loss: 0.0439 0.0425\n",
      "16 16\n",
      "training: 0:02:41<0:08:21 Epoch 1/10 - Batch 78/320, Loss: 0.0437 0.0429\n",
      "16 16\n",
      "training: 0:02:43<0:08:18 Epoch 1/10 - Batch 79/320, Loss: 0.0435 0.0425\n",
      "16 16\n",
      "training: 0:02:45<0:08:16 Epoch 1/10 - Batch 80/320, Loss: 0.0431 0.0416\n",
      "16 16\n",
      "training: 0:02:47<0:08:14 Epoch 1/10 - Batch 81/320, Loss: 0.0427 0.0411\n",
      "16 16\n",
      "training: 0:02:49<0:08:12 Epoch 1/10 - Batch 82/320, Loss: 0.0425 0.0416\n",
      "16 16\n",
      "training: 0:02:51<0:08:10 Epoch 1/10 - Batch 83/320, Loss: 0.0425 0.0425\n",
      "16 16\n",
      "training: 0:02:53<0:08:08 Epoch 1/10 - Batch 84/320, Loss: 0.0423 0.0417\n",
      "16 16\n",
      "training: 0:02:55<0:08:06 Epoch 1/10 - Batch 85/320, Loss: 0.0424 0.0427\n",
      "16 16\n",
      "training: 0:02:58<0:08:04 Epoch 1/10 - Batch 86/320, Loss: 0.0421 0.0409\n",
      "16 16\n",
      "training: 0:03:00<0:08:02 Epoch 1/10 - Batch 87/320, Loss: 0.0415 0.0390\n",
      "16 16\n",
      "training: 0:03:02<0:08:01 Epoch 1/10 - Batch 88/320, Loss: 0.0414 0.0410\n",
      "16 16\n",
      "training: 0:03:04<0:07:59 Epoch 1/10 - Batch 89/320, Loss: 0.0407 0.0378\n",
      "16 16\n",
      "training: 0:03:07<0:07:57 Epoch 1/10 - Batch 90/320, Loss: 0.0405 0.0398\n",
      "16 16\n",
      "training: 0:03:09<0:07:56 Epoch 1/10 - Batch 91/320, Loss: 0.0403 0.0395\n",
      "16 16\n",
      "training: 0:03:11<0:07:54 Epoch 1/10 - Batch 92/320, Loss: 0.0400 0.0386\n",
      "16 16\n",
      "training: 0:03:13<0:07:52 Epoch 1/10 - Batch 93/320, Loss: 0.0398 0.0392\n",
      "16 16\n",
      "training: 0:03:15<0:07:50 Epoch 1/10 - Batch 94/320, Loss: 0.0396 0.0389\n",
      "16 16\n",
      "training: 0:03:17<0:07:48 Epoch 1/10 - Batch 95/320, Loss: 0.0394 0.0385\n",
      "16 16\n",
      "training: 0:03:19<0:07:46 Epoch 1/10 - Batch 96/320, Loss: 0.0394 0.0393\n",
      "16 16\n",
      "training: 0:03:21<0:07:43 Epoch 1/10 - Batch 97/320, Loss: 0.0394 0.0397\n",
      "16 16\n",
      "training: 0:03:23<0:07:41 Epoch 1/10 - Batch 98/320, Loss: 0.0392 0.0384\n",
      "16 16\n",
      "training: 0:03:25<0:07:39 Epoch 1/10 - Batch 99/320, Loss: 0.0389 0.0375\n",
      "16 16\n",
      "training: 0:03:27<0:07:37 Epoch 1/10 - Batch 100/320, Loss: 0.0389 0.0389\n",
      "16 16\n",
      "training: 0:03:29<0:07:34 Epoch 1/10 - Batch 101/320, Loss: 0.0389 0.0390\n",
      "saving searchnet-bcemodel-0-100.pt\n",
      "\n",
      "16 16\n",
      "training: 0:03:42<0:07:54 Epoch 1/10 - Batch 102/320, Loss: 0.0393 0.0407\n",
      "16 16\n",
      "training: 0:03:44<0:07:52 Epoch 1/10 - Batch 103/320, Loss: 0.0398 0.0421\n",
      "16 16\n",
      "training: 0:03:46<0:07:49 Epoch 1/10 - Batch 104/320, Loss: 0.0404 0.0427\n",
      "16 16\n",
      "training: 0:03:48<0:07:47 Epoch 1/10 - Batch 105/320, Loss: 0.0408 0.0424\n",
      "16 16\n",
      "training: 0:03:50<0:07:44 Epoch 1/10 - Batch 106/320, Loss: 0.0412 0.0427\n",
      "16 16\n",
      "training: 0:03:52<0:07:42 Epoch 1/10 - Batch 107/320, Loss: 0.0415 0.0428\n",
      "16 16\n",
      "training: 0:03:54<0:07:40 Epoch 1/10 - Batch 108/320, Loss: 0.0411 0.0394\n",
      "16 16\n",
      "training: 0:03:56<0:07:37 Epoch 1/10 - Batch 109/320, Loss: 0.0414 0.0427\n",
      "16 16\n",
      "training: 0:03:58<0:07:35 Epoch 1/10 - Batch 110/320, Loss: 0.0417 0.0430\n",
      "16 16\n",
      "training: 0:04:00<0:07:33 Epoch 1/10 - Batch 111/320, Loss: 0.0415 0.0404\n",
      "16 16\n",
      "training: 0:04:02<0:07:30 Epoch 1/10 - Batch 112/320, Loss: 0.0412 0.0400\n",
      "16 16\n",
      "training: 0:04:04<0:07:28 Epoch 1/10 - Batch 113/320, Loss: 0.0408 0.0392\n",
      "16 16\n",
      "training: 0:04:06<0:07:25 Epoch 1/10 - Batch 114/320, Loss: 0.0406 0.0401\n",
      "16 16\n",
      "training: 0:04:08<0:07:23 Epoch 1/10 - Batch 115/320, Loss: 0.0405 0.0401\n",
      "16 16\n",
      "training: 0:04:10<0:07:21 Epoch 1/10 - Batch 116/320, Loss: 0.0409 0.0425\n",
      "16 16\n",
      "training: 0:04:13<0:07:19 Epoch 1/10 - Batch 117/320, Loss: 0.0408 0.0403\n",
      "16 16\n",
      "training: 0:04:15<0:07:16 Epoch 1/10 - Batch 118/320, Loss: 0.0407 0.0402\n",
      "16 16\n",
      "training: 0:04:17<0:07:14 Epoch 1/10 - Batch 119/320, Loss: 0.0406 0.0403\n",
      "16 16\n",
      "training: 0:04:19<0:07:12 Epoch 1/10 - Batch 120/320, Loss: 0.0403 0.0392\n",
      "16 16\n",
      "training: 0:04:21<0:07:09 Epoch 1/10 - Batch 121/320, Loss: 0.0401 0.0394\n",
      "16 16\n",
      "training: 0:04:23<0:07:07 Epoch 1/10 - Batch 122/320, Loss: 0.0398 0.0386\n",
      "16 16\n",
      "training: 0:04:25<0:07:05 Epoch 1/10 - Batch 123/320, Loss: 0.0395 0.0381\n",
      "16 16\n",
      "training: 0:04:27<0:07:02 Epoch 1/10 - Batch 124/320, Loss: 0.0397 0.0406\n",
      "16 16\n",
      "training: 0:04:29<0:07:00 Epoch 1/10 - Batch 125/320, Loss: 0.0398 0.0402\n",
      "16 16\n",
      "training: 0:04:31<0:06:57 Epoch 1/10 - Batch 126/320, Loss: 0.0396 0.0387\n",
      "16 16\n",
      "training: 0:04:33<0:06:55 Epoch 1/10 - Batch 127/320, Loss: 0.0398 0.0407\n",
      "16 16\n",
      "training: 0:04:35<0:06:53 Epoch 1/10 - Batch 128/320, Loss: 0.0397 0.0394\n",
      "16 16\n",
      "training: 0:04:37<0:06:50 Epoch 1/10 - Batch 129/320, Loss: 0.0400 0.0409\n",
      "16 16\n",
      "training: 0:04:39<0:06:48 Epoch 1/10 - Batch 130/320, Loss: 0.0399 0.0396\n",
      "16 16\n",
      "training: 0:04:41<0:06:45 Epoch 1/10 - Batch 131/320, Loss: 0.0402 0.0414\n",
      "16 16\n",
      "training: 0:04:43<0:06:43 Epoch 1/10 - Batch 132/320, Loss: 0.0405 0.0415\n",
      "16 16\n",
      "training: 0:04:45<0:06:41 Epoch 1/10 - Batch 133/320, Loss: 0.0405 0.0405\n",
      "16 16\n",
      "training: 0:04:47<0:06:38 Epoch 1/10 - Batch 134/320, Loss: 0.0403 0.0396\n",
      "16 16\n",
      "training: 0:04:49<0:06:36 Epoch 1/10 - Batch 135/320, Loss: 0.0404 0.0408\n",
      "16 16\n",
      "training: 0:04:51<0:06:33 Epoch 1/10 - Batch 136/320, Loss: 0.0402 0.0395\n",
      "16 16\n",
      "training: 0:04:53<0:06:31 Epoch 1/10 - Batch 137/320, Loss: 0.0407 0.0427\n",
      "16 16\n",
      "training: 0:04:55<0:06:29 Epoch 1/10 - Batch 138/320, Loss: 0.0407 0.0408\n",
      "16 16\n",
      "training: 0:04:57<0:06:26 Epoch 1/10 - Batch 139/320, Loss: 0.0408 0.0411\n",
      "16 16\n",
      "training: 0:04:59<0:06:24 Epoch 1/10 - Batch 140/320, Loss: 0.0406 0.0396\n",
      "16 16\n",
      "training: 0:05:01<0:06:22 Epoch 1/10 - Batch 141/320, Loss: 0.0406 0.0406\n",
      "16 16\n",
      "training: 0:05:03<0:06:20 Epoch 1/10 - Batch 142/320, Loss: 0.0407 0.0414\n",
      "16 16\n",
      "training: 0:05:05<0:06:17 Epoch 1/10 - Batch 143/320, Loss: 0.0405 0.0397\n",
      "16 16\n",
      "training: 0:05:07<0:06:15 Epoch 1/10 - Batch 144/320, Loss: 0.0401 0.0386\n",
      "16 16\n",
      "training: 0:05:09<0:06:13 Epoch 1/10 - Batch 145/320, Loss: 0.0395 0.0370\n",
      "16 16\n",
      "training: 0:05:11<0:06:11 Epoch 1/10 - Batch 146/320, Loss: 0.0395 0.0395\n",
      "16 16\n",
      "training: 0:05:14<0:06:09 Epoch 1/10 - Batch 147/320, Loss: 0.0392 0.0380\n",
      "16 16\n",
      "training: 0:05:16<0:06:07 Epoch 1/10 - Batch 148/320, Loss: 0.0388 0.0369\n",
      "16 16\n",
      "training: 0:05:18<0:06:05 Epoch 1/10 - Batch 149/320, Loss: 0.0385 0.0373\n",
      "16 16\n",
      "training: 0:05:20<0:06:03 Epoch 1/10 - Batch 150/320, Loss: 0.0381 0.0366\n",
      "16 16\n",
      "training: 0:05:22<0:06:01 Epoch 1/10 - Batch 151/320, Loss: 0.0376 0.0354\n",
      "16 16\n",
      "training: 0:05:24<0:05:59 Epoch 1/10 - Batch 152/320, Loss: 0.0376 0.0379\n",
      "16 16\n",
      "training: 0:05:26<0:05:56 Epoch 1/10 - Batch 153/320, Loss: 0.0376 0.0375\n",
      "16 16\n",
      "training: 0:05:29<0:05:54 Epoch 1/10 - Batch 154/320, Loss: 0.0377 0.0380\n",
      "16 16\n",
      "training: 0:05:31<0:05:52 Epoch 1/10 - Batch 155/320, Loss: 0.0376 0.0374\n",
      "16 16\n",
      "training: 0:05:33<0:05:50 Epoch 1/10 - Batch 156/320, Loss: 0.0374 0.0366\n",
      "16 16\n",
      "training: 0:05:35<0:05:48 Epoch 1/10 - Batch 157/320, Loss: 0.0375 0.0376\n",
      "16 16\n",
      "training: 0:05:37<0:05:46 Epoch 1/10 - Batch 158/320, Loss: 0.0374 0.0374\n",
      "16 16\n",
      "training: 0:05:39<0:05:43 Epoch 1/10 - Batch 159/320, Loss: 0.0377 0.0386\n",
      "16 16\n",
      "training: 0:05:41<0:05:41 Epoch 1/10 - Batch 160/320, Loss: 0.0375 0.0368\n",
      "16 16\n",
      "training: 0:05:43<0:05:39 Epoch 1/10 - Batch 161/320, Loss: 0.0377 0.0387\n",
      "16 16\n",
      "training: 0:05:45<0:05:37 Epoch 1/10 - Batch 162/320, Loss: 0.0375 0.0365\n",
      "16 16\n",
      "training: 0:05:47<0:05:35 Epoch 1/10 - Batch 163/320, Loss: 0.0375 0.0374\n",
      "16 16\n",
      "training: 0:05:50<0:05:33 Epoch 1/10 - Batch 164/320, Loss: 0.0374 0.0370\n",
      "16 16\n",
      "training: 0:05:52<0:05:30 Epoch 1/10 - Batch 165/320, Loss: 0.0374 0.0373\n",
      "16 16\n",
      "training: 0:05:54<0:05:28 Epoch 1/10 - Batch 166/320, Loss: 0.0372 0.0368\n",
      "16 16\n",
      "training: 0:05:56<0:05:26 Epoch 1/10 - Batch 167/320, Loss: 0.0373 0.0376\n",
      "16 16\n",
      "training: 0:05:58<0:05:24 Epoch 1/10 - Batch 168/320, Loss: 0.0376 0.0385\n",
      "16 16\n",
      "training: 0:06:00<0:05:22 Epoch 1/10 - Batch 169/320, Loss: 0.0376 0.0379\n",
      "16 16\n",
      "training: 0:06:02<0:05:19 Epoch 1/10 - Batch 170/320, Loss: 0.0379 0.0389\n",
      "16 16\n",
      "training: 0:06:04<0:05:17 Epoch 1/10 - Batch 171/320, Loss: 0.0382 0.0396\n",
      "16 16\n",
      "training: 0:06:06<0:05:15 Epoch 1/10 - Batch 172/320, Loss: 0.0385 0.0395\n",
      "16 16\n",
      "training: 0:06:08<0:05:13 Epoch 1/10 - Batch 173/320, Loss: 0.0382 0.0369\n",
      "16 16\n",
      "training: 0:06:10<0:05:11 Epoch 1/10 - Batch 174/320, Loss: 0.0383 0.0387\n",
      "16 16\n",
      "training: 0:06:13<0:05:09 Epoch 1/10 - Batch 175/320, Loss: 0.0382 0.0377\n",
      "16 16\n",
      "training: 0:06:15<0:05:06 Epoch 1/10 - Batch 176/320, Loss: 0.0382 0.0386\n",
      "16 16\n",
      "training: 0:06:17<0:05:04 Epoch 1/10 - Batch 177/320, Loss: 0.0383 0.0385\n",
      "16 16\n",
      "training: 0:06:18<0:05:02 Epoch 1/10 - Batch 178/320, Loss: 0.0381 0.0374\n",
      "16 16\n",
      "training: 0:06:20<0:05:00 Epoch 1/10 - Batch 179/320, Loss: 0.0381 0.0382\n",
      "16 16\n",
      "training: 0:06:22<0:04:57 Epoch 1/10 - Batch 180/320, Loss: 0.0376 0.0356\n",
      "16 16\n",
      "training: 0:06:24<0:04:55 Epoch 1/10 - Batch 181/320, Loss: 0.0376 0.0377\n",
      "16 16\n",
      "training: 0:06:26<0:04:53 Epoch 1/10 - Batch 182/320, Loss: 0.0373 0.0359\n",
      "16 16\n",
      "training: 0:06:28<0:04:51 Epoch 1/10 - Batch 183/320, Loss: 0.0366 0.0338\n",
      "16 16\n",
      "training: 0:06:30<0:04:48 Epoch 1/10 - Batch 184/320, Loss: 0.0360 0.0336\n",
      "16 16\n",
      "training: 0:06:32<0:04:46 Epoch 1/10 - Batch 185/320, Loss: 0.0356 0.0343\n",
      "16 16\n",
      "training: 0:06:34<0:04:44 Epoch 1/10 - Batch 186/320, Loss: 0.0355 0.0351\n",
      "16 16\n",
      "training: 0:06:36<0:04:42 Epoch 1/10 - Batch 187/320, Loss: 0.0350 0.0331\n",
      "16 16\n",
      "training: 0:06:38<0:04:40 Epoch 1/10 - Batch 188/320, Loss: 0.0347 0.0333\n",
      "16 16\n",
      "training: 0:06:40<0:04:37 Epoch 1/10 - Batch 189/320, Loss: 0.0347 0.0349\n",
      "16 16\n",
      "training: 0:06:42<0:04:35 Epoch 1/10 - Batch 190/320, Loss: 0.0343 0.0327\n",
      "16 16\n",
      "training: 0:06:44<0:04:33 Epoch 1/10 - Batch 191/320, Loss: 0.0344 0.0345\n",
      "16 16\n",
      "training: 0:06:46<0:04:31 Epoch 1/10 - Batch 192/320, Loss: 0.0341 0.0331\n",
      "16 16\n",
      "[0.013151898, -0.0065877372, 0.38858342, 0.076819435, 0.11232253, -0.03194341, -0.03213625, 0.02077267, 0.04078984, 0.038005404, 0.045091685, 0.06530587, 0.23966162, 0.07793768, 0.0022339218, 0.055855274, -0.008449703, 0.0084213605, 0.035086557, -0.018956592, 0.10077634, 0.06155828, 0.1464656, 0.031747587, 0.022254653, 0.018229924, 0.04312251, 0.07743824, 0.021213109, 0.09293179, 0.038433313, 0.05750882, -0.0050063124, 0.09522876, -0.005503984, 0.07147003, 0.05276395, 0.047505338, 0.012520332, 0.18131846, 0.04049597, -0.0767394, 0.02504616, 0.044573393, 0.04897986, 0.07864716, 0.1359534, 0.03928499, -0.28431588, 0.02649669, 0.08124789, 0.061596148, 0.02404466, 0.11018192, 0.078822486, 0.03606495, 0.2295344, 0.08645843, 0.00115594, 0.038008634, 0.07655685, 0.050923154, 0.020252548, -0.0008806152, 0.08610052, 0.30596083, -0.036612075, -0.018606318, 0.071798764, 0.025055857, 0.02439856, 0.048746612, -0.083166435, 0.031446252, 0.07162303, 0.08688235, 0.030069605, -0.12936437, 0.08072615, 0.08159535, 0.08522, 0.003620483, 0.056496643, 0.020826299, 0.05614137, -0.0146752205, 0.2621453, 0.056093514, 0.077651106, 0.012811016, 0.09513962, 0.06469402, 0.020579591, 0.04314513, 0.034070123, 0.012682062, 0.04783851, -0.07431057, -0.39934042, 0.03872087, 0.071723916, 0.29216045, 0.05379301, 0.089880936, 0.029862195, 0.09654384, 0.02613871, -0.0090735685, 0.015245513, 0.06776832, 0.026467675, 0.06623667, 0.07440709, -0.06378104, 0.03972721, 0.016617548, 0.07656966, 0.022109887, -0.08913328, 0.04259132, 0.012293577, 0.02200683, 0.10687493, 0.025830058, 0.01946753, 0.034973554, -0.008777152, -0.0019019472, 0.028184388, 0.004863912, -0.10420946, -4.153053e-05, -0.09179247, 0.09813173, 0.023396455, 0.059373323, 0.054740977, 0.09149017, 0.054916393, 0.31475925, 0.022615056, 0.077015705, 0.06153192, 0.015306287, 0.019673403, 0.03148096, 0.080917396, 0.054570194, 0.08890385, 0.10983323, 0.04123886, 0.03237636, 0.039117556, 0.044523552, 0.027223906, 0.089377016, 0.04412381, 0.12275105, 0.07339793, 0.10278019, 0.057569765, 0.049191833, 0.043778006, 0.06156673, 0.047456954, 0.17802231, 0.05363316, 0.11459629, 0.07164595, -0.022136267, 0.1444236, 0.01948848, 0.11411245, -0.02154706, 0.052289266, -0.00047455914, 0.00792974, 0.017363517, 0.04180154, 0.2818448, 0.051178105, 0.059224136, 0.060767155, 0.06418481, 0.019167738, 0.04765159, -0.008620317, 0.0459249, 0.030632647, 0.011070604, 0.063605, 0.16549495, 0.018365, 0.025604429, 0.007944963, -0.013977565, 0.07086158, -0.4957217, 0.044518683, 0.034083456, 0.020659039, -0.015018265, -0.03166922, 0.04694871, 0.09064524, 0.09588906, 0.009501648, 0.02852185, -0.08088686, 0.029343499, 0.09564179, 0.06256943, 0.12838762, 0.063022956, 0.077943124, 0.014045888, 0.03360721, -0.24990752, 0.051914196, -0.013017973, 0.045224562, 0.09969922, 0.106315084, -0.008121085, 0.20256509, 0.07332374, 0.048544377, 0.46725932, 0.07718877, 0.06689647, -0.37011313, 0.019791698, 0.14157775, 0.10300664, 0.057346933, 0.03923712, 0.02589735, -0.1300462, 0.08251927, 0.091500506, -0.06422071, 0.35531968, 0.051167343, 0.13031937, 0.00959978, 0.024155525, 0.056367856, 0.011806604, 0.030734733, 0.035850152, 0.107823394, 0.01962474, 0.051332586, -0.020700973, 0.030085186, 0.09634357, 0.042927578, -0.2341593, 0.07047517, 0.040103234, -0.040515117, 0.017713757, 0.0026623807, 0.039737385, -0.03813959, 0.04274082, -0.027728776, 0.0807065, 0.024250047, 0.07743758, 0.061470978, -0.5845346, 0.14193985, 0.052815087, 0.5838204, 0.22978012, 0.053114414, 0.082908444, -0.0019334985, 0.08869328, 0.03965168, 0.02808831, 0.061539605, 0.004051862, 0.062480643, -0.010681106, 0.017747361, -0.016738525, 0.02870141, -0.2759499, 0.05800331, -0.021692721, 0.009218187, 0.051248267, 0.048551798, 0.029700477, 0.104002774, 0.04950955, -0.07199291, 0.02940465, 0.06288211, 0.1029669, 0.1191548, 0.071781725, -0.027164847, 0.063100964, 0.09734773, 0.07798111, 0.11431189, 0.08145291, 0.026170278, 0.03899011, 0.11107896, 0.07007278, 0.0025026912, 0.02296776, 0.042910844, 0.06576888, 0.007155004, 0.011940767, 0.066663764, 0.054722503, -0.036222324, 0.7704058, 0.064299725, 0.024448473, -0.0086641265, 0.058549374, 0.081719875, 0.023072774, 0.062979184, 0.065637305, 0.054739233, 0.035270672, -0.0014113084, 0.03209997, 0.34981903, -0.008666813, 0.12103045, 0.005640072, 0.078786224, 0.04682305, 0.032622565, 0.014260426, 0.037940387, 0.07090798, 0.08343387, -0.011046048, 0.058491252, 0.13975947, -0.017222876, 0.0432906, 0.026342217, -0.056281526, 0.0480594, 0.08034883, -0.020341778, 0.07587234, 0.19007055, 0.11069909, 0.01949158, 0.10946453, 0.057036463, 0.10392854, 0.04071404, 0.07781373, -0.09940849, 0.02886383, 0.124312386, 0.06858815, 0.05048819, 0.073998205, 0.004490306, -0.014683104, 0.09916258, 0.6551555, 0.100554265, -0.040167406, 0.04979286, 0.06746139, -0.14308096, 0.08436424, 0.07710871, -0.09971538, -0.019497441, 0.0148653565, -0.03592078, 0.019112552, -0.02130192, -0.2637869, 0.033326525, 0.042218607, 0.07393242, 0.02111721, 0.15116112, 0.049562827, 0.021886881, 0.075125374, 0.05387672, 0.17518727, -0.004518393, 0.016851114, 0.05827936, 0.10811536, -0.013142141, 0.05472051, 0.07638942, 0.056030646, 0.053198073, -0.01844584, -0.041321125, 0.030400299, -0.09997006, 0.0008197983, 0.061010245, 0.2346249, 0.082318574, 0.056887638, 0.076728374, 0.11106731, 0.047046807, 1.0984545, 0.056766365, 0.05454555, 0.08091949, 0.073469475, 0.074313566, 0.16509824, 0.06867563, 0.016096517, -0.026779748, 0.09425424, 0.09025445, 0.07209405, 0.0506393, 0.003248581, -0.042965658, 0.016898468, 0.080100134, 0.013171138, 0.041993476, 0.16540222, 0.072423354, 0.10852818, 0.09731752, 0.00963706, 0.025648447, -0.035630107, 0.014231028, 0.08582779, 0.018502455, 0.050069556, 0.16240108, -0.16010189, 0.07756925, 0.028849212, 0.016425822, 0.11600541, 0.04976034, -0.008508367, 0.030373797, 0.08803102, 0.059536483, 0.24227689, -0.13383026, 0.067547575, 0.052024797, 0.027804617, 0.035877787, 0.027259015, 0.0390351, -0.010664936, 0.1592091, 0.05505692, 0.0027693212, 0.08664748, -0.003747614, 0.049358945, 0.0751221, 0.32475376, 0.18346143, -0.054961763, 0.16165647, -0.3803939, 0.07840695, 0.1253657, 0.046603788, 0.09549315, -0.01048667, 0.13619253, 0.008633157, 0.002129465, 0.040446527, 0.07383834, 0.37895814, -0.005363936, -0.0047711604, 0.029532988, 0.02856051, 0.26416725, 0.16441694, 0.0021810147, 0.07165793, 0.06984148, 0.01825976, 0.055299856, 0.09521634, 0.16051476, 0.07017165, 0.1016509, 0.0384084, 0.13309762, 0.0346807, -0.2566177, -0.08680886, 0.05074693, 0.07771496, 0.00317204, 0.07105772, 0.014134497, 0.07785982, -0.052759215, -0.008459954, 0.34956565, 0.13552327, 0.51001495, 0.05865218, 0.048117496, 0.07772637, 0.04870234, 0.10850966, 0.09338102, 0.19917217, -0.24794887, 0.017799027, -0.019730154, 0.0038214796, 0.040766474, 0.021886665, 0.045412775, 0.018295472, 0.09234738, 0.11702044, 0.03855011, 0.09231537, 0.08661798, 0.055654474, 0.05593293, 0.24105081, 0.025868347, 0.06115474, 0.16207066, -0.027699463, 0.098569214, 0.026885739, -0.1811267, 0.13153853, 0.07049246, 0.10702841, 0.05131993, 0.09002798, 0.06826248, 0.23646297, -0.060423166, -0.033442926, 0.032534275, -0.017922543, 0.003180084, 0.19061966, 0.046874385, 0.05085512, 0.060712162, -0.08347575, -0.005014307, 0.026555676, 0.039337683, -0.029697353, 0.074084684, 0.049667116, 0.045902953, 0.03198064, -0.04007235, -0.05959307, 0.09523763, 0.05350435, 0.031036885, 0.14996181, 0.062122483, 0.21974629, 0.041608207, -0.14944252, -0.041145276, 0.03630204, 0.10765783, 0.046919107, 0.03792702, 0.08842424, -0.032864597, 0.12451212, 0.029747717, 0.057275165, 0.061539274, 0.46010306, 0.071597755, 0.050418124, -0.029224355, 0.030009389, -0.034183078, -0.0018189145, 0.0062234723, -0.076477036, 0.02378945, 0.090064846, 0.05401984, 0.023981154, 0.038619243, 0.3002956, 0.043496758, 0.028615661, 0.073047616, 0.14912598, 0.052149065, 0.11590914, 0.09745954, -0.03569807, 0.028004559, -0.04857598, 0.096584484, -0.30645406, -0.010637131, 0.13579609, -0.006230251, 0.014969585, 0.07675727, 0.04076069, 0.04657829, 0.08662825, 0.03841805, 0.5214875, 0.35435966, 0.08261086, 0.02471032, 0.09507282, -0.05207797, 0.142467, 0.25394067, -0.10689193, -0.0053745536, -0.042523257, 0.04121408, 0.05810868, 0.050020758, 0.06445058, 0.017566254, 0.01248172, 0.096017, 0.03163697, -0.08723585, 0.019110614, 0.03916633, -0.04191336, 0.09032007, 0.020875078, 0.04597159, 0.019273393, 0.010532511, -0.0055587995, 0.085856326, 0.037331507, 0.093860194, 0.06567866, 0.102047846, 0.107838005, 0.061011754, -0.003385528, 0.07550334, -0.018482266, 0.038708974, 0.050272074, 0.03867403, 0.11148416, 0.027058225, 0.023575122, -0.05118519, 0.069917165, 0.047427073, 0.027560038, 0.027875455, 0.014434553, 0.13396926, 0.048512515, -0.04626608, 0.030183421, 0.07607277, 0.058790546, 0.06517379, 0.10666462, 1.3514475, 0.12856017, 0.10629899, -0.039316032, 0.006233687, 0.028412905, 0.0695567, 0.2469528, 0.026651919, 0.06072498, 0.034764476, -0.056467794, 0.18973885, 0.09566354, 0.017448394, 0.23016949, 0.012064879, 0.027652942, 0.07146018, 0.07435951, -0.02982819, 0.02448592, 0.014292712, 0.018776007, 0.03578673, 0.027073205, 0.032991324, 0.04875977, 0.11590622, 0.011277368, 0.011272416, 0.041706607, 0.086758226, 0.120186426, -0.07532765, 0.0046065617, 0.0088981725, 0.082518935, 0.0579712, 0.0683183, 0.123414166, 0.023280947, 0.079376176, 0.063887954, 0.050154533, 0.04573311, -0.04879956, 0.00855651, 0.036053393, -0.019052086, 0.07232071, 0.1209942, -0.017281257, 0.07475196, -0.013361175, 0.014359835, 0.090134665, 0.008117916, 0.12476145, 0.04981693, 0.064868554, 0.05348068, 0.07841917, 0.048801742, 0.018352231, 0.04332447, -0.02082411, -0.010284867, 0.022944976, 0.13319194, 0.038962107]\n",
      "training: 0:06:48<0:04:29 Epoch 1/10 - Batch 193/320, Loss: 0.0343 0.0349\n",
      "16 16\n",
      "training: 0:06:51<0:04:26 Epoch 1/10 - Batch 194/320, Loss: 0.0343 0.0342\n",
      "16 16\n",
      "training: 0:06:53<0:04:24 Epoch 1/10 - Batch 195/320, Loss: 0.0341 0.0336\n",
      "16 16\n",
      "training: 0:06:55<0:04:22 Epoch 1/10 - Batch 196/320, Loss: 0.0342 0.0343\n",
      "16 16\n",
      "training: 0:06:57<0:04:20 Epoch 1/10 - Batch 197/320, Loss: 0.0346 0.0366\n",
      "16 16\n",
      "training: 0:06:59<0:04:18 Epoch 1/10 - Batch 198/320, Loss: 0.0349 0.0361\n",
      "16 16\n",
      "training: 0:07:00<0:04:15 Epoch 1/10 - Batch 199/320, Loss: 0.0355 0.0376\n",
      "16 16\n",
      "training: 0:07:03<0:04:13 Epoch 1/10 - Batch 200/320, Loss: 0.0357 0.0367\n",
      "16 16\n",
      "training: 0:07:04<0:04:11 Epoch 1/10 - Batch 201/320, Loss: 0.0363 0.0386\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m set_last_batch(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mhandle_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 150\u001b[0m, in \u001b[0;36mhandle_epoch\u001b[0;34m(batch_size, num_epochs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (i, batch) \u001b[38;5;129;01min\u001b[39;00m get_epoch_part(i0, train_batchs, batch_size):\n\u001b[1;32m    149\u001b[0m     performance \u001b[38;5;241m=\u001b[39m (min_alpha, alpha, running_loss, start_time)\n\u001b[0;32m--> 150\u001b[0m     running_loss, alpha \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_train_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperformance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m i0 \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    154\u001b[0m save(epoch, i0)\n",
      "Cell \u001b[0;32mIn[8], line 112\u001b[0m, in \u001b[0;36mhandle_train_batch\u001b[0;34m(i, epoch, batch, performance, sizes)\u001b[0m\n\u001b[1;32m    110\u001b[0m log(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m, start_time, sizes, epoch, i, running_loss, loss)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m running_loss, alpha\n",
      "Cell \u001b[0;32mIn[8], line 91\u001b[0m, in \u001b[0;36msave\u001b[0;34m(epoch, i)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(epoch, i):\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearchnet-bcemodel-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msearchnet-bceoptimizer-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     set_last_batch(i, epoch)\n\u001b[1;32m     93\u001b[0m     tee(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaving searchnet-bcemodel-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:619\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 619\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:853\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m    852\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 853\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "set_last_batch(0, 0)\n",
    "for epoch in range(num_epochs):\n",
    "    handle_epoch(batch_size, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78b5da4b-c5b6-4ced-9c0a-700d7ecc7a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_last_batch(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "932bf29a-b41d-48c4-be7f-a4e61abfc46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import torch\n",
    "device = torch.device('cuda')\n",
    "model = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e20882d-06a2-40de-96e1-645b661e1192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.251953125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# Assuming 'data' is your 512x786 matrix and 'true_labels' are your true class labels\n",
    "data = np.random.rand(512, 786)  # Replace with your actual data\n",
    "true_labels = np.random.randint(5, size=512)  # Replace with your actual labels\n",
    "\n",
    "# Perform Hierarchical Agglomerative Clustering\n",
    "Z = linkage(pdist(data), method='ward')\n",
    "predicted_labels = fcluster(Z, t=5, criterion='maxclust')\n",
    "\n",
    "# Function to find the best matching between predicted and true labels\n",
    "def best_label_matching(y_true, y_pred):\n",
    "    D = np.max([y_pred.max(), y_true.max()]) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
    "    return list(zip(row_ind, col_ind))\n",
    "\n",
    "# Find the best matching\n",
    "ind = best_label_matching(true_labels, predicted_labels)\n",
    "ind_dict = {i[0]: i[1] for i in ind}\n",
    "\n",
    "# Remap predicted labels to best matching true labels\n",
    "remapped_predicted_labels = np.array([ind_dict.get(label, 0) for label in predicted_labels])\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, remapped_predicted_labels)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20d780a-15f0-4ab0-9f82-3a2c4e1c8d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
