{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6db99f-03f8-4d9c-b270-11efa25b0cd7",
   "metadata": {},
   "source": [
    "# After getting files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8225e83-c777-4e68-8d88-1c2a82ca195f",
   "metadata": {},
   "source": [
    "## Insert into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00457c64-4536-4a2a-be83-04e1f1e66e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting dataset location\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "try:\n",
    "    dataset\n",
    "except:\n",
    "    print('getting dataset location')\n",
    "    dataset = Path('dataset').absolute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28b7e35-3703-4a56-9707-fadde8c3f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(dataset)\n",
    "\n",
    "js_dir = dataset / 'javascript'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40094612-df9b-4969-bdd3-94b0f201afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9c940ca-f75f-4f96-9960-3c301cdde7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3, json\n",
    "from tqdm import tqdm\n",
    "from contextlib import contextmanager\n",
    "\n",
    "train = js_dir / 'final' / 'jsonl' / 'train'\n",
    "os.chdir(train)\n",
    "\n",
    "@contextmanager\n",
    "def get_cursor(database_name='rsn_train'):\n",
    "    with sqlite3.connect(database_name) as conn:\n",
    "        yield conn.cursor()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b02a9475-3743-4131-bc5b-864a34c129c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "def simple_parse_xml(content, nested):\n",
    "    pattern = r'\\s*<([^\\s]*?)>\\s*'\n",
    "    open = re.search(pattern, content)\n",
    "    if not open:\n",
    "        return [content]\n",
    "    [open_start, open_end] = open.span()\n",
    "    before = content[:open_start]\n",
    "    tag = open.group(1)\n",
    "    inner_and_after = content[open_end:]\n",
    "    close = re.search(f'</{re.escape(tag)}>', inner_and_after)\n",
    "    if close:\n",
    "        [close_start, close_end] = close.span()\n",
    "    else:\n",
    "        close = re.search(pattern, inner_and_after)\n",
    "        if close:\n",
    "            [close_start, _] = close.span()\n",
    "        else:\n",
    "            close_start = len(inner_and_after)\n",
    "        close_end = close_start\n",
    "    inner = inner_and_after[:close_start]\n",
    "    after = inner_and_after[close_end:]\n",
    "    inner = simple_parse_xml(inner, nested) if nested else inner\n",
    "    return [[before, tag, inner], *simple_parse_xml(after, nested)]\n",
    "\n",
    "def atom_to_re(s):\n",
    "    tokens = [token.strip() for token in re.split(r'\\s+|(?=\\W)|(?<=\\W)', s.strip()) if token]\n",
    "    escaped = [regex.escape(token, special_only=True) for token in tokens]\n",
    "    return r'\\s*' + r'\\s*'.join(escaped) + r'\\s*'\n",
    "\n",
    "def str_to_re(s):\n",
    "    codes = re.split(r'\\s*(?://[^\\n]*(?:\\n|$)|/\\*.*?\\*/|\\.{3,})\\s*', s,  flags=re.DOTALL)\n",
    "    return '(?:.*?)'.join(atom_to_re(code) for code in codes)\n",
    "\n",
    "def node_to_re(node, c):\n",
    "    if type(node) == str:\n",
    "        return str_to_re(node), []\n",
    "    before, tag, content = node\n",
    "    before_re = str_to_re(before)\n",
    "    c[0] += 1\n",
    "    open_gr = c[0]\n",
    "    content_re, content_tags = make_regex(content, c)\n",
    "    c[0] += 1\n",
    "    close_gr = c[0]\n",
    "    open_re = '\\s*(|<'+re.escape(tag)+'>)\\s*'\n",
    "    close_re = '\\s*(|</'+re.escape(tag)+'>)\\s*'\n",
    "    return before_re+open_re+content_re+close_re, [(tag, open_gr, close_gr, content_tags)]\n",
    "\n",
    "\n",
    "def make_regex(tree, c):\n",
    "    regs, tags = zip(*(node_to_re(node, c) for node in tree))\n",
    "    return re.sub(r'(\\\\s\\*)+', r'\\\\s*', ''.join(regs)), [t for tag in tags for t in tag] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904bebf-5087-4d02-ad73-5e4f5aac1cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re\n",
    "omitted = []\n",
    "output = []\n",
    "for text_file in js_dir.glob('thread2-*.txt'):\n",
    "    outfile = str.replace(str(text_file), '.txt', '.json')\n",
    "    if os.path.exists(outfile):\n",
    "        continue\n",
    "    with open(text_file, 'r') as file:\n",
    "        file_contents = file.read()\n",
    "    parsed = simple_parse_xml(file_contents, nested=False)\n",
    "    output.append({\n",
    "        'file': str(text_file),\n",
    "        'split': [p[0] for p in parsed[:-1]] + [parsed[-1]],\n",
    "        'len': len(parsed),\n",
    "        'ok': len(parsed) == 11\n",
    "    })\n",
    "    if len(parsed) != 11:\n",
    "        omitted.append(text_file)\n",
    "        continue\n",
    "    with open(outfile, 'w') as file:\n",
    "        file.write(json.dumps([p[1:] for p in parsed]))\n",
    "\n",
    "with open('parse3.log', 'w') as file:\n",
    "    file.write(json.dumps(output))\n",
    "for out in output:\n",
    "    if not out['ok']:\n",
    "        continue\n",
    "    print(out['file'])\n",
    "    print(out['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709bdefe-5748-4a18-8be6-ae73ddaeead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    # cursor.execute('drop table if exists snippets')\n",
    "    # cursor.execute('drop table if exists region')\n",
    "    cursor.execute('create table if not exists snippets (ID INTEGER PRIMARY KEY, idx INTEGER, code TEXT, locations JSON, regions JSON, SRP boolean)')\n",
    "    cursor.execute('create table if not exists region (ID INTEGER PRIMARY KEY, code TEXT, vector JSON)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8996c197-897d-4d14-bb18-1034b3a38b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_region(code):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute('insert into region (code) values (?)', (code, ))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "def insert_snippet(id, index, code, locations, regions, srp):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute(\n",
    "            'insert into snippets (ID, idx, code, locations, regions, SRP) values (?, ?, ?, ?, ?, ?)',\n",
    "            (id, index, code, json.dumps(locations), json.dumps(regions), srp))\n",
    "\n",
    "def flat_wrong_tags(tags, code, m):\n",
    "    clean_tags = []\n",
    "    for tag in tags:\n",
    "        clean_tags += flat_wrong_tag(tag, clean_tags, code, m)\n",
    "    return clean_tags\n",
    "\n",
    "def flat_wrong_tag(tag, clean_tags, code, m):\n",
    "    name, open, close, sub_tags = tag\n",
    "    clean_sub_tags = flat_wrong_tags(sub_tags, code, m)\n",
    "    if m.group(open):\n",
    "        if len(clean_tags):\n",
    "            pname, popen, pclose, psub_tags = clean_tags[-1]\n",
    "            if not m.group(pclose):\n",
    "                clean_tags[-1] = (pname, popen, close, psub_tags)\n",
    "        return clean_sub_tags\n",
    "    return [(name, open, close, clean_sub_tags)]\n",
    "\n",
    "def tag_to_json(tag, code, m, handle_region):\n",
    "    name, open, close, sub_tags = tag\n",
    "    start = m.span(open)[0]\n",
    "    regions, body, end = tags_to_json(start, sub_tags, code, m, handle_region)\n",
    "    body += code[end:m.span(close)[0]] \n",
    "    region_id = handle_region(f'function {name} () {{\\n{body}\\n}}')\n",
    "    regions = [(start, region_id)] + regions\n",
    "    return regions\n",
    "\n",
    "\n",
    "def tags_to_json(outer_index, tags, code, m, handle_region):\n",
    "    regions = []\n",
    "    outer_body = ''\n",
    "    for tag in tags:\n",
    "        name, open, close, _ = tag\n",
    "        outer_body += code[outer_index:m.span(open)[0]] + '\\n' + name + '();\\n'\n",
    "        outer_index = m.span(close)[0]\n",
    "        regions += tag_to_json(tag, code, m, handle_region)\n",
    "    return regions, outer_body, outer_index\n",
    "\n",
    "\n",
    "def to_json(tags, code, m, handle_region):\n",
    "    regions, body, end = tags_to_json(0, tags, code, m, handle_region)\n",
    "    body += code[end:] \n",
    "    region_id = handle_region(body)\n",
    "    regions = [(0, region_id)] + regions\n",
    "    if len(regions) > 1:\n",
    "        regions.append((end, region_id))\n",
    "    return regions\n",
    "\n",
    "def strip_js_comments(js_code):\n",
    "    js_code = re.sub(r'\\n?//.*?\\n', '\\n', js_code)\n",
    "    js_code = re.sub(r'/\\*.*?\\*/', '', js_code, flags=re.DOTALL)\n",
    "    return js_code    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aca0325d-5b5d-43ad-9649-e4c60ccd872a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8780487804878049"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(js_dir.glob('thread2-*.json')))/len(list(js_dir.glob('thread2-*.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194654e4-f22c-452e-9dfb-5a953fb6a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ok, n_all = 0, 0\n",
    "for text_file in tqdm(list(js_dir.glob('thread2-*.json'))):\n",
    "    index = int(re.match('.*thread2-(.*)\\.json', str(text_file)).group(1))\n",
    "    limit = 10\n",
    "    with get_cursor() as cursor:\n",
    "        codes = list(cursor.execute('select id, code from shuffled limit ? offset ?', (limit, limit * index + 1)))\n",
    "    with open(text_file, 'r') as file:\n",
    "        file_contents = file.read()\n",
    "    for (id, code), obj in zip(codes, json.loads(file_contents)):\n",
    "        original = strip_js_comments(code)\n",
    "        xml = obj[1]\n",
    "        reg_str, tags = make_regex(simple_parse_xml(xml, True), [0])\n",
    "        reg = regex.compile(reg_str, flags = regex.DOTALL)\n",
    "        m = reg.match(original)\n",
    "        n_all += 1\n",
    "        if m:\n",
    "            n_ok += 1\n",
    "            tags = flat_wrong_tags(tags, original, m)\n",
    "            if len(tags) == 1 and not len(tags[0][3]):\n",
    "                tags = []\n",
    "            regions = to_json(tags, original, m, insert_region)\n",
    "            insert_snippet(id, index, original, *zip(*regions), len(tags) == 0)\n",
    "            \n",
    "            \n",
    "print(f'{n_ok}/{n_all}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a65ab-ab7c-4efe-a4d3-2f0f5145c12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    print(list(cursor.execute('select idx < 650, count(*), SRP from snippets group by idx < 650, SRP')))\n",
    "    print(list(cursor.execute('select count(*) from region')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fee2198e-ec9b-46ee-a700-4a102de567d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8663b41d-56f6-4ebd-ba46-8efbfe5be2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(ids):\n",
    "    with get_cursor() as cursor:\n",
    "        regions = list(cursor.execute(f'select id, code from region where id in ({ids})'))\n",
    "    for region in regions:\n",
    "        id, code = region\n",
    "        tokenized_inputs = tokenizer([code], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        tokenized_inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokenized_inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        #average_hidden_states = last_hidden_states.mean(dim=1)\n",
    "        yield id, list(last_hidden_states.cpu().numpy()[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5e81adab-31e7-478f-8f0c-9bc251b82e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6247\n"
     ]
    }
   ],
   "source": [
    "with get_cursor() as cursor:\n",
    "    print(next(cursor.execute('select count(*) from snippets'))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211bdf4d-2c35-4cd9-81b6-8f3775c59e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('drop table if exists tokenized')\n",
    "    cursor.execute('create table if not exists tokenized (ID INTEGER PRIMARY KEY, input_ids JSON, region_ids JSON)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655692b4-a8bc-4aeb-95f4-9a23fedb1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    snippets = list(cursor.execute('select id, code, locations, regions from snippets where idx < 650'))\n",
    "for (id, code, locations, regions) in tqdm(snippets):\n",
    "    tokens = tokenizer.encode_plus(code, truncation=True, return_offsets_mapping=True)\n",
    "    offset_mapping = tokens['offset_mapping']\n",
    "    regions, locations = json.loads(regions),json.loads(locations)\n",
    "    if not len(regions):\n",
    "        continue\n",
    "    locations.append(len(code))\n",
    "    i = 0\n",
    "    region_ids = []\n",
    "    for (start, end) in offset_mapping[1:-1]:\n",
    "        while start > locations[i+1]:\n",
    "            i += 1\n",
    "        region_ids.append(regions[i])\n",
    "    region_ids = [0] + region_ids + [0]\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute(\n",
    "            'insert into tokenized (ID, input_ids, region_ids) values (?, ?, ?)',\n",
    "            (id, json.dumps(tokens['input_ids']), json.dumps(region_ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bef707b-169d-4458-9f8c-18e57fd47407",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('drop table if exists train_snippets')\n",
    "    cursor.execute('create table train_snippets as select * from snippets where idx < 650')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5569bc6b-d6e2-453d-8dab-c16d2cca3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('create table if not exists current_batch(idx INTEGER)')\n",
    "    if not len(list(cursor.execute('select * from current_batch'))):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78bc3235-55f3-4130-bc94-40624dd4a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('create table if not exists last_batch(idx INTEGER, epoch_idx INTEGER)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fda53217-c701-4ded-9c2b-c8f63e77a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def set_last_batch(last_batch, epoch_idx):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute('update last_batch set idx = ?, epoch_idx = ?', (last_batch, epoch_idx))\n",
    "\n",
    "def get_unsafe_last_batch():\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute('select * from last_batch')\n",
    "        item = cursor.fetchone()\n",
    "        if not item:\n",
    "            cursor.execute('insert into last_batch values (0, 0)')\n",
    "            return 0\n",
    "        else:\n",
    "            return item\n",
    "\n",
    "def get_last_batch(num_batches):\n",
    "    last_batch, last_epoch = get_unsafe_last_batch()\n",
    "    if last_batch >= num_batches:\n",
    "        last_batch = 0\n",
    "        last_epoch += 1\n",
    "        set_last_batch(0, last_epoch)\n",
    "    return last_batch, last_epoch\n",
    "        \n",
    "def get_batch_tokens(batch_size, i):\n",
    "    with get_cursor() as cursor:\n",
    "        ids = ','.join(\n",
    "            str(x[0]) for x in cursor.execute('select id from train_snippets_shuffled limit ? offset ?', (batch_size, i * batch_size))\n",
    "        )\n",
    "        tokens = cursor.execute(f'select input_ids, region_ids from tokenized where id in ({ids})')\n",
    "        return [(json.loads(input_ids), json.loads(region_ids)) for (input_ids, region_ids) in tokens]\n",
    "        \n",
    "\n",
    "def get_batch_regions(tokens):\n",
    "    with get_cursor() as cursor:\n",
    "        region_ids_str = ','.join(set(str(region_id) for (_, region_ids) in tokens for region_id in region_ids))\n",
    "        return dict(get_labels(region_ids_str))\n",
    "\n",
    "def prepare_iteration(input_ids, region_ids, regions):\n",
    "    label_size = 768\n",
    "    size = len(input_ids)\n",
    "    if size > 512:\n",
    "        print(size)\n",
    "    empties = 512 - size\n",
    "    null_vector = [0] * label_size\n",
    "    for r_id in region_ids:\n",
    "        if r_id and not (r_id in regions):\n",
    "            print(r_id)\n",
    "    label = [regions[r_id] if r_id else null_vector for r_id in region_ids]\n",
    "    label += [null_vector] * empties\n",
    "    input_ids += [0] * empties\n",
    "    attention = [1] * size + [0] * empties\n",
    "    try:\n",
    "        torch.FloatTensor(label)\n",
    "    except:\n",
    "        print(regions[region_ids[1]])\n",
    "\n",
    "    return input_ids, attention, label\n",
    "\n",
    "def get_batch_data(batch_size, i):\n",
    "    tokens = get_batch_tokens(batch_size, i)\n",
    "    regions = get_batch_regions(tokens)\n",
    "    batch = [prepare_iteration(input_ids, region_ids, regions) for (input_ids, region_ids) in tokens]\n",
    "    input_ids, attention, label = zip(*batch)\n",
    "    return (torch.IntTensor(input_ids), torch.FloatTensor(attention), torch.FloatTensor(label)), tokens\n",
    "\n",
    "\n",
    "def get_epoch_part(i0, num_batches, batch_size):\n",
    "    return ((i, get_batch_data(batch_size, i)) for i in range(i0, num_batches, 1))\n",
    "\n",
    "def tee(text):\n",
    "    print(f\"\\r{text}\\r\")\n",
    "    with open('train-resnet.log', 'a') as file:\n",
    "        file.write(f\"{text}\\n\")\n",
    "\n",
    "def log(title, start_time, sizes, epoch, i, mean_loss, loss):\n",
    "    _, num_epochs, num_batches = sizes\n",
    "    i += 1\n",
    "    dt = time.time() - start_time\n",
    "    elapsed = timedelta(seconds=int(dt))\n",
    "    remaining = timedelta(seconds=int(dt*(num_batches-i)/i))\n",
    "    text = f\"{title}: {elapsed}<{remaining} Epoch {epoch+1}/{num_epochs} - Batch {i}/{num_batches}, Loss: {mean_loss:.4f} {loss.item():.4f}\"\n",
    "    tee(text)\n",
    "\n",
    "def save(epoch, i):\n",
    "    torch.save(model.state_dict(), f'searchnet-bcemodel-{epoch}-{i}.pt')\n",
    "    torch.save(optimizer.state_dict(), f'searchnet-bceoptimizer-{epoch}-{i}.pt')\n",
    "    set_last_batch(i, epoch)\n",
    "    tee(f'saving searchnet-bcemodel-{epoch}-{i}.pt\\n')\n",
    "\n",
    "def compute_loss(batch_data):\n",
    "    batch_vectors, batch_tokens = batch_data\n",
    "    input_ids, attention, labels = (c.to(device) for c in batch_vectors)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention)\n",
    "\n",
    "    region_ids = torch.tensor([region_ids + [0] * (512 - len(region_ids)) for (input_ids, region_ids) in batch_tokens])\n",
    "    padding_mask = (region_ids == 0).to(device)\n",
    "    labels[padding_mask] = outputs.last_hidden_state[padding_mask]\n",
    "\n",
    "    \n",
    "    return loss_fn(outputs.last_hidden_state, labels)\n",
    "\n",
    "\n",
    "def handle_train_batch(i, epoch, batch_data, performance, sizes):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = compute_loss(batch_data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    min_alpha, alpha, running_loss, start_time = performance\n",
    "\n",
    "    current_alpha = min_alpha + alpha\n",
    "    running_loss = (1 - current_alpha) * running_loss + current_alpha * loss.item()\n",
    "    alpha *= .5\n",
    "    \n",
    "    log('training', start_time, sizes, epoch, i, running_loss, loss)\n",
    "    if (i+1) % 100 == 0:\n",
    "        save(epoch, i)\n",
    "    return running_loss, alpha\n",
    "    \n",
    "def handle_val_batch(i0, i, epoch, batch_data, start_time, cum_loss, sizes):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss = compute_loss(batch_data)\n",
    "    \n",
    "    cum_loss += loss.item()\n",
    "    mean_loss = cum_loss / (i - i0 + 1)\n",
    "    \n",
    "    log('validation', start_time, sizes, epoch, i, mean_loss, loss)\n",
    "    return cum_loss\n",
    "    \n",
    "\n",
    "def handle_epoch(batch_size, num_epochs):\n",
    "    with get_cursor() as cursor:\n",
    "        epoch_size = next(cursor.execute('select count(*) from train_snippets'))[0]\n",
    "    num_batches = (epoch_size - 1) // batch_size + 1\n",
    "    train_batchs = 4 * num_batches // 5\n",
    "    i0, epoch0 = get_last_batch(train_batchs)\n",
    "    num_epochs += epoch0\n",
    "    i = i0 - 1\n",
    "    min_alpha = .2\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    alpha = 1 - min_alpha\n",
    "\n",
    "    model.train()\n",
    "    sizes = (train_batchs, num_epochs, num_batches)\n",
    "    tee('\\nTraining\\n')\n",
    "    for (i, batch_data) in get_epoch_part(i0, train_batchs, batch_size):\n",
    "        performance = (min_alpha, alpha, running_loss, start_time)\n",
    "        running_loss, alpha = handle_train_batch(i, epoch, batch_data, performance, sizes)\n",
    "    \n",
    "    \n",
    "    i0 = i + 1\n",
    "    save(epoch, i0)\n",
    "    \n",
    "    model.eval()\n",
    "    cum_loss = 0\n",
    "    tee('\\nValidation:\\n')\n",
    "    for (i, batch_data) in get_epoch_part(i0, num_batches, batch_size):\n",
    "        cum_loss = handle_val_batch(i0, i, epoch, batch_data, start_time, cum_loss, sizes)\n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59af576d-6e6b-4f4d-910d-ff23dab8cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_last_batch(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df189e27-706e-4ee0-81e2-ef57d1522b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training\n",
      "\n",
      "training: 0:00:27<2:27:32 Epoch 1/10 - Batch 1/320, Loss: 0.1226 0.1226\n",
      "training: 0:00:50<2:13:25 Epoch 1/10 - Batch 2/320, Loss: 0.1353 0.1437\n",
      "training: 0:01:11<2:06:34 Epoch 1/10 - Batch 3/320, Loss: 0.1489 0.1694\n",
      "training: 0:01:33<2:03:26 Epoch 1/10 - Batch 4/320, Loss: 0.1366 0.1078\n",
      "training: 0:01:56<2:01:53 Epoch 1/10 - Batch 5/320, Loss: 0.1379 0.1420\n",
      "training: 0:02:17<1:59:56 Epoch 1/10 - Batch 6/320, Loss: 0.1306 0.1051\n",
      "training: 0:02:39<1:58:55 Epoch 1/10 - Batch 7/320, Loss: 0.1255 0.1069\n",
      "training: 0:03:01<1:57:51 Epoch 1/10 - Batch 8/320, Loss: 0.1145 0.0720\n",
      "training: 0:03:23<1:56:56 Epoch 1/10 - Batch 9/320, Loss: 0.1057 0.0714\n",
      "training: 0:03:44<1:55:53 Epoch 1/10 - Batch 10/320, Loss: 0.0997 0.0758\n",
      "training: 0:04:05<1:54:49 Epoch 1/10 - Batch 11/320, Loss: 0.0934 0.0685\n",
      "training: 0:04:26<1:53:53 Epoch 1/10 - Batch 12/320, Loss: 0.0851 0.0518\n",
      "training: 0:04:48<1:53:21 Epoch 1/10 - Batch 13/320, Loss: 0.0786 0.0529\n",
      "training: 0:05:08<1:52:29 Epoch 1/10 - Batch 14/320, Loss: 0.0697 0.0339\n",
      "training: 0:05:29<1:51:44 Epoch 1/10 - Batch 15/320, Loss: 0.0616 0.0293\n",
      "training: 0:05:50<1:51:03 Epoch 1/10 - Batch 16/320, Loss: 0.0604 0.0554\n",
      "training: 0:06:11<1:50:14 Epoch 1/10 - Batch 17/320, Loss: 0.0565 0.0409\n",
      "training: 0:06:32<1:49:51 Epoch 1/10 - Batch 18/320, Loss: 0.0515 0.0318\n",
      "training: 0:06:54<1:49:28 Epoch 1/10 - Batch 19/320, Loss: 0.0489 0.0383\n",
      "training: 0:07:16<1:49:04 Epoch 1/10 - Batch 20/320, Loss: 0.0436 0.0223\n",
      "training: 0:07:38<1:48:52 Epoch 1/10 - Batch 21/320, Loss: 0.0457 0.0544\n",
      "training: 0:08:00<1:48:30 Epoch 1/10 - Batch 22/320, Loss: 0.0464 0.0489\n",
      "training: 0:08:21<1:47:50 Epoch 1/10 - Batch 23/320, Loss: 0.0409 0.0188\n",
      "training: 0:08:42<1:47:19 Epoch 1/10 - Batch 24/320, Loss: 0.0372 0.0225\n",
      "training: 0:09:03<1:46:56 Epoch 1/10 - Batch 25/320, Loss: 0.0345 0.0239\n",
      "training: 0:09:24<1:46:19 Epoch 1/10 - Batch 26/320, Loss: 0.0336 0.0298\n",
      "training: 0:09:45<1:45:48 Epoch 1/10 - Batch 27/320, Loss: 0.0312 0.0218\n",
      "training: 0:10:07<1:45:34 Epoch 1/10 - Batch 28/320, Loss: 0.0308 0.0291\n",
      "training: 0:10:29<1:45:14 Epoch 1/10 - Batch 29/320, Loss: 0.0315 0.0345\n",
      "training: 0:10:51<1:44:53 Epoch 1/10 - Batch 30/320, Loss: 0.0330 0.0388\n",
      "training: 0:11:12<1:44:27 Epoch 1/10 - Batch 31/320, Loss: 0.0302 0.0192\n",
      "training: 0:11:33<1:44:03 Epoch 1/10 - Batch 32/320, Loss: 0.0283 0.0207\n",
      "training: 0:11:55<1:43:38 Epoch 1/10 - Batch 33/320, Loss: 0.0278 0.0258\n",
      "training: 0:12:16<1:43:14 Epoch 1/10 - Batch 34/320, Loss: 0.0269 0.0234\n",
      "training: 0:12:37<1:42:44 Epoch 1/10 - Batch 35/320, Loss: 0.0260 0.0221\n",
      "training: 0:12:57<1:42:16 Epoch 1/10 - Batch 36/320, Loss: 0.0248 0.0200\n",
      "training: 0:13:19<1:41:53 Epoch 1/10 - Batch 37/320, Loss: 0.0252 0.0268\n",
      "training: 0:13:39<1:41:23 Epoch 1/10 - Batch 38/320, Loss: 0.0230 0.0143\n",
      "training: 0:14:00<1:40:54 Epoch 1/10 - Batch 39/320, Loss: 0.0221 0.0186\n",
      "training: 0:14:21<1:40:31 Epoch 1/10 - Batch 40/320, Loss: 0.0214 0.0185\n",
      "training: 0:14:42<1:40:04 Epoch 1/10 - Batch 41/320, Loss: 0.0212 0.0202\n",
      "training: 0:15:04<1:39:44 Epoch 1/10 - Batch 42/320, Loss: 0.0221 0.0258\n",
      "training: 0:15:24<1:39:17 Epoch 1/10 - Batch 43/320, Loss: 0.0217 0.0201\n",
      "training: 0:15:45<1:38:49 Epoch 1/10 - Batch 44/320, Loss: 0.0212 0.0192\n",
      "training: 0:16:05<1:38:23 Epoch 1/10 - Batch 45/320, Loss: 0.0213 0.0216\n",
      "training: 0:16:28<1:38:07 Epoch 1/10 - Batch 46/320, Loss: 0.0216 0.0227\n",
      "training: 0:16:49<1:37:44 Epoch 1/10 - Batch 47/320, Loss: 0.0224 0.0258\n",
      "training: 0:17:10<1:37:16 Epoch 1/10 - Batch 48/320, Loss: 0.0216 0.0184\n",
      "training: 0:17:31<1:36:55 Epoch 1/10 - Batch 49/320, Loss: 0.0210 0.0185\n",
      "training: 0:17:52<1:36:31 Epoch 1/10 - Batch 50/320, Loss: 0.0211 0.0218\n",
      "training: 0:18:12<1:36:04 Epoch 1/10 - Batch 51/320, Loss: 0.0199 0.0150\n",
      "training: 0:18:33<1:35:39 Epoch 1/10 - Batch 52/320, Loss: 0.0187 0.0137\n",
      "training: 0:18:55<1:35:19 Epoch 1/10 - Batch 53/320, Loss: 0.0182 0.0164\n",
      "training: 0:19:16<1:34:56 Epoch 1/10 - Batch 54/320, Loss: 0.0186 0.0200\n",
      "training: 0:19:38<1:34:36 Epoch 1/10 - Batch 55/320, Loss: 0.0208 0.0295\n",
      "training: 0:19:59<1:34:13 Epoch 1/10 - Batch 56/320, Loss: 0.0203 0.0184\n",
      "training: 0:20:20<1:33:53 Epoch 1/10 - Batch 57/320, Loss: 0.0208 0.0226\n",
      "training: 0:20:41<1:33:30 Epoch 1/10 - Batch 58/320, Loss: 0.0208 0.0208\n",
      "training: 0:21:03<1:33:09 Epoch 1/10 - Batch 59/320, Loss: 0.0202 0.0181\n",
      "training: 0:21:24<1:32:44 Epoch 1/10 - Batch 60/320, Loss: 0.0202 0.0199\n",
      "training: 0:21:46<1:32:26 Epoch 1/10 - Batch 61/320, Loss: 0.0201 0.0196\n",
      "training: 0:22:08<1:32:06 Epoch 1/10 - Batch 62/320, Loss: 0.0191 0.0153\n",
      "training: 0:22:28<1:31:42 Epoch 1/10 - Batch 63/320, Loss: 0.0192 0.0194\n",
      "training: 0:22:51<1:31:24 Epoch 1/10 - Batch 64/320, Loss: 0.0219 0.0330\n",
      "training: 0:23:11<1:31:00 Epoch 1/10 - Batch 65/320, Loss: 0.0207 0.0157\n",
      "training: 0:23:33<1:30:38 Epoch 1/10 - Batch 66/320, Loss: 0.0197 0.0156\n",
      "training: 0:23:54<1:30:16 Epoch 1/10 - Batch 67/320, Loss: 0.0194 0.0182\n",
      "training: 0:24:15<1:29:54 Epoch 1/10 - Batch 68/320, Loss: 0.0193 0.0189\n",
      "training: 0:24:36<1:29:31 Epoch 1/10 - Batch 69/320, Loss: 0.0185 0.0153\n",
      "training: 0:24:57<1:29:06 Epoch 1/10 - Batch 70/320, Loss: 0.0181 0.0163\n",
      "training: 0:25:18<1:28:45 Epoch 1/10 - Batch 71/320, Loss: 0.0195 0.0252\n",
      "training: 0:25:38<1:28:19 Epoch 1/10 - Batch 72/320, Loss: 0.0179 0.0113\n",
      "training: 0:25:59<1:27:55 Epoch 1/10 - Batch 73/320, Loss: 0.0168 0.0127\n",
      "training: 0:26:20<1:27:33 Epoch 1/10 - Batch 74/320, Loss: 0.0173 0.0195\n",
      "training: 0:26:41<1:27:11 Epoch 1/10 - Batch 75/320, Loss: 0.0177 0.0193\n",
      "training: 0:27:02<1:26:48 Epoch 1/10 - Batch 76/320, Loss: 0.0190 0.0238\n",
      "training: 0:27:24<1:26:29 Epoch 1/10 - Batch 77/320, Loss: 0.0201 0.0248\n",
      "training: 0:27:46<1:26:09 Epoch 1/10 - Batch 78/320, Loss: 0.0202 0.0205\n",
      "training: 0:28:06<1:25:44 Epoch 1/10 - Batch 79/320, Loss: 0.0192 0.0153\n",
      "training: 0:28:29<1:25:27 Epoch 1/10 - Batch 80/320, Loss: 0.0223 0.0347\n",
      "training: 0:28:50<1:25:05 Epoch 1/10 - Batch 81/320, Loss: 0.0207 0.0140\n",
      "training: 0:29:11<1:24:43 Epoch 1/10 - Batch 82/320, Loss: 0.0207 0.0209\n",
      "training: 0:29:32<1:24:21 Epoch 1/10 - Batch 83/320, Loss: 0.0199 0.0169\n",
      "training: 0:29:53<1:23:58 Epoch 1/10 - Batch 84/320, Loss: 0.0203 0.0216\n",
      "training: 0:30:13<1:23:34 Epoch 1/10 - Batch 85/320, Loss: 0.0187 0.0122\n",
      "training: 0:30:34<1:23:12 Epoch 1/10 - Batch 86/320, Loss: 0.0193 0.0219\n",
      "training: 0:30:55<1:22:49 Epoch 1/10 - Batch 87/320, Loss: 0.0182 0.0136\n",
      "training: 0:31:15<1:22:24 Epoch 1/10 - Batch 88/320, Loss: 0.0165 0.0098\n",
      "training: 0:31:37<1:22:05 Epoch 1/10 - Batch 89/320, Loss: 0.0178 0.0229\n",
      "training: 0:31:58<1:21:43 Epoch 1/10 - Batch 90/320, Loss: 0.0185 0.0211\n",
      "training: 0:32:19<1:21:21 Epoch 1/10 - Batch 91/320, Loss: 0.0180 0.0163\n",
      "training: 0:32:40<1:20:58 Epoch 1/10 - Batch 92/320, Loss: 0.0183 0.0196\n",
      "training: 0:33:01<1:20:36 Epoch 1/10 - Batch 93/320, Loss: 0.0177 0.0150\n",
      "training: 0:33:22<1:20:14 Epoch 1/10 - Batch 94/320, Loss: 0.0168 0.0135\n",
      "training: 0:33:43<1:19:51 Epoch 1/10 - Batch 95/320, Loss: 0.0161 0.0131\n",
      "training: 0:34:04<1:19:31 Epoch 1/10 - Batch 96/320, Loss: 0.0157 0.0141\n",
      "training: 0:34:26<1:19:11 Epoch 1/10 - Batch 97/320, Loss: 0.0163 0.0188\n",
      "training: 0:35:09<1:18:28 Epoch 1/10 - Batch 99/320, Loss: 0.0165 0.0207\n",
      "training: 0:35:30<1:18:06 Epoch 1/10 - Batch 100/320, Loss: 0.0165 0.0169\n",
      "saving searchnet-bcemodel-0-99.pt\n",
      "\n",
      "training: 0:35:51<1:17:46 Epoch 1/10 - Batch 101/320, Loss: 0.0164 0.0159\n",
      "training: 0:36:12<1:17:23 Epoch 1/10 - Batch 102/320, Loss: 0.0162 0.0152\n",
      "training: 0:36:33<1:17:00 Epoch 1/10 - Batch 103/320, Loss: 0.0154 0.0124\n",
      "training: 0:36:54<1:16:38 Epoch 1/10 - Batch 104/320, Loss: 0.0159 0.0178\n",
      "training: 0:37:15<1:16:18 Epoch 1/10 - Batch 105/320, Loss: 0.0161 0.0169\n",
      "training: 0:37:36<1:15:56 Epoch 1/10 - Batch 106/320, Loss: 0.0153 0.0122\n",
      "training: 0:37:57<1:15:33 Epoch 1/10 - Batch 107/320, Loss: 0.0148 0.0127\n",
      "training: 0:38:18<1:15:11 Epoch 1/10 - Batch 108/320, Loss: 0.0143 0.0123\n",
      "training: 0:38:40<1:14:51 Epoch 1/10 - Batch 109/320, Loss: 0.0150 0.0178\n",
      "training: 0:39:01<1:14:29 Epoch 1/10 - Batch 110/320, Loss: 0.0167 0.0234\n",
      "training: 0:39:21<1:14:06 Epoch 1/10 - Batch 111/320, Loss: 0.0167 0.0167\n",
      "training: 0:39:43<1:13:45 Epoch 1/10 - Batch 112/320, Loss: 0.0170 0.0183\n",
      "training: 0:40:04<1:13:24 Epoch 1/10 - Batch 113/320, Loss: 0.0170 0.0168\n",
      "training: 0:40:25<1:13:02 Epoch 1/10 - Batch 114/320, Loss: 0.0168 0.0160\n",
      "training: 0:40:45<1:12:39 Epoch 1/10 - Batch 115/320, Loss: 0.0177 0.0212\n",
      "training: 0:41:07<1:12:18 Epoch 1/10 - Batch 116/320, Loss: 0.0171 0.0147\n",
      "training: 0:41:28<1:11:56 Epoch 1/10 - Batch 117/320, Loss: 0.0160 0.0117\n",
      "training: 0:41:49<1:11:35 Epoch 1/10 - Batch 118/320, Loss: 0.0155 0.0135\n",
      "training: 0:42:09<1:11:12 Epoch 1/10 - Batch 119/320, Loss: 0.0151 0.0133\n",
      "training: 0:42:30<1:10:50 Epoch 1/10 - Batch 120/320, Loss: 0.0155 0.0172\n",
      "training: 0:42:51<1:10:29 Epoch 1/10 - Batch 121/320, Loss: 0.0171 0.0237\n",
      "training: 0:43:12<1:10:07 Epoch 1/10 - Batch 122/320, Loss: 0.0164 0.0133\n",
      "training: 0:43:34<1:09:47 Epoch 1/10 - Batch 123/320, Loss: 0.0172 0.0205\n",
      "training: 0:43:55<1:09:25 Epoch 1/10 - Batch 124/320, Loss: 0.0166 0.0141\n",
      "training: 0:44:16<1:09:03 Epoch 1/10 - Batch 125/320, Loss: 0.0159 0.0132\n",
      "training: 0:44:38<1:08:44 Epoch 1/10 - Batch 126/320, Loss: 0.0174 0.0233\n",
      "training: 0:45:00<1:08:24 Epoch 1/10 - Batch 127/320, Loss: 0.0180 0.0204\n",
      "training: 0:45:21<1:08:02 Epoch 1/10 - Batch 128/320, Loss: 0.0168 0.0120\n",
      "training: 0:45:42<1:07:40 Epoch 1/10 - Batch 129/320, Loss: 0.0163 0.0142\n",
      "training: 0:46:04<1:07:20 Epoch 1/10 - Batch 130/320, Loss: 0.0160 0.0147\n",
      "training: 0:46:25<1:06:59 Epoch 1/10 - Batch 131/320, Loss: 0.0160 0.0160\n",
      "training: 0:46:46<1:06:37 Epoch 1/10 - Batch 132/320, Loss: 0.0164 0.0179\n",
      "training: 0:47:08<1:06:16 Epoch 1/10 - Batch 133/320, Loss: 0.0175 0.0221\n",
      "training: 0:47:29<1:05:55 Epoch 1/10 - Batch 134/320, Loss: 0.0182 0.0208\n",
      "training: 0:47:50<1:05:33 Epoch 1/10 - Batch 135/320, Loss: 0.0174 0.0143\n",
      "training: 0:48:11<1:05:12 Epoch 1/10 - Batch 136/320, Loss: 0.0178 0.0192\n",
      "training: 0:48:32<1:04:50 Epoch 1/10 - Batch 137/320, Loss: 0.0168 0.0128\n",
      "training: 0:48:54<1:04:29 Epoch 1/10 - Batch 138/320, Loss: 0.0168 0.0171\n",
      "training: 0:49:14<1:04:07 Epoch 1/10 - Batch 139/320, Loss: 0.0158 0.0114\n",
      "training: 0:49:35<1:03:45 Epoch 1/10 - Batch 140/320, Loss: 0.0157 0.0156\n",
      "training: 0:49:56<1:03:24 Epoch 1/10 - Batch 141/320, Loss: 0.0157 0.0157\n",
      "training: 0:50:17<1:03:02 Epoch 1/10 - Batch 142/320, Loss: 0.0160 0.0171\n",
      "training: 0:50:38<1:02:41 Epoch 1/10 - Batch 143/320, Loss: 0.0155 0.0137\n",
      "training: 0:50:59<1:02:18 Epoch 1/10 - Batch 144/320, Loss: 0.0148 0.0118\n",
      "training: 0:51:19<1:01:56 Epoch 1/10 - Batch 145/320, Loss: 0.0144 0.0130\n",
      "training: 0:51:40<1:01:35 Epoch 1/10 - Batch 146/320, Loss: 0.0140 0.0124\n",
      "training: 0:52:01<1:01:13 Epoch 1/10 - Batch 147/320, Loss: 0.0146 0.0168\n",
      "training: 0:52:22<1:00:51 Epoch 1/10 - Batch 148/320, Loss: 0.0147 0.0153\n",
      "training: 0:52:43<1:00:30 Epoch 1/10 - Batch 149/320, Loss: 0.0149 0.0156\n",
      "training: 0:53:04<1:00:09 Epoch 1/10 - Batch 150/320, Loss: 0.0158 0.0192\n",
      "training: 0:53:25<0:59:47 Epoch 1/10 - Batch 151/320, Loss: 0.0150 0.0119\n",
      "training: 0:53:46<0:59:25 Epoch 1/10 - Batch 152/320, Loss: 0.0146 0.0128\n",
      "training: 0:54:07<0:59:04 Epoch 1/10 - Batch 153/320, Loss: 0.0137 0.0104\n",
      "training: 0:54:29<0:58:43 Epoch 1/10 - Batch 154/320, Loss: 0.0149 0.0195\n",
      "training: 0:54:50<0:58:22 Epoch 1/10 - Batch 155/320, Loss: 0.0144 0.0122\n",
      "training: 0:55:11<0:58:01 Epoch 1/10 - Batch 156/320, Loss: 0.0149 0.0169\n",
      "training: 0:55:31<0:57:39 Epoch 1/10 - Batch 157/320, Loss: 0.0142 0.0113\n",
      "training: 0:55:52<0:57:17 Epoch 1/10 - Batch 158/320, Loss: 0.0138 0.0121\n",
      "training: 0:56:12<0:56:55 Epoch 1/10 - Batch 159/320, Loss: 0.0136 0.0130\n",
      "training: 0:56:33<0:56:33 Epoch 1/10 - Batch 160/320, Loss: 0.0136 0.0136\n",
      "training: 0:56:54<0:56:12 Epoch 1/10 - Batch 161/320, Loss: 0.0128 0.0094\n",
      "training: 0:57:15<0:55:50 Epoch 1/10 - Batch 162/320, Loss: 0.0139 0.0183\n",
      "training: 0:57:35<0:55:28 Epoch 1/10 - Batch 163/320, Loss: 0.0129 0.0089\n",
      "training: 0:57:57<0:55:07 Epoch 1/10 - Batch 164/320, Loss: 0.0144 0.0206\n",
      "training: 0:58:18<0:54:46 Epoch 1/10 - Batch 165/320, Loss: 0.0152 0.0182\n",
      "training: 0:58:39<0:54:25 Epoch 1/10 - Batch 166/320, Loss: 0.0154 0.0163\n",
      "training: 0:59:01<0:54:04 Epoch 1/10 - Batch 167/320, Loss: 0.0164 0.0204\n",
      "training: 0:59:22<0:53:43 Epoch 1/10 - Batch 168/320, Loss: 0.0153 0.0107\n",
      "training: 0:59:42<0:53:21 Epoch 1/10 - Batch 169/320, Loss: 0.0141 0.0096\n",
      "training: 1:00:04<0:53:00 Epoch 1/10 - Batch 170/320, Loss: 0.0146 0.0167\n",
      "training: 1:00:24<0:52:38 Epoch 1/10 - Batch 171/320, Loss: 0.0137 0.0100\n",
      "training: 1:00:46<0:52:17 Epoch 1/10 - Batch 172/320, Loss: 0.0137 0.0135\n",
      "training: 1:01:07<0:51:56 Epoch 1/10 - Batch 173/320, Loss: 0.0140 0.0155\n",
      "training: 1:01:27<0:51:34 Epoch 1/10 - Batch 174/320, Loss: 0.0128 0.0076\n",
      "training: 1:01:48<0:51:12 Epoch 1/10 - Batch 175/320, Loss: 0.0128 0.0132\n",
      "training: 1:02:09<0:50:51 Epoch 1/10 - Batch 176/320, Loss: 0.0132 0.0148\n",
      "training: 1:02:29<0:50:29 Epoch 1/10 - Batch 177/320, Loss: 0.0131 0.0124\n",
      "training: 1:02:52<0:50:09 Epoch 1/10 - Batch 178/320, Loss: 0.0142 0.0187\n",
      "training: 1:03:13<0:49:48 Epoch 1/10 - Batch 179/320, Loss: 0.0147 0.0165\n",
      "training: 1:03:35<0:49:27 Epoch 1/10 - Batch 180/320, Loss: 0.0151 0.0170\n",
      "training: 1:03:56<0:49:06 Epoch 1/10 - Batch 181/320, Loss: 0.0150 0.0146\n",
      "training: 1:04:17<0:48:44 Epoch 1/10 - Batch 182/320, Loss: 0.0147 0.0137\n",
      "training: 1:04:38<0:48:23 Epoch 1/10 - Batch 183/320, Loss: 0.0145 0.0133\n",
      "training: 1:04:59<0:48:02 Epoch 1/10 - Batch 184/320, Loss: 0.0140 0.0123\n",
      "training: 1:05:20<0:47:40 Epoch 1/10 - Batch 185/320, Loss: 0.0135 0.0116\n",
      "training: 1:05:41<0:47:19 Epoch 1/10 - Batch 186/320, Loss: 0.0145 0.0182\n",
      "training: 1:06:02<0:46:58 Epoch 1/10 - Batch 187/320, Loss: 0.0144 0.0144\n",
      "training: 1:06:23<0:46:37 Epoch 1/10 - Batch 188/320, Loss: 0.0142 0.0133\n",
      "training: 1:06:46<0:46:16 Epoch 1/10 - Batch 189/320, Loss: 0.0153 0.0196\n",
      "training: 1:07:06<0:45:55 Epoch 1/10 - Batch 190/320, Loss: 0.0154 0.0157\n",
      "training: 1:07:28<0:45:34 Epoch 1/10 - Batch 191/320, Loss: 0.0157 0.0170\n",
      "training: 1:07:50<0:45:13 Epoch 1/10 - Batch 192/320, Loss: 0.0154 0.0142\n",
      "training: 1:08:12<0:44:52 Epoch 1/10 - Batch 193/320, Loss: 0.0155 0.0160\n",
      "training: 1:08:33<0:44:31 Epoch 1/10 - Batch 194/320, Loss: 0.0154 0.0150\n",
      "training: 1:08:55<0:44:10 Epoch 1/10 - Batch 195/320, Loss: 0.0154 0.0152\n",
      "training: 1:09:15<0:43:49 Epoch 1/10 - Batch 196/320, Loss: 0.0155 0.0158\n",
      "training: 1:09:37<0:43:28 Epoch 1/10 - Batch 197/320, Loss: 0.0157 0.0169\n",
      "training: 1:09:58<0:43:06 Epoch 1/10 - Batch 198/320, Loss: 0.0146 0.0100\n",
      "training: 1:10:19<0:42:45 Epoch 1/10 - Batch 199/320, Loss: 0.0150 0.0164\n",
      "training: 1:10:39<0:42:23 Epoch 1/10 - Batch 200/320, Loss: 0.0144 0.0121\n",
      "saving searchnet-bcemodel-0-199.pt\n",
      "\n",
      "training: 1:11:05<0:42:05 Epoch 1/10 - Batch 201/320, Loss: 0.0139 0.0118\n",
      "training: 1:11:26<0:41:43 Epoch 1/10 - Batch 202/320, Loss: 0.0133 0.0107\n",
      "training: 1:11:45<0:41:21 Epoch 1/10 - Batch 203/320, Loss: 0.0120 0.0072\n",
      "training: 1:12:06<0:40:59 Epoch 1/10 - Batch 204/320, Loss: 0.0117 0.0105\n",
      "training: 1:12:26<0:40:38 Epoch 1/10 - Batch 205/320, Loss: 0.0117 0.0117\n",
      "training: 1:12:47<0:40:16 Epoch 1/10 - Batch 206/320, Loss: 0.0119 0.0127\n",
      "training: 1:13:08<0:39:55 Epoch 1/10 - Batch 207/320, Loss: 0.0126 0.0154\n",
      "training: 1:13:29<0:39:34 Epoch 1/10 - Batch 208/320, Loss: 0.0133 0.0163\n",
      "training: 1:13:49<0:39:12 Epoch 1/10 - Batch 209/320, Loss: 0.0125 0.0093\n",
      "training: 1:14:10<0:38:51 Epoch 1/10 - Batch 210/320, Loss: 0.0128 0.0138\n",
      "training: 1:14:32<0:38:30 Epoch 1/10 - Batch 211/320, Loss: 0.0133 0.0153\n",
      "training: 1:14:52<0:38:08 Epoch 1/10 - Batch 212/320, Loss: 0.0132 0.0130\n",
      "training: 1:15:14<0:37:47 Epoch 1/10 - Batch 213/320, Loss: 0.0130 0.0122\n",
      "training: 1:15:34<0:37:26 Epoch 1/10 - Batch 214/320, Loss: 0.0130 0.0129\n",
      "training: 1:15:56<0:37:05 Epoch 1/10 - Batch 215/320, Loss: 0.0136 0.0160\n",
      "training: 1:16:17<0:36:43 Epoch 1/10 - Batch 216/320, Loss: 0.0133 0.0122\n",
      "training: 1:16:38<0:36:22 Epoch 1/10 - Batch 217/320, Loss: 0.0130 0.0118\n",
      "training: 1:16:58<0:36:01 Epoch 1/10 - Batch 218/320, Loss: 0.0126 0.0110\n",
      "training: 1:17:19<0:35:39 Epoch 1/10 - Batch 219/320, Loss: 0.0127 0.0130\n",
      "training: 1:17:40<0:35:18 Epoch 1/10 - Batch 220/320, Loss: 0.0119 0.0088\n",
      "training: 1:18:01<0:34:57 Epoch 1/10 - Batch 221/320, Loss: 0.0115 0.0098\n",
      "training: 1:18:21<0:34:35 Epoch 1/10 - Batch 222/320, Loss: 0.0114 0.0112\n",
      "training: 1:18:42<0:34:14 Epoch 1/10 - Batch 223/320, Loss: 0.0111 0.0099\n",
      "training: 1:19:03<0:33:52 Epoch 1/10 - Batch 224/320, Loss: 0.0115 0.0130\n",
      "training: 1:19:24<0:33:31 Epoch 1/10 - Batch 225/320, Loss: 0.0121 0.0146\n",
      "training: 1:19:47<0:33:11 Epoch 1/10 - Batch 226/320, Loss: 0.0138 0.0207\n",
      "training: 1:20:08<0:32:50 Epoch 1/10 - Batch 227/320, Loss: 0.0145 0.0172\n",
      "training: 1:20:31<0:32:29 Epoch 1/10 - Batch 228/320, Loss: 0.0153 0.0184\n",
      "training: 1:20:52<0:32:08 Epoch 1/10 - Batch 229/320, Loss: 0.0153 0.0154\n",
      "training: 1:21:13<0:31:46 Epoch 1/10 - Batch 230/320, Loss: 0.0150 0.0138\n",
      "training: 1:21:36<0:31:26 Epoch 1/10 - Batch 231/320, Loss: 0.0161 0.0206\n",
      "training: 1:21:57<0:31:05 Epoch 1/10 - Batch 232/320, Loss: 0.0159 0.0151\n",
      "training: 1:22:19<0:30:44 Epoch 1/10 - Batch 233/320, Loss: 0.0159 0.0158\n",
      "training: 1:22:39<0:30:22 Epoch 1/10 - Batch 234/320, Loss: 0.0159 0.0160\n",
      "training: 1:23:01<0:30:01 Epoch 1/10 - Batch 235/320, Loss: 0.0155 0.0138\n",
      "training: 1:23:21<0:29:40 Epoch 1/10 - Batch 236/320, Loss: 0.0145 0.0103\n",
      "training: 1:23:43<0:29:19 Epoch 1/10 - Batch 237/320, Loss: 0.0141 0.0125\n",
      "training: 1:24:05<0:28:58 Epoch 1/10 - Batch 238/320, Loss: 0.0139 0.0131\n",
      "training: 1:24:26<0:28:37 Epoch 1/10 - Batch 239/320, Loss: 0.0135 0.0120\n",
      "training: 1:24:47<0:28:15 Epoch 1/10 - Batch 240/320, Loss: 0.0134 0.0131\n",
      "training: 1:25:08<0:27:54 Epoch 1/10 - Batch 241/320, Loss: 0.0126 0.0092\n",
      "training: 1:25:29<0:27:33 Epoch 1/10 - Batch 242/320, Loss: 0.0132 0.0158\n",
      "training: 1:25:50<0:27:11 Epoch 1/10 - Batch 243/320, Loss: 0.0125 0.0096\n",
      "training: 1:26:10<0:26:50 Epoch 1/10 - Batch 244/320, Loss: 0.0120 0.0100\n",
      "training: 1:26:32<0:26:29 Epoch 1/10 - Batch 245/320, Loss: 0.0121 0.0123\n",
      "training: 1:26:52<0:26:08 Epoch 1/10 - Batch 246/320, Loss: 0.0122 0.0128\n",
      "training: 1:27:15<0:25:47 Epoch 1/10 - Batch 247/320, Loss: 0.0133 0.0179\n",
      "training: 1:27:36<0:25:26 Epoch 1/10 - Batch 248/320, Loss: 0.0143 0.0181\n",
      "training: 1:27:57<0:25:04 Epoch 1/10 - Batch 249/320, Loss: 0.0136 0.0110\n",
      "training: 1:28:18<0:24:43 Epoch 1/10 - Batch 250/320, Loss: 0.0139 0.0150\n",
      "training: 1:28:39<0:24:22 Epoch 1/10 - Batch 251/320, Loss: 0.0139 0.0137\n",
      "training: 1:29:01<0:24:01 Epoch 1/10 - Batch 252/320, Loss: 0.0153 0.0213\n",
      "training: 1:29:22<0:23:40 Epoch 1/10 - Batch 253/320, Loss: 0.0143 0.0102\n",
      "training: 1:29:44<0:23:19 Epoch 1/10 - Batch 254/320, Loss: 0.0139 0.0122\n",
      "training: 1:30:06<0:22:58 Epoch 1/10 - Batch 255/320, Loss: 0.0145 0.0168\n",
      "training: 1:30:27<0:22:36 Epoch 1/10 - Batch 256/320, Loss: 0.0155 0.0193\n",
      "saving searchnet-bcemodel-0-256.pt\n",
      "\n",
      "\n",
      "Validation:\n",
      "\n",
      "validation: 1:30:37<0:22:12 Epoch 1/10 - Batch 257/320, Loss: 0.0002 0.0002\n",
      "validation: 1:30:45<0:21:48 Epoch 1/10 - Batch 258/320, Loss: 0.0001 0.0001\n",
      "validation: 1:30:55<0:21:24 Epoch 1/10 - Batch 259/320, Loss: 0.0002 0.0002\n",
      "validation: 1:31:03<0:21:00 Epoch 1/10 - Batch 260/320, Loss: 0.0002 0.0001\n",
      "validation: 1:31:12<0:20:37 Epoch 1/10 - Batch 261/320, Loss: 0.0002 0.0002\n",
      "validation: 1:31:21<0:20:13 Epoch 1/10 - Batch 262/320, Loss: 0.0002 0.0002\n",
      "validation: 1:31:30<0:19:49 Epoch 1/10 - Batch 263/320, Loss: 0.0002 0.0003\n",
      "validation: 1:31:37<0:19:26 Epoch 1/10 - Batch 264/320, Loss: 0.0002 0.0001\n",
      "validation: 1:31:45<0:19:02 Epoch 1/10 - Batch 265/320, Loss: 0.0002 0.0001\n",
      "validation: 1:31:55<0:18:39 Epoch 1/10 - Batch 266/320, Loss: 0.0002 0.0002\n",
      "validation: 1:32:04<0:18:16 Epoch 1/10 - Batch 267/320, Loss: 0.0002 0.0003\n",
      "validation: 1:32:12<0:17:53 Epoch 1/10 - Batch 268/320, Loss: 0.0002 0.0002\n",
      "validation: 1:32:20<0:17:30 Epoch 1/10 - Batch 269/320, Loss: 0.0002 0.0002\n",
      "validation: 1:32:28<0:17:07 Epoch 1/10 - Batch 270/320, Loss: 0.0002 0.0003\n",
      "validation: 1:32:38<0:16:44 Epoch 1/10 - Batch 271/320, Loss: 0.0002 0.0002\n",
      "validation: 1:32:46<0:16:22 Epoch 1/10 - Batch 272/320, Loss: 0.0002 0.0001\n",
      "validation: 1:32:54<0:15:59 Epoch 1/10 - Batch 273/320, Loss: 0.0002 0.0002\n",
      "validation: 1:33:01<0:15:37 Epoch 1/10 - Batch 274/320, Loss: 0.0002 0.0002\n",
      "validation: 1:33:10<0:15:14 Epoch 1/10 - Batch 275/320, Loss: 0.0002 0.0001\n",
      "validation: 1:33:18<0:14:52 Epoch 1/10 - Batch 276/320, Loss: 0.0002 0.0002\n",
      "validation: 1:33:27<0:14:30 Epoch 1/10 - Batch 277/320, Loss: 0.0002 0.0003\n",
      "validation: 1:33:34<0:14:08 Epoch 1/10 - Batch 278/320, Loss: 0.0002 0.0001\n",
      "validation: 1:33:42<0:13:46 Epoch 1/10 - Batch 279/320, Loss: 0.0002 0.0002\n",
      "validation: 1:33:50<0:13:24 Epoch 1/10 - Batch 280/320, Loss: 0.0002 0.0001\n",
      "validation: 1:33:57<0:13:02 Epoch 1/10 - Batch 281/320, Loss: 0.0002 0.0001\n",
      "validation: 1:34:06<0:12:40 Epoch 1/10 - Batch 282/320, Loss: 0.0002 0.0002\n",
      "validation: 1:34:13<0:12:19 Epoch 1/10 - Batch 283/320, Loss: 0.0002 0.0002\n",
      "validation: 1:34:22<0:11:57 Epoch 1/10 - Batch 284/320, Loss: 0.0002 0.0002\n",
      "validation: 1:34:30<0:11:36 Epoch 1/10 - Batch 285/320, Loss: 0.0002 0.0002\n",
      "validation: 1:34:38<0:11:15 Epoch 1/10 - Batch 286/320, Loss: 0.0002 0.0001\n",
      "validation: 1:34:47<0:10:53 Epoch 1/10 - Batch 287/320, Loss: 0.0002 0.0002\n",
      "validation: 1:34:54<0:10:32 Epoch 1/10 - Batch 288/320, Loss: 0.0002 0.0002\n",
      "validation: 1:35:04<0:10:11 Epoch 1/10 - Batch 289/320, Loss: 0.0002 0.0002\n",
      "validation: 1:35:11<0:09:50 Epoch 1/10 - Batch 290/320, Loss: 0.0002 0.0001\n",
      "validation: 1:35:19<0:09:30 Epoch 1/10 - Batch 291/320, Loss: 0.0002 0.0002\n",
      "validation: 1:35:28<0:09:09 Epoch 1/10 - Batch 292/320, Loss: 0.0002 0.0001\n",
      "validation: 1:35:36<0:08:48 Epoch 1/10 - Batch 293/320, Loss: 0.0002 0.0002\n",
      "validation: 1:35:45<0:08:28 Epoch 1/10 - Batch 294/320, Loss: 0.0002 0.0002\n",
      "validation: 1:35:53<0:08:07 Epoch 1/10 - Batch 295/320, Loss: 0.0002 0.0002\n",
      "validation: 1:36:02<0:07:47 Epoch 1/10 - Batch 296/320, Loss: 0.0002 0.0003\n",
      "validation: 1:36:09<0:07:26 Epoch 1/10 - Batch 297/320, Loss: 0.0002 0.0001\n",
      "validation: 1:36:17<0:07:06 Epoch 1/10 - Batch 298/320, Loss: 0.0002 0.0002\n",
      "validation: 1:36:25<0:06:46 Epoch 1/10 - Batch 299/320, Loss: 0.0002 0.0002\n",
      "validation: 1:36:33<0:06:26 Epoch 1/10 - Batch 300/320, Loss: 0.0002 0.0001\n",
      "validation: 1:36:42<0:06:06 Epoch 1/10 - Batch 301/320, Loss: 0.0002 0.0002\n",
      "validation: 1:36:51<0:05:46 Epoch 1/10 - Batch 302/320, Loss: 0.0002 0.0002\n",
      "validation: 1:36:59<0:05:26 Epoch 1/10 - Batch 303/320, Loss: 0.0002 0.0002\n",
      "validation: 1:37:07<0:05:06 Epoch 1/10 - Batch 304/320, Loss: 0.0002 0.0002\n",
      "validation: 1:37:16<0:04:47 Epoch 1/10 - Batch 305/320, Loss: 0.0002 0.0002\n",
      "validation: 1:37:24<0:04:27 Epoch 1/10 - Batch 306/320, Loss: 0.0002 0.0002\n",
      "validation: 1:37:31<0:04:07 Epoch 1/10 - Batch 307/320, Loss: 0.0002 0.0002\n",
      "validation: 1:37:39<0:03:48 Epoch 1/10 - Batch 308/320, Loss: 0.0002 0.0003\n",
      "validation: 1:37:48<0:03:28 Epoch 1/10 - Batch 309/320, Loss: 0.0002 0.0003\n",
      "validation: 1:37:57<0:03:09 Epoch 1/10 - Batch 310/320, Loss: 0.0002 0.0003\n",
      "validation: 1:38:06<0:02:50 Epoch 1/10 - Batch 311/320, Loss: 0.0002 0.0002\n",
      "validation: 1:38:15<0:02:31 Epoch 1/10 - Batch 312/320, Loss: 0.0002 0.0002\n",
      "validation: 1:38:23<0:02:12 Epoch 1/10 - Batch 313/320, Loss: 0.0002 0.0002\n",
      "validation: 1:38:32<0:01:52 Epoch 1/10 - Batch 314/320, Loss: 0.0002 0.0002\n",
      "validation: 1:38:40<0:01:33 Epoch 1/10 - Batch 315/320, Loss: 0.0002 0.0001\n",
      "validation: 1:38:48<0:01:15 Epoch 1/10 - Batch 316/320, Loss: 0.0002 0.0001\n",
      "validation: 1:38:55<0:00:56 Epoch 1/10 - Batch 317/320, Loss: 0.0002 0.0001\n",
      "validation: 1:39:04<0:00:37 Epoch 1/10 - Batch 318/320, Loss: 0.0002 0.0002\n",
      "validation: 1:39:11<0:00:18 Epoch 1/10 - Batch 319/320, Loss: 0.0002 0.0002\n",
      "validation: 1:39:13<0:00:00 Epoch 1/10 - Batch 320/320, Loss: 0.0002 0.0001\n",
      "\n",
      "Training\n",
      "\n",
      "training: 0:00:21<1:53:54 Epoch 2/11 - Batch 1/320, Loss: 0.0174 0.0174\n",
      "training: 0:00:42<1:53:15 Epoch 2/11 - Batch 2/320, Loss: 0.0158 0.0147\n",
      "training: 0:01:04<1:53:28 Epoch 2/11 - Batch 3/320, Loss: 0.0174 0.0198\n",
      "training: 0:01:26<1:53:34 Epoch 2/11 - Batch 4/320, Loss: 0.0158 0.0122\n",
      "training: 0:01:48<1:53:59 Epoch 2/11 - Batch 5/320, Loss: 0.0159 0.0163\n",
      "training: 0:02:09<1:53:17 Epoch 2/11 - Batch 6/320, Loss: 0.0165 0.0186\n",
      "training: 0:02:31<1:53:07 Epoch 2/11 - Batch 7/320, Loss: 0.0166 0.0168\n",
      "training: 0:02:53<1:52:49 Epoch 2/11 - Batch 8/320, Loss: 0.0158 0.0127\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m set_last_batch(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mhandle_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#model.load_state_dict(torch.load('searchnet-bcemodel-0-200.pt'))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 156\u001b[0m, in \u001b[0;36mhandle_epoch\u001b[0;34m(batch_size, num_epochs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (i, batch_data) \u001b[38;5;129;01min\u001b[39;00m get_epoch_part(i0, train_batchs, batch_size):\n\u001b[1;32m    155\u001b[0m     performance \u001b[38;5;241m=\u001b[39m (min_alpha, alpha, running_loss, start_time)\n\u001b[0;32m--> 156\u001b[0m     running_loss, alpha \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_train_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperformance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m i0 \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    160\u001b[0m save(epoch, i0)\n",
      "Cell \u001b[0;32mIn[29], line 109\u001b[0m, in \u001b[0;36mhandle_train_batch\u001b[0;34m(i, epoch, batch_data, performance, sizes)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandle_train_batch\u001b[39m(i, epoch, batch_data, performance, sizes):\n\u001b[1;32m    108\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 109\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    111\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[29], line 99\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(batch_data)\u001b[0m\n\u001b[1;32m     96\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention)\n\u001b[1;32m     98\u001b[0m region_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([region_ids \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m512\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(region_ids)) \u001b[38;5;28;01mfor\u001b[39;00m (input_ids, region_ids) \u001b[38;5;129;01min\u001b[39;00m batch_tokens])\n\u001b[0;32m---> 99\u001b[0m padding_mask \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mregion_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m labels[padding_mask] \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[padding_mask]\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_fn(outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state, labels)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "set_last_batch(0, 0)\n",
    "for epoch in range(num_epochs):\n",
    "    handle_epoch(batch_size, num_epochs)\n",
    "\n",
    "#model.load_state_dict(torch.load('searchnet-bcemodel-0-200.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78b5da4b-c5b6-4ced-9c0a-700d7ecc7a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_last_batch(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "932bf29a-b41d-48c4-be7f-a4e61abfc46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import torch\n",
    "device = torch.device('cuda')\n",
    "model = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e20882d-06a2-40de-96e1-645b661e1192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# Function to find the best matching between predicted and true labels\n",
    "def best_label_matching(y_true, y_pred):\n",
    "    D = np.max([y_pred.max(), y_true.max()]) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
    "    return list(zip(row_ind, col_ind))\n",
    "\n",
    "\n",
    "\n",
    "def compute_accuracy(data, true_labels):\n",
    "\n",
    "\n",
    "    unique_labels = np.unique(true_labels)\n",
    "    label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    true_labels = np.array([label_mapping[label] for label in true_labels])\n",
    "\n",
    "    \n",
    "    # Perform Hierarchical Agglomerative Clustering\n",
    "    Z = linkage(pdist(data), method='ward')\n",
    "    predicted_labels = fcluster(Z, t=len(set(true_labels)), criterion='maxclust')\n",
    "    \n",
    "    \n",
    "    # Find the best matching\n",
    "    ind = best_label_matching(true_labels, predicted_labels)\n",
    "    ind_dict = {i[0]: i[1] for i in ind}\n",
    "    \n",
    "    # Remap predicted labels to best matching true labels\n",
    "    remapped_predicted_labels = np.array([ind_dict.get(label, 0) for label in predicted_labels])\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    return accuracy_score(true_labels, remapped_predicted_labels)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a962ea4-5b19-4f37-972a-5985e53131e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.rand(512, 786)  # Replace with your actual data\n",
    "true_labels = np.random.randint(5, size=512)  # Replace with your actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0d7d52f-4906-4bdf-b6f9-d93446126463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.26171875\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {compute_accuracy(data, true_labels+10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a20d780a-15f0-4ab0-9f82-3a2c4e1c8d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "d = []\n",
    "def get_batcgh(batch_size, i):\n",
    "    batch, tokens = get_batch_data(batch_size, i)\n",
    "    input_ids, attention, label = (c.to(device) for c in batch)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention)[0]\n",
    "        data = outputs.cpu()[0].numpy()\n",
    "        true_labels = np.array(tokens[0][1])\n",
    "        size = len(true_labels[1:-1])\n",
    "        \n",
    "        data = data[1:size]\n",
    "        true_labels = true_labels[1:size]\n",
    "        \n",
    "        unique_labels = np.unique(true_labels)\n",
    "        label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        true_labels = np.array([label_mapping[label] for label in true_labels])\n",
    "        \n",
    "        Z = linkage(pdist(data), method='ward')\n",
    "        \n",
    "        predicted_labels = fcluster(Z, t=len(set(true_labels)), criterion='maxclust')\n",
    "        \n",
    "        ind = best_label_matching(true_labels, predicted_labels)\n",
    "        ind_dict = {i[0]: i[1] for i in ind}\n",
    "        \n",
    "        remapped_predicted_labels = np.array([ind_dict.get(label, 0) for label in predicted_labels])\n",
    "        return accuracy_score(true_labels, remapped_predicted_labels)\n",
    "\n",
    "get_batcgh(1, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fab2ca97-c058-4356-ac5a-57d932952fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [get_batcgh(1, x) for x in range(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "52c2711c-a137-45d0-9251-06df13a6e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [x for x in c if x != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "24e166f8-536c-44fa-9ac2-9706a636fcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4781032156239851"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(c)/len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a3b92e1c-6721-44a5-9f17-524f733f3710",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data, true_labels, input_ids), = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c4fe67bd-5f25-45bb-9e8b-a59334e78a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = len(true_labels[1:-1])\n",
    "\n",
    "data = data[1:size]\n",
    "true_labels = true_labels[1:size]\n",
    "\n",
    "unique_labels = np.unique(true_labels)\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "true_labels = np.array([label_mapping[label] for label in true_labels])\n",
    "\n",
    "Z = linkage(pdist(data), method='ward')\n",
    "\n",
    "predicted_labels = fcluster(Z, t=len(set(true_labels)), criterion='maxclust')\n",
    "\n",
    "ind = best_label_matching(true_labels, predicted_labels)\n",
    "ind_dict = {i[0]: i[1] for i in ind}\n",
    "\n",
    "remapped_predicted_labels = np.array([ind_dict.get(label, 0) for label in predicted_labels])\n",
    "accuracy_score(true_labels, remapped_predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4dc09c7f-0ddc-47af-be38-f4c17b996445",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[1:57]\n",
    "true_labels = true_labels[1:57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "494dde43-fa47-4cc6-8eac-0080e9ba575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = np.unique(true_labels)\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "true_labels = np.array([label_mapping[label] for label in true_labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da9bc09c-d573-41d6-8319-8a004a235836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(true_labels[:58])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a4b94a73-e426-4504-8753-975cae4d5901",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(pdist(data), method='ward')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2b786ff3-31a5-4706-a61a-eb79d9c64a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = fcluster(Z, t=len(set(true_labels)), criterion='maxclust')\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d40682bd-9ec2-4112-906a-6035b012335d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2094d89b-330e-412a-a26a-cf1cd0717f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ind = best_label_matching(true_labels, predicted_labels)\n",
    "ind_dict = {i[0]: i[1] for i in ind}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d493a7ca-9108-454e-a6fe-fa47c1d828ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "remapped_predicted_labels = np.array([ind_dict.get(label, 0) for label in predicted_labels])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "208166f6-98fb-4590-a915-7584eea06867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remapped_predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9c731101-d3e4-42ff-ba7f-a581abfd95a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(true_labels, remapped_predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff47af24-e147-4608-8445-3634f0482eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeSegmentationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.codebert = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "        self.conv = nn.Conv1d(in_channels=768, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bilstm = nn.LSTM(input_size=256, hidden_size=128, num_layers=1, bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get embeddings from CodeBERT\n",
    "        embeddings = self.codebert(input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        \n",
    "        # Mask padding embeddings\n",
    "        padding_mask = (input_ids != padding_token_id)  # Define padding_token_id\n",
    "        embeddings[~padding_mask] = 0\n",
    "\n",
    "        # Pass through additional layers\n",
    "        embeddings = embeddings.transpose(1, 2)  # Needed for Conv1d\n",
    "        conv_out = self.conv(embeddings).transpose(1, 2)\n",
    "        lstm_out, _ = self.bilstm(conv_out)\n",
    "        # Further processing and return\n",
    "\n",
    "model = CodeSegmentationModel(model)  # Initialize with pre-trained CodeBERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ad4722b-a048-4a19-ab54-9b8fab23af24",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
