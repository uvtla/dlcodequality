{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6db99f-03f8-4d9c-b270-11efa25b0cd7",
   "metadata": {},
   "source": [
    "# After getting files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8225e83-c777-4e68-8d88-1c2a82ca195f",
   "metadata": {},
   "source": [
    "## Insert into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00457c64-4536-4a2a-be83-04e1f1e66e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting dataset location\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "try:\n",
    "    dataset\n",
    "except:\n",
    "    print('getting dataset location')\n",
    "    dataset = Path('dataset').absolute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28b7e35-3703-4a56-9707-fadde8c3f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(dataset)\n",
    "\n",
    "js_dir = dataset / 'javascript'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40094612-df9b-4969-bdd3-94b0f201afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9c940ca-f75f-4f96-9960-3c301cdde7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3, json\n",
    "from tqdm import tqdm\n",
    "from contextlib import contextmanager\n",
    "\n",
    "train = js_dir / 'final' / 'jsonl' / 'train'\n",
    "os.chdir(train)\n",
    "\n",
    "@contextmanager\n",
    "def get_cursor(database_name='rsn_train'):\n",
    "    with sqlite3.connect(database_name) as conn:\n",
    "        yield conn.cursor()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b02a9475-3743-4131-bc5b-864a34c129c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "def simple_parse_xml(content, nested):\n",
    "    pattern = r'\\s*<([^\\s]*?)>\\s*'\n",
    "    open = re.search(pattern, content)\n",
    "    if not open:\n",
    "        return [content]\n",
    "    [open_start, open_end] = open.span()\n",
    "    before = content[:open_start]\n",
    "    tag = open.group(1)\n",
    "    inner_and_after = content[open_end:]\n",
    "    close = re.search(f'</{re.escape(tag)}>', inner_and_after)\n",
    "    if close:\n",
    "        [close_start, close_end] = close.span()\n",
    "    else:\n",
    "        close = re.search(pattern, inner_and_after)\n",
    "        if close:\n",
    "            [close_start, _] = close.span()\n",
    "        else:\n",
    "            close_start = len(inner_and_after)\n",
    "        close_end = close_start\n",
    "    inner = inner_and_after[:close_start]\n",
    "    after = inner_and_after[close_end:]\n",
    "    inner = simple_parse_xml(inner, nested) if nested else inner\n",
    "    return [[before, tag, inner], *simple_parse_xml(after, nested)]\n",
    "\n",
    "def atom_to_re(s):\n",
    "    tokens = [token.strip() for token in re.split(r'\\s+|(?=\\W)|(?<=\\W)', s.strip()) if token]\n",
    "    escaped = [regex.escape(token, special_only=True) for token in tokens]\n",
    "    return r'\\s*' + r'\\s*'.join(escaped) + r'\\s*'\n",
    "\n",
    "def str_to_re(s):\n",
    "    codes = re.split(r'\\s*(?://[^\\n]*(?:\\n|$)|/\\*.*?\\*/|\\.{3,})\\s*', s,  flags=re.DOTALL)\n",
    "    return '(?:.*?)'.join(atom_to_re(code) for code in codes)\n",
    "\n",
    "def node_to_re(node, c):\n",
    "    if type(node) == str:\n",
    "        return str_to_re(node), []\n",
    "    before, tag, content = node\n",
    "    before_re = str_to_re(before)\n",
    "    c[0] += 1\n",
    "    open_gr = c[0]\n",
    "    content_re, content_tags = make_regex(content, c)\n",
    "    c[0] += 1\n",
    "    close_gr = c[0]\n",
    "    open_re = '\\s*(|<'+re.escape(tag)+'>)\\s*'\n",
    "    close_re = '\\s*(|</'+re.escape(tag)+'>)\\s*'\n",
    "    return before_re+open_re+content_re+close_re, [(tag, open_gr, close_gr, content_tags)]\n",
    "\n",
    "\n",
    "def make_regex(tree, c):\n",
    "    regs, tags = zip(*(node_to_re(node, c) for node in tree))\n",
    "    return re.sub(r'(\\\\s\\*)+', r'\\\\s*', ''.join(regs)), [t for tag in tags for t in tag] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904bebf-5087-4d02-ad73-5e4f5aac1cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re\n",
    "omitted = []\n",
    "output = []\n",
    "for text_file in js_dir.glob('thread2-*.txt'):\n",
    "    outfile = str.replace(str(text_file), '.txt', '.json')\n",
    "    if os.path.exists(outfile):\n",
    "        continue\n",
    "    with open(text_file, 'r') as file:\n",
    "        file_contents = file.read()\n",
    "    parsed = simple_parse_xml(file_contents, nested=False)\n",
    "    output.append({\n",
    "        'file': str(text_file),\n",
    "        'split': [p[0] for p in parsed[:-1]] + [parsed[-1]],\n",
    "        'len': len(parsed),\n",
    "        'ok': len(parsed) == 11\n",
    "    })\n",
    "    if len(parsed) != 11:\n",
    "        omitted.append(text_file)\n",
    "        continue\n",
    "    with open(outfile, 'w') as file:\n",
    "        file.write(json.dumps([p[1:] for p in parsed]))\n",
    "\n",
    "with open('parse3.log', 'w') as file:\n",
    "    file.write(json.dumps(output))\n",
    "for out in output:\n",
    "    if not out['ok']:\n",
    "        continue\n",
    "    print(out['file'])\n",
    "    print(out['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709bdefe-5748-4a18-8be6-ae73ddaeead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    # cursor.execute('drop table if exists snippets')\n",
    "    # cursor.execute('drop table if exists region')\n",
    "    cursor.execute('create table if not exists snippets (ID INTEGER PRIMARY KEY, idx INTEGER, code TEXT, locations JSON, regions JSON, SRP boolean)')\n",
    "    cursor.execute('create table if not exists region (ID INTEGER PRIMARY KEY, code TEXT, vector JSON)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8996c197-897d-4d14-bb18-1034b3a38b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_region(code):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute('insert into region (code) values (?)', (code, ))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "def insert_snippet(id, index, code, locations, regions, srp):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute(\n",
    "            'insert into snippets (ID, idx, code, locations, regions, SRP) values (?, ?, ?, ?, ?, ?)',\n",
    "            (id, index, code, json.dumps(locations), json.dumps(regions), srp))\n",
    "\n",
    "def flat_wrong_tags(tags, code, m):\n",
    "    clean_tags = []\n",
    "    for tag in tags:\n",
    "        clean_tags += flat_wrong_tag(tag, clean_tags, code, m)\n",
    "    return clean_tags\n",
    "\n",
    "def flat_wrong_tag(tag, clean_tags, code, m):\n",
    "    name, open, close, sub_tags = tag\n",
    "    clean_sub_tags = flat_wrong_tags(sub_tags, code, m)\n",
    "    if m.group(open):\n",
    "        if len(clean_tags):\n",
    "            pname, popen, pclose, psub_tags = clean_tags[-1]\n",
    "            if not m.group(pclose):\n",
    "                clean_tags[-1] = (pname, popen, close, psub_tags)\n",
    "        return clean_sub_tags\n",
    "    return [(name, open, close, clean_sub_tags)]\n",
    "\n",
    "def tag_to_json(tag, code, m, handle_region):\n",
    "    name, open, close, sub_tags = tag\n",
    "    start = m.span(open)[0]\n",
    "    regions, body, end = tags_to_json(start, sub_tags, code, m, handle_region)\n",
    "    body += code[end:m.span(close)[0]] \n",
    "    region_id = handle_region(f'function {name} () {{\\n{body}\\n}}')\n",
    "    regions = [(start, region_id)] + regions\n",
    "    return regions\n",
    "\n",
    "\n",
    "def tags_to_json(outer_index, tags, code, m, handle_region):\n",
    "    regions = []\n",
    "    outer_body = ''\n",
    "    for tag in tags:\n",
    "        name, open, close, _ = tag\n",
    "        outer_body += code[outer_index:m.span(open)[0]] + '\\n' + name + '();\\n'\n",
    "        outer_index = m.span(close)[0]\n",
    "        regions += tag_to_json(tag, code, m, handle_region)\n",
    "    return regions, outer_body, outer_index\n",
    "\n",
    "\n",
    "def to_json(tags, code, m, handle_region):\n",
    "    regions, body, end = tags_to_json(0, tags, code, m, handle_region)\n",
    "    body += code[end:] \n",
    "    region_id = handle_region(body)\n",
    "    regions = [(0, region_id)] + regions\n",
    "    if len(regions) > 1:\n",
    "        regions.append((end, region_id))\n",
    "    return regions\n",
    "\n",
    "def strip_js_comments(js_code):\n",
    "    js_code = re.sub(r'\\n?//.*?\\n', '\\n', js_code)\n",
    "    js_code = re.sub(r'/\\*.*?\\*/', '', js_code, flags=re.DOTALL)\n",
    "    return js_code    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194654e4-f22c-452e-9dfb-5a953fb6a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ok, n_all = 0, 0\n",
    "for text_file in tqdm(list(js_dir.glob('thread2-*.json'))):\n",
    "    index = int(re.match('.*thread2-(.*)\\.json', str(text_file)).group(1))\n",
    "    limit = 10\n",
    "    with get_cursor() as cursor:\n",
    "        codes = list(cursor.execute('select id, code from shuffled limit ? offset ?', (limit, limit * index + 1)))\n",
    "    with open(text_file, 'r') as file:\n",
    "        file_contents = file.read()\n",
    "    for (id, code), obj in zip(codes, json.loads(file_contents)):\n",
    "        original = strip_js_comments(code)\n",
    "        xml = obj[1]\n",
    "        reg_str, tags = make_regex(simple_parse_xml(xml, True), [0])\n",
    "        reg = regex.compile(reg_str, flags = regex.DOTALL)\n",
    "        m = reg.match(original)\n",
    "        n_all += 1\n",
    "        if m:\n",
    "            n_ok += 1\n",
    "            tags = flat_wrong_tags(tags, original, m)\n",
    "            if len(tags) == 1 and not len(tags[0][3]):\n",
    "                tags = []\n",
    "            regions = to_json(tags, original, m, insert_region)\n",
    "            insert_snippet(id, index, original, *zip(*regions), len(tags) == 0)\n",
    "            \n",
    "            \n",
    "print(f'{n_ok}/{n_all}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a65ab-ab7c-4efe-a4d3-2f0f5145c12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    print(list(cursor.execute('select idx < 650, count(*), SRP from snippets group by idx < 650, SRP')))\n",
    "    print(list(cursor.execute('select count(*) from region')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fee2198e-ec9b-46ee-a700-4a102de567d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8663b41d-56f6-4ebd-ba46-8efbfe5be2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt(v):\n",
    "     return v / 2 + .5\n",
    "def get_labels(ids):\n",
    "    with get_cursor() as cursor:\n",
    "        regions = cursor.execute(f'select id, code from region where id in ({ids})')\n",
    "        for region in list(regions):\n",
    "            id, code = region\n",
    "            tokenized_inputs = tokenizer([code], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            tokenized_inputs.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**tokenized_inputs)\n",
    "                last_hidden_states = outputs.last_hidden_state\n",
    "            average_hidden_states = last_hidden_states.mean(dim=1)\n",
    "            yield id, list(average_hidden_states.cpu().numpy()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e81adab-31e7-478f-8f0c-9bc251b82e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5106\n"
     ]
    }
   ],
   "source": [
    "with get_cursor() as cursor:\n",
    "    print(next(cursor.execute('select count(*) from train_snippets'))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211bdf4d-2c35-4cd9-81b6-8f3775c59e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('drop table if exists tokenized')\n",
    "    cursor.execute('create table if not exists tokenized (ID INTEGER PRIMARY KEY, input_ids JSON, region_ids JSON)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655692b4-a8bc-4aeb-95f4-9a23fedb1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    snippets = list(cursor.execute('select id, code, locations, regions from snippets where idx < 650'))\n",
    "for (id, code, locations, regions) in tqdm(snippets):\n",
    "    tokens = tokenizer.encode_plus(code, truncation=True, return_offsets_mapping=True)\n",
    "    offset_mapping = tokens['offset_mapping']\n",
    "    regions, locations = json.loads(regions),json.loads(locations)\n",
    "    if not len(regions):\n",
    "        continue\n",
    "    locations.append(len(code))\n",
    "    i = 0\n",
    "    region_ids = []\n",
    "    for (start, end) in offset_mapping[1:-1]:\n",
    "        while start > locations[i+1]:\n",
    "            i += 1\n",
    "        region_ids.append(regions[i])\n",
    "    region_ids = [0] + region_ids + [0]\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute(\n",
    "            'insert into tokenized (ID, input_ids, region_ids) values (?, ?, ?)',\n",
    "            (id, json.dumps(tokens['input_ids']), json.dumps(region_ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bef707b-169d-4458-9f8c-18e57fd47407",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('drop table if exists train_snippets')\n",
    "    cursor.execute('create table train_snippets as select * from snippets where idx < 650')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5569bc6b-d6e2-453d-8dab-c16d2cca3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('create table if not exists current_batch(idx INTEGER)')\n",
    "    if not len(list(cursor.execute('select * from current_batch'))):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78bc3235-55f3-4130-bc94-40624dd4a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_cursor() as cursor:\n",
    "    cursor.execute('create table if not exists last_batch(idx INTEGER, epoch_idx INTEGER)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fda53217-c701-4ded-9c2b-c8f63e77a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def set_last_batch(last_batch, epoch_idx):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute('update last_batch set idx = ?, epoch_idx = ?', (last_batch, epoch_idx))\n",
    "\n",
    "def get_unsafe_last_batch(batches):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute('select * from last_batch')\n",
    "        item = cursor.fetchone()\n",
    "        if not item:\n",
    "            cursor.execute('insert into last_batch values (0, 0)')\n",
    "            return 0\n",
    "        else:\n",
    "            return item\n",
    "\n",
    "def get_last_batch(batches):\n",
    "    last_batch, last_epoch = get_unsafe_last_batch(batches)\n",
    "    if last_batch >= batches:\n",
    "        last_batch = 0\n",
    "        last_epoch += 1\n",
    "        set_last_batch(0, last_epoch)\n",
    "    return last_batch, last_epoch\n",
    "        \n",
    "def get_batch_tokens(batch_size, i):\n",
    "    with get_cursor() as cursor:\n",
    "        ids = ','.join(\n",
    "            str(x[0]) for x in cursor.execute('select id from train_snippets_shuffled limit ? offset ?', (batch_size, i))\n",
    "        )\n",
    "        tokens = cursor.execute(f'select input_ids, region_ids from tokenized where id in ({ids})')\n",
    "        zz = [(json.loads(input_ids), json.loads(region_ids)) for (input_ids, region_ids) in tokens]\n",
    "        print(batch_size, len(zz))\n",
    "        return zz\n",
    "        \n",
    "\n",
    "def get_batch_regions(tokens):\n",
    "    with get_cursor() as cursor:\n",
    "        region_ids_str = ','.join(set(str(region_id) for (_, region_ids) in tokens for region_id in region_ids))\n",
    "        return dict(get_labels(region_ids_str))\n",
    "\n",
    "def prepare_iteration(input_ids, region_ids, regions):\n",
    "    label_size = 768\n",
    "    size = len(input_ids)\n",
    "    if size > 512:\n",
    "        print(size)\n",
    "    empties = 512 - size\n",
    "    null_vector = [0] * label_size\n",
    "    for r_id in region_ids:\n",
    "        if r_id and not (r_id in regions):\n",
    "            print(r_id)\n",
    "    label = [regions[r_id] if r_id else null_vector for r_id in region_ids]\n",
    "    label += [null_vector] * empties\n",
    "    input_ids += [0] * empties\n",
    "    attention = [1] * size + [0] * empties\n",
    "    try:\n",
    "        torch.FloatTensor(label)\n",
    "    except:\n",
    "        print(regions[region_ids[1]])\n",
    "\n",
    "    return input_ids, attention, label\n",
    "\n",
    "def get_batch(batch_size, i):\n",
    "    tokens = get_batch_tokens(batch_size, i)\n",
    "    regions = get_batch_regions(tokens)\n",
    "    batch = [prepare_iteration(input_ids, region_ids, regions) for (input_ids, region_ids) in tokens]\n",
    "    input_ids, attention, label = zip(*batch)\n",
    "    return torch.IntTensor(input_ids), torch.FloatTensor(attention), torch.FloatTensor(label)\n",
    "\n",
    "\n",
    "def get_epoch_part(i0, epoch, batch_size):\n",
    "    return ((i, get_batch(batch_size, i)) for i in range(i0, epoch, batch_size))\n",
    "\n",
    "def tee(text):\n",
    "    print(f\"\\r{text}\\r\")\n",
    "    with open('train-resnet.log', 'a') as file:\n",
    "        file.write(f\"{text}\\n\")\n",
    "\n",
    "def log(title, start_time, sizes, epoch, i, mean_loss, loss):\n",
    "    _, num_epochs, batches = sizes\n",
    "    i += 1\n",
    "    dt = time.time() - start_time\n",
    "    elapsed = timedelta(seconds=int(dt))\n",
    "    remaining = timedelta(seconds=int(dt*(batches-i)/i))\n",
    "    text = f\"{title}: {elapsed}<{remaining} Epoch {epoch+1}/{num_epochs} - Batch {i}/{batches}, Loss: {mean_loss:.4f} {loss.item():.4f}\"\n",
    "    tee(text)\n",
    "\n",
    "def save(epoch, i):\n",
    "    \n",
    "    torch.save(model.state_dict(), f'searchnet-bcemodel-{epoch}-{i}.pt')\n",
    "    torch.save(optimizer.state_dict(), f'searchnet-bceoptimizer-{epoch}-{i}.pt')\n",
    "    set_last_batch(i, epoch)\n",
    "    tee(f'saving searchnet-bcemodel-{epoch}-{i}.pt\\n')\n",
    "\n",
    "def handle_train_batch(i, epoch, batch, performance, sizes):\n",
    "    input_ids, attention, labels = (c.to(device) for c in batch)\n",
    "    min_alpha, alpha, running_loss, start_time = performance\n",
    "    train_batchs, num_epochs, batches = sizes\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids, attention_mask=attention)\n",
    "    loss = loss_fn(outputs.last_hidden_state, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    current_alpha = min_alpha + alpha\n",
    "    running_loss = (1 - current_alpha) * running_loss + current_alpha * loss.item()\n",
    "    alpha *= .5\n",
    "    \n",
    "    log('training', start_time, sizes, epoch, i, running_loss, loss)\n",
    "    if i % 100 == 0:\n",
    "        save(epoch, i)\n",
    "    return running_loss, alpha\n",
    "    \n",
    "def handle_val_batch(i0, i, epoch, batch, start_time, cum_loss, sizes):\n",
    "    input_ids, attention, labels = (c.to(device) for c in batch)\n",
    "    _, num_epochs, batches = sizes\n",
    "\n",
    "    with torch.no_grad():            \n",
    "        outputs = model(input_ids, attention_mask=attention)\n",
    "        loss = loss_fn(outputs.last_hidden_state, labels)\n",
    "    \n",
    "    cum_loss += loss.item()\n",
    "    mean_loss = cum_loss / (i - i0 + 1)\n",
    "    \n",
    "    log('validation', start_time, sizes, epoch, i, mean_loss, loss)\n",
    "    return cum_loss\n",
    "    \n",
    "\n",
    "def handle_epoch(batch_size, num_epochs):\n",
    "    with get_cursor() as cursor:\n",
    "        epoch_size = next(cursor.execute('select count(*) from train_snippets'))[0]\n",
    "    batches = (epoch_size - 1) // batch_size + 1\n",
    "    train_batchs = 4 * batches // 5\n",
    "    i0, epoch0 = get_last_batch(train_batchs)\n",
    "    num_epochs += epoch0\n",
    "    i = i0 - 1\n",
    "    min_alpha = .2\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    alpha = 1 - min_alpha\n",
    "\n",
    "    model.train()\n",
    "    sizes = (train_batchs, num_epochs, batches)\n",
    "    tee('\\nTraining\\n')\n",
    "    for (i, batch) in get_epoch_part(i0, train_batchs, 1):\n",
    "        performance = (min_alpha, alpha, running_loss, start_time)\n",
    "        running_loss, alpha = handle_train_batch(i, epoch, batch, performance, sizes)\n",
    "    \n",
    "    \n",
    "    i0 = i + 1\n",
    "    save(epoch, i0)\n",
    "    \n",
    "    model.eval()\n",
    "    cum_loss = 0\n",
    "    tee('\\nValidation:\\n')\n",
    "    for (i, batch) in get_epoch_part(i0, batches, 1):\n",
    "        cum_loss = handle_val_batch(i0, i, epoch, batch, start_time, cum_loss, sizes)\n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59af576d-6e6b-4f4d-910d-ff23dab8cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_last_batch(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df189e27-706e-4ee0-81e2-ef57d1522b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training\n",
      "\n",
      "1 1\n",
      "training: 0:00:01<0:03:04 Epoch 1/10 - Batch 1/160, Loss: 0.7263 0.7263\n",
      "saving searchnet-bcemodel-0-0.pt\n",
      "\n",
      "1 1\n",
      "training: 0:00:02<0:03:53 Epoch 1/10 - Batch 2/160, Loss: 0.6616 0.6185\n",
      "1 1\n",
      "training: 0:00:03<0:03:12 Epoch 1/10 - Batch 3/160, Loss: 0.6695 0.6814\n",
      "1 1\n",
      "training: 0:00:04<0:02:50 Epoch 1/10 - Batch 4/160, Loss: 0.6795 0.7027\n",
      "1 1\n",
      "training: 0:00:05<0:02:36 Epoch 1/10 - Batch 5/160, Loss: 0.6869 0.7094\n",
      "1 1\n",
      "training: 0:00:05<0:02:27 Epoch 1/10 - Batch 6/160, Loss: 0.6841 0.6745\n",
      "1 1\n",
      "training: 0:00:06<0:02:21 Epoch 1/10 - Batch 7/160, Loss: 0.6885 0.7048\n",
      "1 1\n",
      "training: 0:00:07<0:02:17 Epoch 1/10 - Batch 8/160, Loss: 0.6349 0.4287\n",
      "1 1\n",
      "training: 0:00:07<0:02:12 Epoch 1/10 - Batch 9/160, Loss: 0.6523 0.7203\n",
      "1 1\n",
      "training: 0:00:08<0:02:08 Epoch 1/10 - Batch 10/160, Loss: 0.6260 0.5218\n",
      "1 1\n",
      "training: 0:00:09<0:02:05 Epoch 1/10 - Batch 11/160, Loss: 0.6373 0.6822\n",
      "1 1\n",
      "training: 0:00:09<0:02:02 Epoch 1/10 - Batch 12/160, Loss: 0.6503 0.7023\n",
      "1 1\n",
      "training: 0:00:10<0:02:00 Epoch 1/10 - Batch 13/160, Loss: 0.6364 0.5810\n",
      "1 1\n",
      "training: 0:00:11<0:01:58 Epoch 1/10 - Batch 14/160, Loss: 0.6429 0.6687\n",
      "1 1\n",
      "training: 0:00:12<0:01:56 Epoch 1/10 - Batch 15/160, Loss: 0.6547 0.7018\n",
      "1 1\n",
      "training: 0:00:12<0:01:54 Epoch 1/10 - Batch 16/160, Loss: 0.6452 0.6073\n",
      "1 1\n",
      "training: 0:00:13<0:01:52 Epoch 1/10 - Batch 17/160, Loss: 0.6536 0.6873\n",
      "1 1\n",
      "training: 0:00:14<0:01:51 Epoch 1/10 - Batch 18/160, Loss: 0.6482 0.6263\n",
      "1 1\n",
      "training: 0:00:14<0:01:50 Epoch 1/10 - Batch 19/160, Loss: 0.6038 0.4264\n",
      "1 1\n",
      "training: 0:00:15<0:01:48 Epoch 1/10 - Batch 20/160, Loss: 0.6196 0.6827\n",
      "1 1\n",
      "training: 0:00:16<0:01:48 Epoch 1/10 - Batch 21/160, Loss: 0.5657 0.3500\n",
      "1 1\n",
      "training: 0:00:17<0:01:46 Epoch 1/10 - Batch 22/160, Loss: 0.5874 0.6742\n",
      "1 1\n",
      "training: 0:00:17<0:01:45 Epoch 1/10 - Batch 23/160, Loss: 0.5672 0.4864\n",
      "1 1\n",
      "training: 0:00:18<0:01:44 Epoch 1/10 - Batch 24/160, Loss: 0.5706 0.5841\n",
      "1 1\n",
      "training: 0:00:19<0:01:43 Epoch 1/10 - Batch 25/160, Loss: 0.5394 0.4149\n",
      "1 1\n",
      "training: 0:00:19<0:01:42 Epoch 1/10 - Batch 26/160, Loss: 0.5412 0.5483\n",
      "1 1\n",
      "training: 0:00:20<0:01:40 Epoch 1/10 - Batch 27/160, Loss: 0.5075 0.3727\n",
      "1 1\n",
      "training: 0:00:21<0:01:39 Epoch 1/10 - Batch 28/160, Loss: 0.5235 0.5875\n",
      "1 1\n",
      "training: 0:00:21<0:01:38 Epoch 1/10 - Batch 29/160, Loss: 0.5451 0.6316\n",
      "1 1\n",
      "training: 0:00:22<0:01:37 Epoch 1/10 - Batch 30/160, Loss: 0.4804 0.2217\n",
      "1 1\n",
      "training: 0:00:23<0:01:36 Epoch 1/10 - Batch 31/160, Loss: 0.5059 0.6079\n",
      "1 1\n",
      "training: 0:00:23<0:01:35 Epoch 1/10 - Batch 32/160, Loss: 0.4383 0.1676\n",
      "1 1\n",
      "training: 0:00:24<0:01:34 Epoch 1/10 - Batch 33/160, Loss: 0.3951 0.2227\n",
      "1 1\n",
      "training: 0:00:25<0:01:34 Epoch 1/10 - Batch 34/160, Loss: 0.3425 0.1320\n",
      "1 1\n",
      "training: 0:00:26<0:01:33 Epoch 1/10 - Batch 35/160, Loss: 0.3652 0.4558\n",
      "1 1\n",
      "training: 0:00:26<0:01:32 Epoch 1/10 - Batch 36/160, Loss: 0.4005 0.5418\n",
      "1 1\n",
      "training: 0:00:27<0:01:31 Epoch 1/10 - Batch 37/160, Loss: 0.3584 0.1901\n",
      "1 1\n",
      "training: 0:00:28<0:01:30 Epoch 1/10 - Batch 38/160, Loss: 0.3925 0.5288\n",
      "1 1\n",
      "training: 0:00:28<0:01:29 Epoch 1/10 - Batch 39/160, Loss: 0.3648 0.2542\n",
      "1 1\n",
      "training: 0:00:29<0:01:28 Epoch 1/10 - Batch 40/160, Loss: 0.3775 0.4280\n",
      "1 1\n",
      "training: 0:00:30<0:01:27 Epoch 1/10 - Batch 41/160, Loss: 0.3154 0.0670\n",
      "1 1\n",
      "training: 0:00:30<0:01:27 Epoch 1/10 - Batch 42/160, Loss: 0.3600 0.5382\n",
      "1 1\n",
      "training: 0:00:31<0:01:26 Epoch 1/10 - Batch 43/160, Loss: 0.3920 0.5201\n",
      "1 1\n",
      "training: 0:00:32<0:01:25 Epoch 1/10 - Batch 44/160, Loss: 0.4478 0.6708\n",
      "1 1\n",
      "training: 0:00:33<0:01:24 Epoch 1/10 - Batch 45/160, Loss: 0.3868 0.1429\n",
      "1 1\n",
      "training: 0:00:33<0:01:23 Epoch 1/10 - Batch 46/160, Loss: 0.3705 0.3055\n",
      "1 1\n",
      "training: 0:00:34<0:01:23 Epoch 1/10 - Batch 47/160, Loss: 0.3073 0.0543\n",
      "1 1\n",
      "training: 0:00:35<0:01:22 Epoch 1/10 - Batch 48/160, Loss: 0.3188 0.3650\n",
      "1 1\n",
      "training: 0:00:35<0:01:21 Epoch 1/10 - Batch 49/160, Loss: 0.3638 0.5435\n",
      "1 1\n",
      "training: 0:00:36<0:01:20 Epoch 1/10 - Batch 50/160, Loss: 0.4269 0.6792\n",
      "1 1\n",
      "training: 0:00:37<0:01:19 Epoch 1/10 - Batch 51/160, Loss: 0.3891 0.2379\n",
      "1 1\n",
      "training: 0:00:38<0:01:18 Epoch 1/10 - Batch 52/160, Loss: 0.3100 -0.0063\n",
      "1 1\n",
      "training: 0:00:38<0:01:18 Epoch 1/10 - Batch 53/160, Loss: 0.3313 0.4163\n",
      "1 1\n",
      "training: 0:00:39<0:01:17 Epoch 1/10 - Batch 54/160, Loss: 0.3770 0.5601\n",
      "1 1\n",
      "training: 0:00:40<0:01:16 Epoch 1/10 - Batch 55/160, Loss: 0.2914 -0.0512\n",
      "1 1\n",
      "training: 0:00:40<0:01:15 Epoch 1/10 - Batch 56/160, Loss: 0.3170 0.4196\n",
      "1 1\n",
      "training: 0:00:41<0:01:14 Epoch 1/10 - Batch 57/160, Loss: 0.3628 0.5460\n",
      "1 1\n",
      "training: 0:00:42<0:01:14 Epoch 1/10 - Batch 58/160, Loss: 0.3498 0.2977\n",
      "1 1\n",
      "training: 0:00:42<0:01:13 Epoch 1/10 - Batch 59/160, Loss: 0.2741 -0.0289\n",
      "1 1\n",
      "training: 0:00:43<0:01:12 Epoch 1/10 - Batch 60/160, Loss: 0.3454 0.6307\n",
      "1 1\n",
      "training: 0:00:44<0:01:11 Epoch 1/10 - Batch 61/160, Loss: 0.4034 0.6356\n",
      "1 1\n",
      "training: 0:00:44<0:01:10 Epoch 1/10 - Batch 62/160, Loss: 0.4345 0.5585\n",
      "1 1\n",
      "training: 0:00:45<0:01:10 Epoch 1/10 - Batch 63/160, Loss: 0.4731 0.6275\n",
      "1 1\n",
      "training: 0:00:46<0:01:09 Epoch 1/10 - Batch 64/160, Loss: 0.4643 0.4290\n",
      "1 1\n",
      "training: 0:00:46<0:01:08 Epoch 1/10 - Batch 65/160, Loss: 0.3629 -0.0426\n",
      "1 1\n",
      "training: 0:00:47<0:01:07 Epoch 1/10 - Batch 66/160, Loss: 0.4250 0.6733\n",
      "1 1\n",
      "training: 0:00:48<0:01:07 Epoch 1/10 - Batch 67/160, Loss: 0.4247 0.4237\n",
      "1 1\n",
      "training: 0:00:49<0:01:06 Epoch 1/10 - Batch 68/160, Loss: 0.4054 0.3279\n",
      "1 1\n",
      "training: 0:00:49<0:01:05 Epoch 1/10 - Batch 69/160, Loss: 0.3952 0.3544\n",
      "1 1\n",
      "training: 0:00:50<0:01:04 Epoch 1/10 - Batch 70/160, Loss: 0.4333 0.5858\n",
      "1 1\n",
      "training: 0:00:51<0:01:04 Epoch 1/10 - Batch 71/160, Loss: 0.3344 -0.0610\n",
      "1 1\n",
      "training: 0:00:51<0:01:03 Epoch 1/10 - Batch 72/160, Loss: 0.3992 0.6583\n",
      "1 1\n",
      "training: 0:00:52<0:01:02 Epoch 1/10 - Batch 73/160, Loss: 0.2951 -0.1215\n",
      "1 1\n",
      "training: 0:00:53<0:01:02 Epoch 1/10 - Batch 74/160, Loss: 0.3581 0.6101\n",
      "1 1\n",
      "training: 0:00:54<0:01:01 Epoch 1/10 - Batch 75/160, Loss: 0.2608 -0.1283\n",
      "1 1\n",
      "training: 0:00:54<0:01:00 Epoch 1/10 - Batch 76/160, Loss: 0.2624 0.2688\n",
      "1 1\n",
      "training: 0:00:55<0:00:59 Epoch 1/10 - Batch 77/160, Loss: 0.2906 0.4035\n",
      "1 1\n",
      "training: 0:00:56<0:00:59 Epoch 1/10 - Batch 78/160, Loss: 0.2496 0.0856\n",
      "1 1\n",
      "training: 0:00:56<0:00:58 Epoch 1/10 - Batch 79/160, Loss: 0.1728 -0.1343\n",
      "1 1\n",
      "training: 0:00:57<0:00:57 Epoch 1/10 - Batch 80/160, Loss: 0.2551 0.5841\n",
      "1 1\n",
      "training: 0:00:58<0:00:56 Epoch 1/10 - Batch 81/160, Loss: 0.2607 0.2831\n",
      "1 1\n",
      "training: 0:00:59<0:00:56 Epoch 1/10 - Batch 82/160, Loss: 0.3335 0.6246\n",
      "1 1\n",
      "training: 0:00:59<0:00:55 Epoch 1/10 - Batch 83/160, Loss: 0.3384 0.3582\n",
      "1 1\n",
      "training: 0:01:00<0:00:54 Epoch 1/10 - Batch 84/160, Loss: 0.2488 -0.1096\n",
      "1 1\n",
      "training: 0:01:01<0:00:53 Epoch 1/10 - Batch 85/160, Loss: 0.2655 0.3323\n",
      "1 1\n",
      "training: 0:01:01<0:00:53 Epoch 1/10 - Batch 86/160, Loss: 0.3136 0.5059\n",
      "1 1\n",
      "training: 0:01:02<0:00:52 Epoch 1/10 - Batch 87/160, Loss: 0.2275 -0.1169\n",
      "1 1\n",
      "training: 0:01:03<0:00:51 Epoch 1/10 - Batch 88/160, Loss: 0.2565 0.3725\n",
      "1 1\n",
      "training: 0:01:03<0:00:51 Epoch 1/10 - Batch 89/160, Loss: 0.2277 0.1125\n",
      "1 1\n",
      "training: 0:01:04<0:00:50 Epoch 1/10 - Batch 90/160, Loss: 0.2402 0.2904\n",
      "1 1\n",
      "training: 0:01:05<0:00:49 Epoch 1/10 - Batch 91/160, Loss: 0.2590 0.3340\n",
      "1 1\n",
      "training: 0:01:06<0:00:48 Epoch 1/10 - Batch 92/160, Loss: 0.1625 -0.2236\n",
      "1 1\n",
      "training: 0:01:06<0:00:48 Epoch 1/10 - Batch 93/160, Loss: 0.2570 0.6349\n",
      "1 1\n",
      "training: 0:01:07<0:00:47 Epoch 1/10 - Batch 94/160, Loss: 0.1658 -0.1990\n",
      "1 1\n",
      "training: 0:01:08<0:00:46 Epoch 1/10 - Batch 95/160, Loss: 0.1502 0.0881\n",
      "1 1\n",
      "training: 0:01:08<0:00:45 Epoch 1/10 - Batch 96/160, Loss: 0.2256 0.5273\n",
      "1 1\n",
      "training: 0:01:09<0:00:45 Epoch 1/10 - Batch 97/160, Loss: 0.1970 0.0825\n",
      "1 1\n",
      "training: 0:01:10<0:00:44 Epoch 1/10 - Batch 98/160, Loss: 0.1795 0.1093\n",
      "1 1\n",
      "training: 0:01:11<0:00:43 Epoch 1/10 - Batch 99/160, Loss: 0.2109 0.3369\n",
      "1 1\n",
      "training: 0:01:11<0:00:43 Epoch 1/10 - Batch 100/160, Loss: 0.1486 -0.1007\n",
      "1 1\n",
      "training: 0:01:12<0:00:42 Epoch 1/10 - Batch 101/160, Loss: 0.0694 -0.2475\n",
      "saving searchnet-bcemodel-0-100.pt\n",
      "\n",
      "1 1\n",
      "training: 0:01:14<0:00:42 Epoch 1/10 - Batch 102/160, Loss: 0.0016 -0.2694\n",
      "1 1\n",
      "training: 0:01:15<0:00:41 Epoch 1/10 - Batch 103/160, Loss: 0.0989 0.4879\n",
      "1 1\n",
      "training: 0:01:16<0:00:41 Epoch 1/10 - Batch 104/160, Loss: 0.0311 -0.2399\n",
      "1 1\n",
      "training: 0:01:16<0:00:40 Epoch 1/10 - Batch 105/160, Loss: 0.1360 0.5556\n",
      "1 1\n",
      "training: 0:01:17<0:00:39 Epoch 1/10 - Batch 106/160, Loss: 0.1796 0.3541\n",
      "1 1\n",
      "training: 0:01:18<0:00:38 Epoch 1/10 - Batch 107/160, Loss: 0.0894 -0.2717\n",
      "1 1\n",
      "training: 0:01:19<0:00:38 Epoch 1/10 - Batch 108/160, Loss: 0.1197 0.2409\n",
      "1 1\n",
      "training: 0:01:19<0:00:37 Epoch 1/10 - Batch 109/160, Loss: 0.1450 0.2463\n",
      "1 1\n",
      "training: 0:01:20<0:00:36 Epoch 1/10 - Batch 110/160, Loss: 0.0896 -0.1322\n",
      "1 1\n",
      "training: 0:01:21<0:00:35 Epoch 1/10 - Batch 111/160, Loss: 0.1814 0.5489\n",
      "1 1\n",
      "training: 0:01:21<0:00:35 Epoch 1/10 - Batch 112/160, Loss: 0.2533 0.5409\n",
      "1 1\n",
      "training: 0:01:22<0:00:34 Epoch 1/10 - Batch 113/160, Loss: 0.2498 0.2360\n",
      "1 1\n",
      "training: 0:01:23<0:00:33 Epoch 1/10 - Batch 114/160, Loss: 0.1770 -0.1143\n",
      "1 1\n",
      "training: 0:01:23<0:00:32 Epoch 1/10 - Batch 115/160, Loss: 0.1245 -0.0858\n",
      "1 1\n",
      "training: 0:01:24<0:00:32 Epoch 1/10 - Batch 116/160, Loss: 0.1465 0.2344\n",
      "1 1\n",
      "training: 0:01:25<0:00:31 Epoch 1/10 - Batch 117/160, Loss: 0.2392 0.6102\n",
      "1 1\n",
      "training: 0:01:26<0:00:30 Epoch 1/10 - Batch 118/160, Loss: 0.3133 0.6095\n",
      "1 1\n",
      "training: 0:01:26<0:00:29 Epoch 1/10 - Batch 119/160, Loss: 0.3552 0.5228\n",
      "1 1\n",
      "training: 0:01:27<0:00:29 Epoch 1/10 - Batch 120/160, Loss: 0.3980 0.5691\n",
      "1 1\n",
      "training: 0:01:28<0:00:28 Epoch 1/10 - Batch 121/160, Loss: 0.4396 0.6063\n",
      "1 1\n",
      "training: 0:01:28<0:00:27 Epoch 1/10 - Batch 122/160, Loss: 0.2892 -0.3126\n",
      "1 1\n",
      "training: 0:01:29<0:00:26 Epoch 1/10 - Batch 123/160, Loss: 0.1680 -0.3165\n",
      "1 1\n",
      "training: 0:01:30<0:00:26 Epoch 1/10 - Batch 124/160, Loss: 0.0697 -0.3238\n",
      "1 1\n",
      "training: 0:01:30<0:00:25 Epoch 1/10 - Batch 125/160, Loss: 0.1799 0.6206\n",
      "1 1\n",
      "training: 0:01:31<0:00:24 Epoch 1/10 - Batch 126/160, Loss: 0.0868 -0.2855\n",
      "1 1\n",
      "training: 0:01:32<0:00:23 Epoch 1/10 - Batch 127/160, Loss: 0.1917 0.6115\n",
      "1 1\n",
      "training: 0:01:32<0:00:23 Epoch 1/10 - Batch 128/160, Loss: 0.2813 0.6395\n",
      "saving searchnet-bcemodel-0-128.pt\n",
      "\n",
      "\n",
      "Validation:\n",
      "\n",
      "1 1\n",
      "validation: 0:01:34<0:00:22 Epoch 1/10 - Batch 129/160, Loss: -0.3276 -0.3276\n",
      "1 1\n",
      "validation: 0:01:34<0:00:21 Epoch 1/10 - Batch 130/160, Loss: 0.0054 0.3383\n",
      "1 1\n",
      "validation: 0:01:34<0:00:20 Epoch 1/10 - Batch 131/160, Loss: 0.1191 0.3466\n",
      "1 1\n",
      "validation: 0:01:34<0:00:20 Epoch 1/10 - Batch 132/160, Loss: 0.1058 0.0659\n",
      "1 1\n",
      "validation: 0:01:34<0:00:19 Epoch 1/10 - Batch 133/160, Loss: 0.0048 -0.3992\n",
      "1 1\n",
      "validation: 0:01:34<0:00:18 Epoch 1/10 - Batch 134/160, Loss: 0.0965 0.5550\n",
      "1 1\n",
      "validation: 0:01:34<0:00:17 Epoch 1/10 - Batch 135/160, Loss: 0.1674 0.5928\n",
      "1 1\n",
      "validation: 0:01:34<0:00:16 Epoch 1/10 - Batch 136/160, Loss: 0.0952 -0.4102\n",
      "1 1\n",
      "validation: 0:01:34<0:00:15 Epoch 1/10 - Batch 137/160, Loss: 0.1287 0.3963\n",
      "1 1\n",
      "validation: 0:01:34<0:00:15 Epoch 1/10 - Batch 138/160, Loss: 0.1443 0.2846\n",
      "1 1\n",
      "validation: 0:01:34<0:00:14 Epoch 1/10 - Batch 139/160, Loss: 0.1865 0.6093\n",
      "1 1\n",
      "validation: 0:01:34<0:00:13 Epoch 1/10 - Batch 140/160, Loss: 0.1379 -0.3970\n",
      "1 1\n",
      "validation: 0:01:35<0:00:12 Epoch 1/10 - Batch 141/160, Loss: 0.1627 0.4609\n",
      "1 1\n",
      "validation: 0:01:35<0:00:12 Epoch 1/10 - Batch 142/160, Loss: 0.1952 0.6173\n",
      "1 1\n",
      "validation: 0:01:35<0:00:11 Epoch 1/10 - Batch 143/160, Loss: 0.2226 0.6060\n",
      "1 1\n",
      "validation: 0:01:35<0:00:10 Epoch 1/10 - Batch 144/160, Loss: 0.2395 0.4922\n",
      "1 1\n",
      "validation: 0:01:35<0:00:09 Epoch 1/10 - Batch 145/160, Loss: 0.2606 0.5987\n",
      "1 1\n",
      "validation: 0:01:35<0:00:09 Epoch 1/10 - Batch 146/160, Loss: 0.2688 0.4089\n",
      "1 1\n",
      "validation: 0:01:35<0:00:08 Epoch 1/10 - Batch 147/160, Loss: 0.2795 0.4724\n",
      "1 1\n",
      "validation: 0:01:35<0:00:07 Epoch 1/10 - Batch 148/160, Loss: 0.2915 0.5188\n",
      "1 1\n",
      "validation: 0:01:35<0:00:07 Epoch 1/10 - Batch 149/160, Loss: 0.2980 0.4272\n",
      "1 1\n",
      "validation: 0:01:35<0:00:06 Epoch 1/10 - Batch 150/160, Loss: 0.2982 0.3026\n",
      "1 1\n",
      "validation: 0:01:35<0:00:05 Epoch 1/10 - Batch 151/160, Loss: 0.2688 -0.3764\n",
      "1 1\n",
      "validation: 0:01:35<0:00:05 Epoch 1/10 - Batch 152/160, Loss: 0.2732 0.3744\n",
      "1 1\n",
      "validation: 0:01:35<0:00:04 Epoch 1/10 - Batch 153/160, Loss: 0.2624 0.0012\n",
      "1 1\n",
      "validation: 0:01:35<0:00:03 Epoch 1/10 - Batch 154/160, Loss: 0.2657 0.3486\n",
      "1 1\n",
      "validation: 0:01:35<0:00:03 Epoch 1/10 - Batch 155/160, Loss: 0.2755 0.5314\n",
      "1 1\n",
      "validation: 0:01:36<0:00:02 Epoch 1/10 - Batch 156/160, Loss: 0.2856 0.5577\n",
      "1 1\n",
      "validation: 0:01:36<0:00:01 Epoch 1/10 - Batch 157/160, Loss: 0.2647 -0.3197\n",
      "1 1\n",
      "validation: 0:01:36<0:00:01 Epoch 1/10 - Batch 158/160, Loss: 0.2426 -0.4003\n",
      "1 1\n",
      "validation: 0:01:36<0:00:00 Epoch 1/10 - Batch 159/160, Loss: 0.2226 -0.3766\n",
      "1 1\n",
      "validation: 0:01:36<0:00:00 Epoch 1/10 - Batch 160/160, Loss: 0.2047 -0.3501\n",
      "\n",
      "Training\n",
      "\n",
      "1 1\n",
      "training: 0:00:00<0:01:51 Epoch 2/11 - Batch 1/160, Loss: 0.5510 0.5510\n",
      "saving searchnet-bcemodel-1-0.pt\n",
      "\n",
      "1 1\n",
      "training: 0:00:02<0:03:44 Epoch 2/11 - Batch 2/160, Loss: 0.0642 -0.2603\n",
      "1 1\n",
      "training: 0:00:03<0:03:06 Epoch 2/11 - Batch 3/160, Loss: 0.1589 0.3008\n",
      "1 1\n",
      "training: 0:00:04<0:02:45 Epoch 2/11 - Batch 4/160, Loss: 0.2520 0.4693\n",
      "1 1\n",
      "training: 0:00:04<0:02:32 Epoch 2/11 - Batch 5/160, Loss: 0.3196 0.5225\n",
      "1 1\n",
      "training: 0:00:05<0:02:24 Epoch 2/11 - Batch 6/160, Loss: 0.3122 0.2866\n",
      "1 1\n",
      "training: 0:00:06<0:02:18 Epoch 2/11 - Batch 7/160, Loss: 0.3570 0.5230\n",
      "1 1\n",
      "training: 0:00:07<0:02:13 Epoch 2/11 - Batch 8/160, Loss: 0.2421 -0.1999\n",
      "1 1\n",
      "training: 0:00:07<0:02:09 Epoch 2/11 - Batch 9/160, Loss: 0.3162 0.6066\n",
      "1 1\n",
      "training: 0:00:08<0:02:06 Epoch 2/11 - Batch 10/160, Loss: 0.2001 -0.2596\n",
      "1 1\n",
      "training: 0:00:09<0:02:03 Epoch 2/11 - Batch 11/160, Loss: 0.2647 0.5220\n",
      "1 1\n",
      "training: 0:00:09<0:02:00 Epoch 2/11 - Batch 12/160, Loss: 0.3233 0.5568\n",
      "1 1\n",
      "training: 0:00:10<0:01:58 Epoch 2/11 - Batch 13/160, Loss: 0.3025 0.2195\n",
      "1 1\n",
      "training: 0:00:11<0:01:56 Epoch 2/11 - Batch 14/160, Loss: 0.3306 0.4428\n",
      "1 1\n",
      "training: 0:00:11<0:01:54 Epoch 2/11 - Batch 15/160, Loss: 0.3818 0.5867\n",
      "1 1\n",
      "training: 0:00:12<0:01:53 Epoch 2/11 - Batch 16/160, Loss: 0.3650 0.2979\n",
      "1 1\n",
      "training: 0:00:13<0:01:51 Epoch 2/11 - Batch 17/160, Loss: 0.4027 0.5535\n",
      "1 1\n",
      "training: 0:00:13<0:01:49 Epoch 2/11 - Batch 18/160, Loss: 0.3963 0.3707\n",
      "1 1\n",
      "training: 0:00:14<0:01:48 Epoch 2/11 - Batch 19/160, Loss: 0.3142 -0.0145\n",
      "1 1\n",
      "training: 0:00:15<0:01:47 Epoch 2/11 - Batch 20/160, Loss: 0.3590 0.5382\n",
      "1 1\n",
      "training: 0:00:16<0:01:47 Epoch 2/11 - Batch 21/160, Loss: 0.2294 -0.2890\n",
      "1 1\n",
      "training: 0:00:16<0:01:45 Epoch 2/11 - Batch 22/160, Loss: 0.2932 0.5486\n",
      "1 1\n",
      "training: 0:00:17<0:01:44 Epoch 2/11 - Batch 23/160, Loss: 0.2580 0.1171\n",
      "1 1\n",
      "training: 0:00:18<0:01:43 Epoch 2/11 - Batch 24/160, Loss: 0.2821 0.3784\n",
      "1 1\n",
      "training: 0:00:18<0:01:42 Epoch 2/11 - Batch 25/160, Loss: 0.2347 0.0454\n",
      "1 1\n",
      "training: 0:00:19<0:01:41 Epoch 2/11 - Batch 26/160, Loss: 0.2446 0.2843\n",
      "1 1\n",
      "training: 0:00:20<0:01:40 Epoch 2/11 - Batch 27/160, Loss: 0.1892 -0.0326\n",
      "1 1\n",
      "training: 0:00:21<0:01:39 Epoch 2/11 - Batch 28/160, Loss: 0.2245 0.3659\n",
      "1 1\n",
      "training: 0:00:21<0:01:38 Epoch 2/11 - Batch 29/160, Loss: 0.2831 0.5174\n",
      "1 1\n",
      "training: 0:00:22<0:01:37 Epoch 2/11 - Batch 30/160, Loss: 0.1750 -0.2574\n",
      "1 1\n",
      "training: 0:00:23<0:01:36 Epoch 2/11 - Batch 31/160, Loss: 0.2354 0.4769\n",
      "1 1\n",
      "training: 0:00:23<0:01:35 Epoch 2/11 - Batch 32/160, Loss: 0.1217 -0.3332\n",
      "1 1\n",
      "training: 0:00:24<0:01:34 Epoch 2/11 - Batch 33/160, Loss: 0.0328 -0.3225\n",
      "1 1\n",
      "training: 0:00:25<0:01:33 Epoch 2/11 - Batch 34/160, Loss: -0.0376 -0.3192\n",
      "1 1\n",
      "training: 0:00:26<0:01:32 Epoch 2/11 - Batch 35/160, Loss: 0.0127 0.2136\n",
      "1 1\n",
      "training: 0:00:26<0:01:31 Epoch 2/11 - Batch 36/160, Loss: 0.0838 0.3683\n",
      "1 1\n",
      "training: 0:00:27<0:01:31 Epoch 2/11 - Batch 37/160, Loss: 0.0350 -0.1601\n",
      "1 1\n",
      "training: 0:00:28<0:01:30 Epoch 2/11 - Batch 38/160, Loss: 0.1055 0.3872\n",
      "1 1\n",
      "training: 0:00:28<0:01:29 Epoch 2/11 - Batch 39/160, Loss: 0.0661 -0.0916\n",
      "1 1\n",
      "training: 0:00:29<0:01:28 Epoch 2/11 - Batch 40/160, Loss: 0.0965 0.2182\n",
      "1 1\n",
      "training: 0:00:30<0:01:27 Epoch 2/11 - Batch 41/160, Loss: 0.0075 -0.3487\n",
      "1 1\n",
      "training: 0:00:30<0:01:27 Epoch 2/11 - Batch 42/160, Loss: 0.0847 0.3939\n",
      "1 1\n",
      "training: 0:00:31<0:01:26 Epoch 2/11 - Batch 43/160, Loss: 0.1410 0.3661\n",
      "1 1\n",
      "training: 0:00:32<0:01:25 Epoch 2/11 - Batch 44/160, Loss: 0.2365 0.6183\n",
      "1 1\n",
      "training: 0:00:33<0:01:24 Epoch 2/11 - Batch 45/160, Loss: 0.1532 -0.1800\n",
      "1 1\n",
      "training: 0:00:33<0:01:23 Epoch 2/11 - Batch 46/160, Loss: 0.1320 0.0471\n",
      "1 1\n",
      "training: 0:00:34<0:01:23 Epoch 2/11 - Batch 47/160, Loss: 0.0426 -0.3150\n",
      "1 1\n",
      "training: 0:00:35<0:01:22 Epoch 2/11 - Batch 48/160, Loss: 0.0682 0.1708\n",
      "1 1\n",
      "training: 0:00:35<0:01:21 Epoch 2/11 - Batch 49/160, Loss: 0.1444 0.4491\n",
      "1 1\n",
      "training: 0:00:36<0:01:20 Epoch 2/11 - Batch 50/160, Loss: 0.2433 0.6392\n",
      "1 1\n",
      "training: 0:00:37<0:01:19 Epoch 2/11 - Batch 51/160, Loss: 0.1892 -0.0273\n",
      "1 1\n",
      "training: 0:00:38<0:01:19 Epoch 2/11 - Batch 52/160, Loss: 0.0798 -0.3577\n",
      "1 1\n",
      "training: 0:00:38<0:01:18 Epoch 2/11 - Batch 53/160, Loss: 0.1181 0.2711\n",
      "1 1\n",
      "training: 0:00:39<0:01:17 Epoch 2/11 - Batch 54/160, Loss: 0.1898 0.4766\n",
      "1 1\n",
      "training: 0:00:40<0:01:16 Epoch 2/11 - Batch 55/160, Loss: 0.0771 -0.3738\n",
      "1 1\n",
      "training: 0:00:40<0:01:15 Epoch 2/11 - Batch 56/160, Loss: 0.1181 0.2824\n",
      "1 1\n",
      "training: 0:00:41<0:01:15 Epoch 2/11 - Batch 57/160, Loss: 0.1868 0.4618\n",
      "1 1\n",
      "training: 0:00:42<0:01:14 Epoch 2/11 - Batch 58/160, Loss: 0.1664 0.0844\n",
      "1 1\n",
      "training: 0:00:42<0:01:13 Epoch 2/11 - Batch 59/160, Loss: 0.0600 -0.3656\n",
      "1 1\n",
      "training: 0:00:43<0:01:12 Epoch 2/11 - Batch 60/160, Loss: 0.1641 0.5805\n",
      "1 1\n",
      "training: 0:00:44<0:01:11 Epoch 2/11 - Batch 61/160, Loss: 0.2481 0.5840\n",
      "1 1\n",
      "training: 0:00:45<0:01:11 Epoch 2/11 - Batch 62/160, Loss: 0.2956 0.4859\n",
      "1 1\n",
      "training: 0:00:45<0:01:10 Epoch 2/11 - Batch 63/160, Loss: 0.3531 0.5829\n",
      "1 1\n",
      "training: 0:00:46<0:01:09 Epoch 2/11 - Batch 64/160, Loss: 0.3428 0.3016\n",
      "1 1\n",
      "training: 0:00:47<0:01:08 Epoch 2/11 - Batch 65/160, Loss: 0.1983 -0.3797\n",
      "1 1\n",
      "training: 0:00:47<0:01:07 Epoch 2/11 - Batch 66/160, Loss: 0.2879 0.6462\n",
      "1 1\n",
      "training: 0:00:48<0:01:07 Epoch 2/11 - Batch 67/160, Loss: 0.2918 0.3075\n",
      "1 1\n",
      "training: 0:00:49<0:01:06 Epoch 2/11 - Batch 68/160, Loss: 0.2726 0.1956\n",
      "1 1\n",
      "training: 0:00:49<0:01:05 Epoch 2/11 - Batch 69/160, Loss: 0.2629 0.2243\n",
      "1 1\n",
      "training: 0:00:50<0:01:04 Epoch 2/11 - Batch 70/160, Loss: 0.3167 0.5317\n",
      "1 1\n",
      "training: 0:00:51<0:01:04 Epoch 2/11 - Batch 71/160, Loss: 0.1867 -0.3332\n",
      "1 1\n",
      "training: 0:00:51<0:01:03 Epoch 2/11 - Batch 72/160, Loss: 0.2739 0.6227\n",
      "1 1\n",
      "training: 0:00:52<0:01:02 Epoch 2/11 - Batch 73/160, Loss: 0.1423 -0.3839\n",
      "1 1\n",
      "training: 0:00:53<0:01:01 Epoch 2/11 - Batch 74/160, Loss: 0.2269 0.5651\n",
      "1 1\n",
      "training: 0:00:54<0:01:01 Epoch 2/11 - Batch 75/160, Loss: 0.1050 -0.3825\n",
      "1 1\n",
      "training: 0:00:54<0:01:00 Epoch 2/11 - Batch 76/160, Loss: 0.1068 0.1140\n",
      "1 1\n",
      "training: 0:00:55<0:00:59 Epoch 2/11 - Batch 77/160, Loss: 0.1494 0.3196\n",
      "1 1\n",
      "training: 0:00:56<0:00:58 Epoch 2/11 - Batch 78/160, Loss: 0.1027 -0.0839\n",
      "1 1\n",
      "training: 0:00:56<0:00:58 Epoch 2/11 - Batch 79/160, Loss: 0.0066 -0.3780\n",
      "1 1\n",
      "training: 0:00:57<0:00:57 Epoch 2/11 - Batch 80/160, Loss: 0.1142 0.5447\n",
      "1 1\n",
      "training: 0:00:58<0:00:56 Epoch 2/11 - Batch 81/160, Loss: 0.1237 0.1619\n",
      "1 1\n",
      "training: 0:00:58<0:00:55 Epoch 2/11 - Batch 82/160, Loss: 0.2171 0.5907\n",
      "1 1\n",
      "training: 0:00:59<0:00:55 Epoch 2/11 - Batch 83/160, Loss: 0.2242 0.2527\n",
      "1 1\n",
      "training: 0:01:00<0:00:54 Epoch 2/11 - Batch 84/160, Loss: 0.1163 -0.3153\n",
      "1 1\n",
      "training: 0:01:00<0:00:53 Epoch 2/11 - Batch 85/160, Loss: 0.1397 0.2332\n",
      "1 1\n",
      "training: 0:01:01<0:00:53 Epoch 2/11 - Batch 86/160, Loss: 0.2028 0.4551\n",
      "1 1\n",
      "training: 0:01:02<0:00:52 Epoch 2/11 - Batch 87/160, Loss: 0.1015 -0.3035\n",
      "1 1\n",
      "training: 0:01:03<0:00:51 Epoch 2/11 - Batch 88/160, Loss: 0.1409 0.2982\n",
      "1 1\n",
      "training: 0:01:03<0:00:50 Epoch 2/11 - Batch 89/160, Loss: 0.1110 -0.0085\n",
      "1 1\n",
      "training: 0:01:04<0:00:50 Epoch 2/11 - Batch 90/160, Loss: 0.1266 0.1889\n",
      "1 1\n",
      "training: 0:01:05<0:00:49 Epoch 2/11 - Batch 91/160, Loss: 0.1512 0.2496\n",
      "1 1\n",
      "training: 0:01:05<0:00:48 Epoch 2/11 - Batch 92/160, Loss: 0.0445 -0.3823\n",
      "1 1\n",
      "training: 0:01:06<0:00:47 Epoch 2/11 - Batch 93/160, Loss: 0.1581 0.6123\n",
      "1 1\n",
      "training: 0:01:07<0:00:47 Epoch 2/11 - Batch 94/160, Loss: 0.0540 -0.3624\n",
      "1 1\n",
      "training: 0:01:07<0:00:46 Epoch 2/11 - Batch 95/160, Loss: 0.0388 -0.0217\n",
      "1 1\n",
      "training: 0:01:08<0:00:45 Epoch 2/11 - Batch 96/160, Loss: 0.1292 0.4905\n",
      "1 1\n",
      "training: 0:01:09<0:00:45 Epoch 2/11 - Batch 97/160, Loss: 0.0988 -0.0225\n",
      "1 1\n",
      "training: 0:01:10<0:00:44 Epoch 2/11 - Batch 98/160, Loss: 0.0819 0.0142\n",
      "1 1\n",
      "training: 0:01:10<0:00:43 Epoch 2/11 - Batch 99/160, Loss: 0.1189 0.2667\n",
      "1 1\n",
      "training: 0:01:11<0:00:42 Epoch 2/11 - Batch 100/160, Loss: 0.0485 -0.2330\n",
      "1 1\n",
      "training: 0:01:12<0:00:42 Epoch 2/11 - Batch 101/160, Loss: -0.0380 -0.3839\n",
      "saving searchnet-bcemodel-1-100.pt\n",
      "\n",
      "1 1\n",
      "training: 0:01:14<0:00:42 Epoch 2/11 - Batch 102/160, Loss: -0.1120 -0.4080\n",
      "1 1\n",
      "training: 0:01:14<0:00:41 Epoch 2/11 - Batch 103/160, Loss: 0.0009 0.4526\n",
      "1 1\n",
      "training: 0:01:15<0:00:40 Epoch 2/11 - Batch 104/160, Loss: -0.0770 -0.3885\n",
      "1 1\n",
      "training: 0:01:16<0:00:39 Epoch 2/11 - Batch 105/160, Loss: 0.0434 0.5247\n",
      "1 1\n",
      "training: 0:01:17<0:00:39 Epoch 2/11 - Batch 106/160, Loss: 0.0936 0.2946\n",
      "1 1\n",
      "training: 0:01:17<0:00:38 Epoch 2/11 - Batch 107/160, Loss: -0.0021 -0.3848\n",
      "1 1\n",
      "training: 0:01:18<0:00:37 Epoch 2/11 - Batch 108/160, Loss: 0.0340 0.1784\n",
      "1 1\n",
      "training: 0:01:19<0:00:37 Epoch 2/11 - Batch 109/160, Loss: 0.0658 0.1928\n",
      "1 1\n",
      "training: 0:01:19<0:00:36 Epoch 2/11 - Batch 110/160, Loss: 0.0035 -0.2459\n",
      "1 1\n",
      "training: 0:01:20<0:00:35 Epoch 2/11 - Batch 111/160, Loss: 0.1075 0.5237\n",
      "1 1\n",
      "training: 0:01:21<0:00:34 Epoch 2/11 - Batch 112/160, Loss: 0.1890 0.5148\n",
      "1 1\n",
      "training: 0:01:21<0:00:34 Epoch 2/11 - Batch 113/160, Loss: 0.1867 0.1776\n",
      "1 1\n",
      "training: 0:01:22<0:00:33 Epoch 2/11 - Batch 114/160, Loss: 0.1108 -0.1930\n",
      "1 1\n",
      "training: 0:01:23<0:00:32 Epoch 2/11 - Batch 115/160, Loss: 0.0556 -0.1650\n",
      "1 1\n",
      "training: 0:01:24<0:00:31 Epoch 2/11 - Batch 116/160, Loss: 0.0812 0.1836\n",
      "1 1\n",
      "training: 0:01:24<0:00:31 Epoch 2/11 - Batch 117/160, Loss: 0.1831 0.5907\n",
      "1 1\n",
      "training: 0:01:25<0:00:30 Epoch 2/11 - Batch 118/160, Loss: 0.2649 0.5922\n",
      "1 1\n",
      "training: 0:01:26<0:00:29 Epoch 2/11 - Batch 119/160, Loss: 0.3115 0.4976\n",
      "1 1\n",
      "training: 0:01:26<0:00:28 Epoch 2/11 - Batch 120/160, Loss: 0.3584 0.5464\n",
      "1 1\n",
      "training: 0:01:27<0:00:28 Epoch 2/11 - Batch 121/160, Loss: 0.4046 0.5895\n",
      "1 1\n",
      "training: 0:01:28<0:00:27 Epoch 2/11 - Batch 122/160, Loss: 0.2471 -0.3831\n",
      "1 1\n",
      "training: 0:01:29<0:00:26 Epoch 2/11 - Batch 123/160, Loss: 0.1184 -0.3965\n",
      "1 1\n",
      "training: 0:01:29<0:00:26 Epoch 2/11 - Batch 124/160, Loss: 0.0135 -0.4061\n",
      "1 1\n",
      "training: 0:01:30<0:00:25 Epoch 2/11 - Batch 125/160, Loss: 0.1321 0.6067\n",
      "1 1\n",
      "training: 0:01:31<0:00:24 Epoch 2/11 - Batch 126/160, Loss: 0.0279 -0.3892\n",
      "1 1\n",
      "training: 0:01:31<0:00:23 Epoch 2/11 - Batch 127/160, Loss: 0.1415 0.5961\n",
      "1 1\n",
      "training: 0:01:32<0:00:23 Epoch 2/11 - Batch 128/160, Loss: 0.2385 0.6266\n",
      "saving searchnet-bcemodel-1-128.pt\n",
      "\n",
      "\n",
      "Validation:\n",
      "\n",
      "1 1\n",
      "validation: 0:01:34<0:00:22 Epoch 2/11 - Batch 129/160, Loss: -0.3883 -0.3883\n",
      "1 1\n",
      "validation: 0:01:34<0:00:21 Epoch 2/11 - Batch 130/160, Loss: -0.0359 0.3165\n",
      "1 1\n",
      "validation: 0:01:34<0:00:20 Epoch 2/11 - Batch 131/160, Loss: 0.0853 0.3277\n",
      "1 1\n",
      "validation: 0:01:34<0:00:20 Epoch 2/11 - Batch 132/160, Loss: 0.0739 0.0395\n",
      "1 1\n",
      "validation: 0:01:34<0:00:19 Epoch 2/11 - Batch 133/160, Loss: -0.0294 -0.4424\n",
      "1 1\n",
      "validation: 0:01:35<0:00:18 Epoch 2/11 - Batch 134/160, Loss: 0.0662 0.5439\n",
      "1 1\n",
      "validation: 0:01:35<0:00:17 Epoch 2/11 - Batch 135/160, Loss: 0.1402 0.5843\n",
      "1 1\n",
      "validation: 0:01:35<0:00:16 Epoch 2/11 - Batch 136/160, Loss: 0.0661 -0.4524\n",
      "1 1\n",
      "validation: 0:01:35<0:00:15 Epoch 2/11 - Batch 137/160, Loss: 0.1011 0.3811\n",
      "1 1\n",
      "validation: 0:01:35<0:00:15 Epoch 2/11 - Batch 138/160, Loss: 0.1175 0.2649\n",
      "1 1\n",
      "validation: 0:01:35<0:00:14 Epoch 2/11 - Batch 139/160, Loss: 0.1614 0.6004\n",
      "1 1\n",
      "validation: 0:01:35<0:00:13 Epoch 2/11 - Batch 140/160, Loss: 0.1113 -0.4399\n",
      "1 1\n",
      "validation: 0:01:35<0:00:12 Epoch 2/11 - Batch 141/160, Loss: 0.1373 0.4491\n",
      "1 1\n",
      "validation: 0:01:35<0:00:12 Epoch 2/11 - Batch 142/160, Loss: 0.1710 0.6099\n",
      "1 1\n",
      "validation: 0:01:35<0:00:11 Epoch 2/11 - Batch 143/160, Loss: 0.1993 0.5952\n",
      "1 1\n",
      "validation: 0:01:35<0:00:10 Epoch 2/11 - Batch 144/160, Loss: 0.2169 0.4804\n",
      "1 1\n",
      "validation: 0:01:35<0:00:09 Epoch 2/11 - Batch 145/160, Loss: 0.2387 0.5877\n",
      "1 1\n",
      "validation: 0:01:35<0:00:09 Epoch 2/11 - Batch 146/160, Loss: 0.2474 0.3949\n",
      "1 1\n",
      "validation: 0:01:35<0:00:08 Epoch 2/11 - Batch 147/160, Loss: 0.2584 0.4563\n",
      "1 1\n",
      "validation: 0:01:35<0:00:07 Epoch 2/11 - Batch 148/160, Loss: 0.2708 0.5069\n",
      "1 1\n",
      "validation: 0:01:36<0:00:07 Epoch 2/11 - Batch 149/160, Loss: 0.2775 0.4115\n",
      "1 1\n",
      "validation: 0:01:36<0:00:06 Epoch 2/11 - Batch 150/160, Loss: 0.2777 0.2816\n",
      "1 1\n",
      "validation: 0:01:36<0:00:05 Epoch 2/11 - Batch 151/160, Loss: 0.2473 -0.4204\n",
      "1 1\n",
      "validation: 0:01:36<0:00:05 Epoch 2/11 - Batch 152/160, Loss: 0.2519 0.3566\n",
      "1 1\n",
      "validation: 0:01:36<0:00:04 Epoch 2/11 - Batch 153/160, Loss: 0.2408 -0.0265\n",
      "1 1\n",
      "validation: 0:01:36<0:00:03 Epoch 2/11 - Batch 154/160, Loss: 0.2439 0.3218\n",
      "1 1\n",
      "validation: 0:01:36<0:00:03 Epoch 2/11 - Batch 155/160, Loss: 0.2540 0.5166\n",
      "1 1\n",
      "validation: 0:01:36<0:00:02 Epoch 2/11 - Batch 156/160, Loss: 0.2645 0.5482\n",
      "1 1\n",
      "validation: 0:01:36<0:00:01 Epoch 2/11 - Batch 157/160, Loss: 0.2429 -0.3614\n",
      "1 1\n",
      "validation: 0:01:36<0:00:01 Epoch 2/11 - Batch 158/160, Loss: 0.2203 -0.4348\n",
      "1 1\n",
      "validation: 0:01:36<0:00:00 Epoch 2/11 - Batch 159/160, Loss: 0.1995 -0.4258\n",
      "1 1\n",
      "validation: 0:01:37<0:00:00 Epoch 2/11 - Batch 160/160, Loss: 0.1808 -0.3987\n",
      "\n",
      "Training\n",
      "\n",
      "1 1\n",
      "training: 0:00:00<0:01:51 Epoch 3/12 - Batch 1/160, Loss: 0.5343 0.5343\n",
      "saving searchnet-bcemodel-2-0.pt\n",
      "\n",
      "1 1\n",
      "training: 0:00:04<0:05:55 Epoch 3/12 - Batch 2/160, Loss: 0.0028 -0.3515\n",
      "1 1\n",
      "training: 0:00:05<0:04:32 Epoch 3/12 - Batch 3/160, Loss: 0.1078 0.2653\n",
      "1 1\n",
      "training: 0:00:05<0:03:50 Epoch 3/12 - Batch 4/160, Loss: 0.2080 0.4417\n",
      "1 1\n",
      "training: 0:00:06<0:03:24 Epoch 3/12 - Batch 5/160, Loss: 0.2803 0.4971\n",
      "1 1\n",
      "training: 0:00:07<0:03:07 Epoch 3/12 - Batch 6/160, Loss: 0.2698 0.2339\n",
      "1 1\n",
      "training: 0:00:08<0:02:55 Epoch 3/12 - Batch 7/160, Loss: 0.3162 0.4881\n",
      "1 1\n",
      "training: 0:00:08<0:02:45 Epoch 3/12 - Batch 8/160, Loss: 0.1671 -0.4068\n",
      "1 1\n",
      "training: 0:00:09<0:02:37 Epoch 3/12 - Batch 9/160, Loss: 0.2530 0.5900\n",
      "1 1\n",
      "training: 0:00:10<0:02:31 Epoch 3/12 - Batch 10/160, Loss: 0.1328 -0.3436\n",
      "1 1\n",
      "training: 0:00:10<0:02:26 Epoch 3/12 - Batch 11/160, Loss: 0.2063 0.4991\n",
      "1 1\n",
      "training: 0:00:11<0:02:21 Epoch 3/12 - Batch 12/160, Loss: 0.2726 0.5369\n",
      "1 1\n",
      "training: 0:00:12<0:02:17 Epoch 3/12 - Batch 13/160, Loss: 0.2509 0.1644\n",
      "1 1\n",
      "training: 0:00:12<0:02:14 Epoch 3/12 - Batch 14/160, Loss: 0.2830 0.4114\n",
      "1 1\n",
      "training: 0:00:13<0:02:11 Epoch 3/12 - Batch 15/160, Loss: 0.3400 0.5678\n",
      "1 1\n",
      "training: 0:00:14<0:02:08 Epoch 3/12 - Batch 16/160, Loss: 0.3230 0.2550\n",
      "1 1\n",
      "training: 0:00:14<0:02:05 Epoch 3/12 - Batch 17/160, Loss: 0.3646 0.5308\n",
      "1 1\n",
      "training: 0:00:15<0:02:03 Epoch 3/12 - Batch 18/160, Loss: 0.3567 0.3250\n",
      "1 1\n",
      "training: 0:00:16<0:02:01 Epoch 3/12 - Batch 19/160, Loss: 0.2661 -0.0963\n",
      "1 1\n",
      "training: 0:00:17<0:01:59 Epoch 3/12 - Batch 20/160, Loss: 0.3150 0.5107\n",
      "1 1\n",
      "training: 0:00:17<0:01:57 Epoch 3/12 - Batch 21/160, Loss: 0.1740 -0.3900\n",
      "1 1\n",
      "training: 0:00:18<0:01:55 Epoch 3/12 - Batch 22/160, Loss: 0.2446 0.5269\n",
      "1 1\n",
      "training: 0:00:19<0:01:54 Epoch 3/12 - Batch 23/160, Loss: 0.2084 0.0635\n",
      "1 1\n",
      "training: 0:00:19<0:01:52 Epoch 3/12 - Batch 24/160, Loss: 0.2362 0.3478\n",
      "1 1\n",
      "training: 0:00:20<0:01:51 Epoch 3/12 - Batch 25/160, Loss: 0.1855 -0.0177\n",
      "1 1\n",
      "training: 0:00:21<0:01:49 Epoch 3/12 - Batch 26/160, Loss: 0.1967 0.2415\n",
      "1 1\n",
      "training: 0:00:21<0:01:48 Epoch 3/12 - Batch 27/160, Loss: 0.1408 -0.0826\n",
      "1 1\n",
      "training: 0:00:22<0:01:46 Epoch 3/12 - Batch 28/160, Loss: 0.1800 0.3367\n",
      "1 1\n",
      "training: 0:00:23<0:01:45 Epoch 3/12 - Batch 29/160, Loss: 0.2426 0.4929\n",
      "1 1\n",
      "training: 0:00:24<0:01:44 Epoch 3/12 - Batch 30/160, Loss: 0.1301 -0.3196\n",
      "1 1\n",
      "training: 0:00:24<0:01:42 Epoch 3/12 - Batch 31/160, Loss: 0.1941 0.4501\n",
      "1 1\n",
      "training: 0:00:25<0:01:41 Epoch 3/12 - Batch 32/160, Loss: 0.0744 -0.4043\n",
      "1 1\n",
      "training: 0:00:26<0:01:40 Epoch 3/12 - Batch 33/160, Loss: -0.0221 -0.4084\n",
      "1 1\n",
      "training: 0:00:26<0:01:39 Epoch 3/12 - Batch 34/160, Loss: -0.0991 -0.4072\n",
      "1 1\n",
      "training: 0:00:27<0:01:38 Epoch 3/12 - Batch 35/160, Loss: -0.0439 0.1773\n",
      "1 1\n",
      "training: 0:00:28<0:01:37 Epoch 3/12 - Batch 36/160, Loss: 0.0332 0.3413\n",
      "1 1\n",
      "training: 0:00:28<0:01:36 Epoch 3/12 - Batch 37/160, Loss: -0.0206 -0.2356\n",
      "1 1\n",
      "training: 0:00:29<0:01:35 Epoch 3/12 - Batch 38/160, Loss: 0.0549 0.3569\n",
      "1 1\n",
      "training: 0:00:30<0:01:34 Epoch 3/12 - Batch 39/160, Loss: 0.0158 -0.1409\n",
      "1 1\n",
      "training: 0:00:31<0:01:33 Epoch 3/12 - Batch 40/160, Loss: 0.0495 0.1847\n",
      "1 1\n",
      "training: 0:00:31<0:01:32 Epoch 3/12 - Batch 41/160, Loss: -0.0442 -0.4194\n",
      "1 1\n",
      "training: 0:00:32<0:01:31 Epoch 3/12 - Batch 42/160, Loss: 0.0379 0.3665\n",
      "1 1\n",
      "training: 0:00:33<0:01:30 Epoch 3/12 - Batch 43/160, Loss: 0.0988 0.3424\n",
      "1 1\n",
      "training: 0:00:33<0:01:29 Epoch 3/12 - Batch 44/160, Loss: 0.2007 0.6083\n",
      "1 1\n",
      "training: 0:00:34<0:01:28 Epoch 3/12 - Batch 45/160, Loss: 0.1137 -0.2343\n",
      "1 1\n",
      "training: 0:00:35<0:01:27 Epoch 3/12 - Batch 46/160, Loss: 0.0922 0.0061\n",
      "1 1\n",
      "training: 0:00:36<0:01:26 Epoch 3/12 - Batch 47/160, Loss: -0.0047 -0.3919\n",
      "1 1\n",
      "training: 0:00:36<0:01:25 Epoch 3/12 - Batch 48/160, Loss: 0.0235 0.1361\n",
      "1 1\n",
      "training: 0:00:37<0:01:24 Epoch 3/12 - Batch 49/160, Loss: 0.1054 0.4330\n",
      "1 1\n",
      "training: 0:00:38<0:01:23 Epoch 3/12 - Batch 50/160, Loss: 0.2097 0.6270\n",
      "1 1\n",
      "training: 0:00:38<0:01:22 Epoch 3/12 - Batch 51/160, Loss: 0.1535 -0.0713\n",
      "1 1\n",
      "training: 0:00:39<0:01:22 Epoch 3/12 - Batch 52/160, Loss: 0.0390 -0.4190\n",
      "1 1\n",
      "training: 0:00:40<0:01:21 Epoch 3/12 - Batch 53/160, Loss: 0.0800 0.2439\n",
      "1 1\n",
      "training: 0:00:40<0:01:20 Epoch 3/12 - Batch 54/160, Loss: 0.1551 0.4556\n",
      "1 1\n",
      "training: 0:00:41<0:01:19 Epoch 3/12 - Batch 55/160, Loss: 0.0376 -0.4325\n",
      "1 1\n",
      "training: 0:00:42<0:01:18 Epoch 3/12 - Batch 56/160, Loss: 0.0809 0.2541\n",
      "1 1\n",
      "training: 0:00:43<0:01:17 Epoch 3/12 - Batch 57/160, Loss: 0.1535 0.4440\n",
      "1 1\n",
      "training: 0:00:43<0:01:16 Epoch 3/12 - Batch 58/160, Loss: 0.1315 0.0436\n",
      "1 1\n",
      "training: 0:00:44<0:01:16 Epoch 3/12 - Batch 59/160, Loss: 0.0204 -0.4240\n",
      "1 1\n",
      "training: 0:00:45<0:01:15 Epoch 3/12 - Batch 60/160, Loss: 0.1293 0.5647\n",
      "1 1\n",
      "training: 0:00:45<0:01:14 Epoch 3/12 - Batch 61/160, Loss: 0.2170 0.5678\n",
      "1 1\n",
      "training: 0:00:46<0:01:13 Epoch 3/12 - Batch 62/160, Loss: 0.2676 0.4701\n",
      "1 1\n",
      "training: 0:00:47<0:01:12 Epoch 3/12 - Batch 63/160, Loss: 0.3281 0.5700\n",
      "1 1\n",
      "training: 0:00:47<0:01:11 Epoch 3/12 - Batch 64/160, Loss: 0.3186 0.2805\n",
      "1 1\n",
      "training: 0:00:48<0:01:11 Epoch 3/12 - Batch 65/160, Loss: 0.1684 -0.4325\n",
      "1 1\n",
      "training: 0:00:49<0:01:10 Epoch 3/12 - Batch 66/160, Loss: 0.2620 0.6366\n",
      "1 1\n",
      "training: 0:00:49<0:01:09 Epoch 3/12 - Batch 67/160, Loss: 0.2665 0.2845\n",
      "1 1\n",
      "training: 0:00:50<0:01:08 Epoch 3/12 - Batch 68/160, Loss: 0.2476 0.1721\n",
      "1 1\n",
      "training: 0:00:51<0:01:07 Epoch 3/12 - Batch 69/160, Loss: 0.2372 0.1953\n",
      "1 1\n",
      "training: 0:00:52<0:01:06 Epoch 3/12 - Batch 70/160, Loss: 0.2936 0.5195\n",
      "1 1\n",
      "training: 0:00:52<0:01:06 Epoch 3/12 - Batch 71/160, Loss: 0.1581 -0.3838\n",
      "1 1\n",
      "training: 0:00:53<0:01:05 Epoch 3/12 - Batch 72/160, Loss: 0.2491 0.6130\n",
      "1 1\n",
      "training: 0:00:54<0:01:04 Epoch 3/12 - Batch 73/160, Loss: 0.1130 -0.4316\n",
      "1 1\n",
      "training: 0:00:54<0:01:03 Epoch 3/12 - Batch 74/160, Loss: 0.2007 0.5519\n",
      "1 1\n",
      "training: 0:00:55<0:01:03 Epoch 3/12 - Batch 75/160, Loss: 0.0751 -0.4274\n",
      "1 1\n",
      "training: 0:00:56<0:01:02 Epoch 3/12 - Batch 76/160, Loss: 0.0742 0.0705\n",
      "1 1\n",
      "training: 0:00:57<0:01:01 Epoch 3/12 - Batch 77/160, Loss: 0.1186 0.2961\n",
      "1 1\n",
      "training: 0:00:57<0:01:00 Epoch 3/12 - Batch 78/160, Loss: 0.0706 -0.1214\n",
      "1 1\n",
      "training: 0:00:58<0:00:59 Epoch 3/12 - Batch 79/160, Loss: -0.0296 -0.4303\n",
      "1 1\n",
      "training: 0:00:59<0:00:59 Epoch 3/12 - Batch 80/160, Loss: 0.0822 0.5296\n",
      "1 1\n",
      "training: 0:00:59<0:00:58 Epoch 3/12 - Batch 81/160, Loss: 0.0930 0.1363\n",
      "1 1\n",
      "training: 0:01:00<0:00:57 Epoch 3/12 - Batch 82/160, Loss: 0.1900 0.5776\n",
      "1 1\n",
      "training: 0:01:01<0:00:56 Epoch 3/12 - Batch 83/160, Loss: 0.1979 0.2295\n",
      "1 1\n",
      "training: 0:01:02<0:00:56 Epoch 3/12 - Batch 84/160, Loss: 0.0870 -0.3567\n",
      "1 1\n",
      "training: 0:01:02<0:00:55 Epoch 3/12 - Batch 85/160, Loss: 0.1115 0.2096\n",
      "1 1\n",
      "training: 0:01:03<0:00:54 Epoch 3/12 - Batch 86/160, Loss: 0.1768 0.4380\n",
      "1 1\n",
      "training: 0:01:04<0:00:53 Epoch 3/12 - Batch 87/160, Loss: 0.0739 -0.3379\n",
      "1 1\n",
      "training: 0:01:04<0:00:53 Epoch 3/12 - Batch 88/160, Loss: 0.1147 0.2782\n",
      "1 1\n",
      "training: 0:01:05<0:00:52 Epoch 3/12 - Batch 89/160, Loss: 0.0846 -0.0360\n",
      "1 1\n",
      "training: 0:01:06<0:00:51 Epoch 3/12 - Batch 90/160, Loss: 0.1005 0.1642\n",
      "1 1\n",
      "training: 0:01:06<0:00:50 Epoch 3/12 - Batch 91/160, Loss: 0.1251 0.2237\n",
      "1 1\n",
      "training: 0:01:07<0:00:49 Epoch 3/12 - Batch 92/160, Loss: 0.0138 -0.4317\n",
      "1 1\n",
      "training: 0:01:08<0:00:49 Epoch 3/12 - Batch 93/160, Loss: 0.1318 0.6039\n",
      "1 1\n",
      "training: 0:01:09<0:00:48 Epoch 3/12 - Batch 94/160, Loss: 0.0215 -0.4197\n",
      "1 1\n",
      "training: 0:01:09<0:00:47 Epoch 3/12 - Batch 95/160, Loss: 0.0067 -0.0526\n",
      "1 1\n",
      "training: 0:01:10<0:00:46 Epoch 3/12 - Batch 96/160, Loss: 0.1007 0.4770\n",
      "1 1\n",
      "training: 0:01:11<0:00:46 Epoch 3/12 - Batch 97/160, Loss: 0.0697 -0.0544\n",
      "1 1\n",
      "training: 0:01:11<0:00:45 Epoch 3/12 - Batch 98/160, Loss: 0.0531 -0.0135\n",
      "1 1\n",
      "training: 0:01:12<0:00:44 Epoch 3/12 - Batch 99/160, Loss: 0.0920 0.2476\n",
      "1 1\n",
      "training: 0:01:13<0:00:43 Epoch 3/12 - Batch 100/160, Loss: 0.0195 -0.2704\n",
      "1 1\n",
      "training: 0:01:13<0:00:43 Epoch 3/12 - Batch 101/160, Loss: -0.0692 -0.4241\n",
      "saving searchnet-bcemodel-2-100.pt\n",
      "\n",
      "1 1\n",
      "training: 0:01:17<0:00:43 Epoch 3/12 - Batch 102/160, Loss: -0.1429 -0.4376\n",
      "1 1\n",
      "training: 0:01:17<0:00:43 Epoch 3/12 - Batch 103/160, Loss: -0.0269 0.4370\n",
      "1 1\n",
      "training: 0:01:18<0:00:42 Epoch 3/12 - Batch 104/160, Loss: -0.1070 -0.4275\n",
      "1 1\n",
      "training: 0:01:19<0:00:41 Epoch 3/12 - Batch 105/160, Loss: 0.0168 0.5120\n",
      "1 1\n",
      "training: 0:01:20<0:00:40 Epoch 3/12 - Batch 106/160, Loss: 0.0676 0.2709\n",
      "1 1\n",
      "training: 0:01:20<0:00:40 Epoch 3/12 - Batch 107/160, Loss: -0.0323 -0.4317\n",
      "1 1\n",
      "training: 0:01:21<0:00:39 Epoch 3/12 - Batch 108/160, Loss: 0.0067 0.1624\n",
      "1 1\n",
      "training: 0:01:22<0:00:38 Epoch 3/12 - Batch 109/160, Loss: 0.0410 0.1785\n",
      "1 1\n",
      "training: 0:01:22<0:00:37 Epoch 3/12 - Batch 110/160, Loss: -0.0223 -0.2756\n",
      "1 1\n",
      "training: 0:01:23<0:00:36 Epoch 3/12 - Batch 111/160, Loss: 0.0848 0.5133\n",
      "1 1\n",
      "training: 0:01:24<0:00:36 Epoch 3/12 - Batch 112/160, Loss: 0.1685 0.5034\n",
      "1 1\n",
      "training: 0:01:24<0:00:35 Epoch 3/12 - Batch 113/160, Loss: 0.1662 0.1569\n",
      "1 1\n",
      "training: 0:01:25<0:00:34 Epoch 3/12 - Batch 114/160, Loss: 0.0888 -0.2210\n",
      "1 1\n",
      "training: 0:01:26<0:00:33 Epoch 3/12 - Batch 115/160, Loss: 0.0322 -0.1939\n",
      "1 1\n",
      "training: 0:01:27<0:00:33 Epoch 3/12 - Batch 116/160, Loss: 0.0597 0.1694\n",
      "1 1\n",
      "training: 0:01:27<0:00:32 Epoch 3/12 - Batch 117/160, Loss: 0.1643 0.5828\n",
      "1 1\n",
      "training: 0:01:28<0:00:31 Epoch 3/12 - Batch 118/160, Loss: 0.2485 0.5853\n",
      "1 1\n",
      "training: 0:01:29<0:00:30 Epoch 3/12 - Batch 119/160, Loss: 0.2963 0.4878\n",
      "1 1\n",
      "training: 0:01:29<0:00:29 Epoch 3/12 - Batch 120/160, Loss: 0.3444 0.5364\n",
      "1 1\n",
      "training: 0:01:30<0:00:29 Epoch 3/12 - Batch 121/160, Loss: 0.3916 0.5806\n",
      "1 1\n",
      "training: 0:01:31<0:00:28 Epoch 3/12 - Batch 122/160, Loss: 0.2287 -0.4228\n",
      "1 1\n",
      "training: 0:01:31<0:00:27 Epoch 3/12 - Batch 123/160, Loss: 0.0958 -0.4359\n",
      "1 1\n",
      "training: 0:01:32<0:00:26 Epoch 3/12 - Batch 124/160, Loss: -0.0106 -0.4364\n",
      "1 1\n",
      "training: 0:01:33<0:00:26 Epoch 3/12 - Batch 125/160, Loss: 0.1108 0.5967\n",
      "1 1\n",
      "training: 0:01:33<0:00:25 Epoch 3/12 - Batch 126/160, Loss: 0.0051 -0.4178\n",
      "1 1\n",
      "training: 0:01:34<0:00:24 Epoch 3/12 - Batch 127/160, Loss: 0.1213 0.5861\n",
      "1 1\n",
      "training: 0:01:35<0:00:23 Epoch 3/12 - Batch 128/160, Loss: 0.2213 0.6215\n",
      "saving searchnet-bcemodel-2-128.pt\n",
      "\n",
      "\n",
      "Validation:\n",
      "\n",
      "1 1\n",
      "validation: 0:01:37<0:00:23 Epoch 3/12 - Batch 129/160, Loss: -0.4215 -0.4215\n",
      "1 1\n",
      "validation: 0:01:37<0:00:22 Epoch 3/12 - Batch 130/160, Loss: -0.0602 0.3010\n",
      "1 1\n",
      "validation: 0:01:37<0:00:21 Epoch 3/12 - Batch 131/160, Loss: 0.0646 0.3142\n",
      "1 1\n",
      "validation: 0:01:37<0:00:20 Epoch 3/12 - Batch 132/160, Loss: 0.0548 0.0254\n",
      "1 1\n",
      "validation: 0:01:37<0:00:19 Epoch 3/12 - Batch 133/160, Loss: -0.0492 -0.4652\n",
      "1 1\n",
      "validation: 0:01:37<0:00:19 Epoch 3/12 - Batch 134/160, Loss: 0.0482 0.5354\n",
      "1 1\n",
      "validation: 0:01:37<0:00:18 Epoch 3/12 - Batch 135/160, Loss: 0.1235 0.5749\n",
      "1 1\n",
      "validation: 0:01:38<0:00:17 Epoch 3/12 - Batch 136/160, Loss: 0.0490 -0.4721\n",
      "1 1\n",
      "validation: 0:01:38<0:00:16 Epoch 3/12 - Batch 137/160, Loss: 0.0847 0.3696\n",
      "1 1\n",
      "validation: 0:01:38<0:00:15 Epoch 3/12 - Batch 138/160, Loss: 0.1012 0.2500\n",
      "1 1\n",
      "validation: 0:01:38<0:00:14 Epoch 3/12 - Batch 139/160, Loss: 0.1459 0.5931\n",
      "1 1\n",
      "validation: 0:01:38<0:00:14 Epoch 3/12 - Batch 140/160, Loss: 0.0952 -0.4625\n",
      "1 1\n",
      "validation: 0:01:38<0:00:13 Epoch 3/12 - Batch 141/160, Loss: 0.1217 0.4401\n",
      "1 1\n",
      "validation: 0:01:38<0:00:12 Epoch 3/12 - Batch 142/160, Loss: 0.1561 0.6024\n",
      "1 1\n",
      "validation: 0:01:38<0:00:11 Epoch 3/12 - Batch 143/160, Loss: 0.1849 0.5877\n",
      "1 1\n",
      "validation: 0:01:38<0:00:10 Epoch 3/12 - Batch 144/160, Loss: 0.2027 0.4706\n",
      "1 1\n",
      "validation: 0:01:38<0:00:10 Epoch 3/12 - Batch 145/160, Loss: 0.2248 0.5788\n",
      "1 1\n",
      "validation: 0:01:38<0:00:09 Epoch 3/12 - Batch 146/160, Loss: 0.2337 0.3839\n",
      "1 1\n",
      "validation: 0:01:38<0:00:08 Epoch 3/12 - Batch 147/160, Loss: 0.2447 0.4438\n",
      "1 1\n",
      "validation: 0:01:38<0:00:08 Epoch 3/12 - Batch 148/160, Loss: 0.2574 0.4984\n",
      "1 1\n",
      "validation: 0:01:38<0:00:07 Epoch 3/12 - Batch 149/160, Loss: 0.2642 0.4010\n",
      "1 1\n",
      "validation: 0:01:39<0:00:06 Epoch 3/12 - Batch 150/160, Loss: 0.2646 0.2711\n",
      "1 1\n",
      "validation: 0:01:39<0:00:05 Epoch 3/12 - Batch 151/160, Loss: 0.2337 -0.4445\n",
      "1 1\n",
      "validation: 0:01:39<0:00:05 Epoch 3/12 - Batch 152/160, Loss: 0.2383 0.3443\n",
      "1 1\n",
      "validation: 0:01:39<0:00:04 Epoch 3/12 - Batch 153/160, Loss: 0.2269 -0.0488\n",
      "1 1\n",
      "validation: 0:01:39<0:00:03 Epoch 3/12 - Batch 154/160, Loss: 0.2298 0.3035\n",
      "1 1\n",
      "validation: 0:01:39<0:00:03 Epoch 3/12 - Batch 155/160, Loss: 0.2400 0.5065\n",
      "1 1\n",
      "validation: 0:01:39<0:00:02 Epoch 3/12 - Batch 156/160, Loss: 0.2508 0.5408\n",
      "1 1\n",
      "validation: 0:01:39<0:00:01 Epoch 3/12 - Batch 157/160, Loss: 0.2288 -0.3869\n",
      "1 1\n",
      "validation: 0:01:39<0:00:01 Epoch 3/12 - Batch 158/160, Loss: 0.2058 -0.4606\n",
      "1 1\n",
      "validation: 0:01:39<0:00:00 Epoch 3/12 - Batch 159/160, Loss: 0.1845 -0.4543\n",
      "1 1\n",
      "validation: 0:01:39<0:00:00 Epoch 3/12 - Batch 160/160, Loss: 0.1654 -0.4282\n",
      "\n",
      "Training\n",
      "\n",
      "1 1\n",
      "training: 0:00:00<0:01:51 Epoch 4/13 - Batch 1/160, Loss: 0.5294 0.5294\n",
      "saving searchnet-bcemodel-3-0.pt\n",
      "\n",
      "1 1\n",
      "training: 0:00:07<0:10:04 Epoch 4/13 - Batch 2/160, Loss: -0.0302 -0.4032\n",
      "1 1\n",
      "training: 0:00:08<0:07:18 Epoch 4/13 - Batch 3/160, Loss: 0.0803 0.2459\n",
      "1 1\n",
      "training: 0:00:09<0:05:54 Epoch 4/13 - Batch 4/160, Loss: 0.1855 0.4311\n",
      "1 1\n",
      "training: 0:00:09<0:05:03 Epoch 4/13 - Batch 5/160, Loss: 0.2609 0.4869\n",
      "1 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m set_last_batch(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mhandle_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[43], line 150\u001b[0m, in \u001b[0;36mhandle_epoch\u001b[0;34m(batch_size, num_epochs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (i, batch) \u001b[38;5;129;01min\u001b[39;00m get_epoch_part(i0, train_batchs, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    149\u001b[0m     performance \u001b[38;5;241m=\u001b[39m (min_alpha, alpha, running_loss, start_time)\n\u001b[0;32m--> 150\u001b[0m     running_loss, alpha \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_train_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperformance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m i0 \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    154\u001b[0m save(epoch, i0)\n",
      "Cell \u001b[0;32mIn[43], line 107\u001b[0m, in \u001b[0;36mhandle_train_batch\u001b[0;34m(i, epoch, batch, performance, sizes)\u001b[0m\n\u001b[1;32m    104\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    106\u001b[0m current_alpha \u001b[38;5;241m=\u001b[39m min_alpha \u001b[38;5;241m+\u001b[39m alpha\n\u001b[0;32m--> 107\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m current_alpha) \u001b[38;5;241m*\u001b[39m running_loss \u001b[38;5;241m+\u001b[39m current_alpha \u001b[38;5;241m*\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m alpha \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.5\u001b[39m\n\u001b[1;32m    110\u001b[0m log(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m, start_time, sizes, epoch, i, running_loss, loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "xd = []\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "def loss_fn(result, label):\n",
    "    return bce(result.flatten(end_dim=1), label.flatten(end_dim=1))\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "set_last_batch(0, 0)\n",
    "for epoch in range(num_epochs):\n",
    "    handle_epoch(batch_size, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78b5da4b-c5b6-4ced-9c0a-700d7ecc7a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_last_batch(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "932bf29a-b41d-48c4-be7f-a4e61abfc46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import torch\n",
    "device = torch.device('cuda')\n",
    "model = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b34d52e-0463-45fc-8da9-4e218cff499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, label = xd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7351595-b9dd-4dba-bc50-f453fc8d3f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[bool(x > 1 or x < -1) for x in label[0][1].flatten()].index(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e39937b1-e407-4fb2-8f15-094e3b85e635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.3694)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0][1][77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e20882d-06a2-40de-96e1-645b661e1192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "845"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "845"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20d780a-15f0-4ab0-9f82-3a2c4e1c8d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
