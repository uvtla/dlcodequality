{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14cdc3d5-8c11-490e-8116-3cdbe5d3a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir('dataset/Diff_Quality_Estimation')\n",
    "msg_file = 'comments.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c36085d3-4c21-434b-9258-4287e156e12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42115a2c-8661-45d6-b746-d89786e0eccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "132918it [00:18, 7246.47it/s]\n",
      "132918it [00:17, 7633.51it/s]\n",
      "132918it [00:30, 4358.46it/s]\n",
      "132918it [00:30, 4358.03it/s]\n",
      "132918it [00:25, 5272.25it/s]\n",
      "132918it [00:25, 5283.98it/s]\n",
      "132918it [00:24, 5348.41it/s]\n",
      "132918it [00:24, 5358.29it/s]\n",
      "132918it [00:24, 5350.80it/s]\n",
      "132918it [00:24, 5376.43it/s]\n",
      "132918it [00:24, 5381.71it/s]\n",
      "132918it [00:24, 5377.03it/s]\n",
      "132918it [00:24, 5393.68it/s]\n",
      "132918it [00:24, 5374.89it/s]\n",
      "132918it [00:24, 5377.50it/s]\n",
      "132918it [00:24, 5390.00it/s]\n",
      "132918it [00:24, 5385.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.023*\"like\" + 0.013*\"would\" + 0.012*\"seems\" + 0.012*\"think\" + 0.011*\"set\" + 0.010*\"could\" + 0.009*\"used\" + 0.008*\"call\" + 0.008*\"value\" + 0.008*\"needed\"')\n",
      "(1, '0.046*\"use\" + 0.025*\"instead\" + 0.019*\"would\" + 0.017*\"name\" + 0.016*\"think\" + 0.015*\"method\" + 0.014*\"make\" + 0.011*\"using\" + 0.011*\"could\" + 0.011*\"better\"')\n",
      "(2, '0.029*\"need\" + 0.024*\"test\" + 0.013*\"think\" + 0.011*\"want\" + 0.010*\"tests\" + 0.010*\"get\" + 0.008*\"sure\" + 0.008*\"work\" + 0.008*\"necessary\" + 0.008*\"check\"')\n",
      "(3, '0.061*\"line\" + 0.034*\"return\" + 0.032*\"error\" + 0.024*\"remove\" + 0.019*\"long\" + 0.016*\"null\" + 0.014*\"check\" + 0.014*\"message\" + 0.014*\"empty\" + 0.013*\"missing\"')\n",
      "(4, '0.025*\"file\" + 0.025*\"add\" + 0.023*\"code\" + 0.021*\"please\" + 0.016*\"pr\" + 0.015*\"function\" + 0.015*\"move\" + 0.015*\"remove\" + 0.014*\"class\" + 0.013*\"change\"')\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "class ReviewCorpus:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.dictionary = corpora.Dictionary(self.iter_docs())\n",
    "\n",
    "    def iter_docs(self):\n",
    "        # Iterate over the file and yield documents\n",
    "        with open(self.filepath, 'r', encoding='utf-8') as fr:\n",
    "            for line in tqdm(fr):\n",
    "                stop_words = set(stopwords.words('english'))\n",
    "                words = [word for word in word_tokenize(line.lower()) if word.isalpha() and word not in stop_words]\n",
    "                yield words\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Iterate over the file and yield BOW vectors\n",
    "        for tokens in self.iter_docs():\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "\n",
    "corpus_stream = ReviewCorpus(msg_file)\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = models.LdaModel(corpus=(corpus_stream), num_topics=5, id2word=corpus_stream.dictionary, passes=15)\n",
    "\n",
    "# Display topics\n",
    "topics = lda_model.print_topics()\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33fdbbf5-0679-4140-bfc1-c9cbc22b7c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "Words: 0.072*\"test\" + 0.015*\"make\" + 0.014*\"tests\" + 0.014*\"code\" + 0.013*\"also\" + 0.012*\"change\" + 0.009*\"file\" + 0.009*\"case\" + 0.009*\"sure\" + 0.009*\"one\"\n",
      "\n",
      "Topic: 1\n",
      "Words: 0.054*\"need\" + 0.045*\"think\" + 0.009*\"also\" + 0.009*\"method\" + 0.008*\"check\" + 0.008*\"add\" + 0.007*\"one\" + 0.007*\"since\" + 0.006*\"want\" + 0.005*\"new\"\n",
      "\n",
      "Topic: 2\n",
      "Words: 0.064*\"like\" + 0.015*\"looks\" + 0.015*\"seems\" + 0.014*\"something\" + 0.014*\"line\" + 0.013*\"could\" + 0.008*\"one\" + 0.008*\"change\" + 0.008*\"code\" + 0.008*\"name\"\n",
      "\n",
      "Topic: 3\n",
      "Words: 0.096*\"would\" + 0.014*\"make\" + 0.010*\"better\" + 0.007*\"could\" + 0.006*\"think\" + 0.006*\"error\" + 0.006*\"code\" + 0.005*\"also\" + 0.005*\"sense\" + 0.005*\"return\"\n",
      "\n",
      "Topic: 4\n",
      "Words: 0.139*\"use\" + 0.037*\"instead\" + 0.014*\"could\" + 0.011*\"please\" + 0.007*\"name\" + 0.007*\"string\" + 0.006*\"using\" + 0.006*\"function\" + 0.006*\"also\" + 0.005*\"method\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# Assuming texts is a list of documents\n",
    "with open(msg_file, 'r') as fr:\n",
    "    texts = [json.loads(line) for line in fr]\n",
    "\n",
    "# Enhanced Preprocessing\n",
    "texts_tokenized = [[word for word in word_tokenize(document.lower()) if word.isalpha() and word not in stop_words] for document in texts]\n",
    "\n",
    "# Detect and form bigrams\n",
    "bigram_model = Phrases(texts_tokenized, min_count=5, threshold=100)\n",
    "bigram_phraser = Phraser(bigram_model)\n",
    "texts_bigrams = [bigram_phraser[text] for text in texts_tokenized]\n",
    "\n",
    "# Create a dictionary and corpus for LDA\n",
    "dictionary = corpora.Dictionary(texts_bigrams)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_bigrams]\n",
    "\n",
    "# Using NMF for topic modeling\n",
    "nmf_model = models.Nmf(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "\n",
    "# Display topics\n",
    "for idx, topic in nmf_model.print_topics(-1):\n",
    "    print(f\"Topic: {idx}\\nWords: {topic}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e447826-b81f-4824-8eb3-b19d34299280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
